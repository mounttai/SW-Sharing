link,abstract
/doi/10.1287/isre.2019.0891," Using a data set on mobile technologies and mobile money in the emerging markets from 2000 to 2014, we examine the demand patterns of mobile technologies and mobile money when multiple generations of mobile technologies coexist in the market and each generation of the technologies may be bundled with mobile money. Using a structural model, we estimate the own and cross price elasticities for both mobile technologies and mobile money, and the demand effects between mobile money and mobile technologies. We find that the current, dominant technology (i.e., 3G) tends to have more robust demand as compared with other technologies that are either declining (i.e., 1G and 2G) or new to the market (i.e., 4G), and that customers are more likely to substitute forward toward newer technologies than backward toward older technologies when the price increases for a technology. We also find that mobile money differentiates the market and mitigates competition for those firms that offer mobile money. Theoretical and managerial implications are also discussed."
/doi/10.1287/opre.2021.2103," We consider nonparametric production technologies characterized by several component production processes and allow both component-specific and shared inputs and outputs. Each process uses its specific inputs and an unknown part of the shared inputs to produce its specific outputs and an unknown part of the shared outputs. For the described setting, we develop two new models of production technologies, under the assumptions of variable and constant returns to scale (VRS and CRS). These models are based on the worst-case assumption about the allocation of the shared inputs and outputs to component processes and, therefore, do not require exact knowledge of the actual allocation. The new models are larger than the standard VRS and CRS technologies. We provide a formal axiomatic derivation of the new technologies, explore their dual interpretation, and demonstrate their usefulness in an application."
/doi/10.1287/trsc.2021.1065," As freight deliveries in cities increase due to retail fragmentation and e-commerce, parking is becoming a more and more relevant part of transportation. In fact, many freight vehicles in cities spend more time parked than they are moving. Moreover, part of the public parking space is shared with passenger vehicles, especially cars. Both arrival processes and parking and delivery processes are stochastic in nature. In order to develop a framework for analysis, we propose a queueing model for an urban parking system consisting of delivery bays and general on-street parking spaces. Freight vehicles may park both in the dedicated bays and in general on-street parking, whereas passenger vehicles only make use of general on-street parking. Our model allows us to create parsimonious insights into the behavior of a delivery bay parking stretch as part of a limited length of curbside. We are able to find explicit expressions for the relevant performance measures, and formally prove a number of monotonicity results. We further conduct a series of numerical experiments to show more intricate properties that cannot be shown analytically. The model helps us shed light onto the effects of allocating scarce urban curb space to dedicated unloading bays at the expense of general on-street parking. In particular, we show that allocating more space to dedicated delivery bays can also make passenger cars better off."
/doi/10.1287/inte.2014.0788," When we encounter an unexpected critical health problem, a hospital’s emergency department (ED) becomes our vital medical resource. Improving an ED’s timeliness of care, quality of care, and operational efficiency while reducing avoidable readmissions, is fraught with difficulties, which arise from complexity and uncertainty. In this paper, we describe an ED decision support system that couples machine learning, simulation, and optimization to address these improvement goals. The system allows healthcare administrators to globally optimize workflow, taking into account the uncertainties of incoming patient injuries and diseases and their associated care, thereby significantly reducing patient length of stay. This is achieved without changing physical layout, focusing instead on process consolidation, operations tracking, and staffing. First implemented at Grady Memorial Hospital in Atlanta, Georgia, the system helped reduce length of stay at Grady by roughly 33 percent. By repurposing existing resources, the hospital established a clinical decision unit that resulted in a 28 percent reduction in ED readmissions. Insights gained from the implementation also led to an investment in a walk-in center that eliminated more than 32 percent of the nonurgent-care cases from the ED. As a result of these improvements, the hospital enhanced its financial standing and achieved its target goal of an average ED length of stay of close to seven hours. ED and trauma efficiencies improved throughput by over 16 percent and reduced the number of patients who left without being seen by more than 30 percent. The annual revenue realized plus savings generated are approximately $190 million, a large amount relative to the hospital’s $1.5 billion annual economic impact. The underlying model, which we generalized, has been tested and implemented successfully at 10 other EDs and in other hospital units. The system offers significant advantages in that it permits a comprehensive analysis of the entire patient flow from registration to discharge, enables a decision maker to understand the complexities and interdependencies of individual steps in the process sequence, and ultimately allows the users to perform system optimization."
/doi/10.1287/mksc.1080.0367," We investigate biases in product valuation and usage decisions that arise when consumers consider a new generation of a product that offers an expanded set of capabilities of uncertain value. Two experiments using a novel computer game show evidence of a valuation-usage disparity : Participants display a high willingness to pay for a new version of the game that offers a new set of controls, but fail to fully exercise the option to use these controls after purchase. This discrepancy is attributed to a fundamental difference in how new capabilities are valued at the time of purchase versus use. Consumer usage decisions appear to be driven by such myopic concerns as a desire to avoid short-term learning costs, whereas purchase decisions often fail to take into account the factors that drive usage, and are further inflated by global optimism in the future usefulness of new capabilities. We show that this lack of foresight can be explained by an intertemporal judgment model in which consumers attempt to value the option to use new capabilities as would be prescribed by economic theory, but are prone to hyperbolic discounting in their temporal valuation of present versus future costs and benefits."
/doi/10.1287/trsc.1120.0439," This paper deals with one-to-many-to-one vehicle routing and scheduling problems with pickups and deliveries and studies the effect of various backhauling strategies. Initially, focus is given on problem instances with clustered backhauls where all delivery customers must be visited before pickup customers. Afterward, operational settings with mixed backhauls and varying visiting sequence restrictions with respect to the capacity of the vehicles are examined. The proposed solution method evolves a set of reference solutions on the basis of a novel Adaptive Path Relinking framework. The latter encompasses an adaptive multisolution recombination procedure to generate provisional solutions based on the recurrence of particular solution attributes. On return, these solutions are used as guiding points for performing search trajectories from initial reference solutions via tunneling. Computational results on benchmark data sets of the literature illustrate the competitiveness and robustness of the proposed approach compared to state-of-the-art solution methods for well-known vehicle routing and scheduling problems. Finally, various experiments are also reported to demonstrate the economic effect of different mixing levels and densities of linehaul and backhaul customers."
/doi/10.1287/isre.14.3.291.16563," This paper examines the evolution of portfolio of controls over the duration of outsourced information systems development (ISD) projects. Drawing on five cases, it concludes that many findings from research on control of internal ISD projects apply to the outsourced context as well, but with some interesting differences. The portfolios of control in outsourced projects are dominated by outcome controls, especially at the start of the project; although the precision and frequency of these controls varies across projects. Behavior controls are often added later in the project, as are controls aimed to encourage and enable vendor self–control. Clan controls were used in only two of the cases—when the client and vendor had shared goals, and when frequent interactions led to shared values. In general, the outsourced projects we studied began with relatively simple controls but often required significant additional controls after experiencing performance problems. Factors influencing the choice and evolution of controls are also examined."
/doi/10.1287/msom.2017.0697," This paper seeks an efficient way to screen a population of patients at risk for hepatocellular carcinoma when (1) each patient’s disease evolves stochastically and (2) there are limited screening resources shared by the population. Recent medical discoveries have shown that biological information can be learned at each screening to differentiate patients into varying levels of risk for cancer. We investigate how to exploit this knowledge to choose which patients to screen to maximize early-stage cancer detections while limiting resource usage. We model the problem as a family of restless bandits, with each patient’s disease progression evolving as a partially observable Markov decision process. We derive an optimal policy for this problem and discuss managerial insights into what characterizes more effective screening. To provide numerical evidence, we use two independent data sets of over 800 patients, one to train the optimal policy, and the other to build a computer simulation to act as a test bed for said policy. We are able to show that our policy detects 22% more early-stage cancers than current practice, while using the same amount of resource expenditure. We provide insights into the structure underlying our policy and discuss the implications of our findings. The e-companion is available at https://doi.org/10.1287/msom.2017.0697 ."
/doi/10.1287/orsc.2020.1399," Despite the large literature on alliance contract design, we know little about how transacting parties change and amend their underlying contracts during the execution of strategic alliances. Drawing on existing research in the alliance contracting literature, we develop the empirical question of how contract detail and prior ties influence the amount, direction, and type of change in such agreements during the collaboration. We generated a sample of 115 joint ventures (JVs) by distributing a survey to JV board members or top managers and found that the amount of contract change is negatively associated with the level of detail in the initial contract but is positively associated with the number of prior ties between alliance partners. In relation to the direction of contract change, we find that the level of detail of the initial agreements negatively correlates with the likelihood of removing or weakening existing provisions and that prior collaborative experience positively correlates with the likelihood of strengthening of existing provisions or adding of new ones. We also find that prior ties affect the type of change in that JV parents prefer to change enforcement provisions more so than the coordination provisions in the contract. Our paper generates new insights on the complementarities between relational governance and transaction cost economics perspectives on alliance contracting."
/doi/10.1287/stsc.2017.0033," We study the level of market competition as a determinant for the propensity of cooperation between startups entering new markets and incumbents operating in these markets. We provide ample empirical evidence suggesting that startups and incumbents are more likely to cooperate in the commercialization of startups’ technological innovations in markets with either high or low competition levels than in markets with moderate competition levels. Importantly, we further show that startups’ innovativeness has a contingent effect—it encourages cooperation at low-to-moderate levels of competition, but encourages competition at moderate-to-high levels of competition."
/doi/10.1287/mnsc.2013.1879," We propose a dynamic structural model that illuminates the economic mechanisms shaping individual behavior and outcomes on crowdsourced ideation platforms. We estimate the model using a rich data set obtained from IdeaStorm.com, a crowdsourced ideation initiative affiliated with Dell. We find that, on IdeaStorm.com, individuals tend to significantly underestimate the costs to the firm for implementing their ideas but overestimate the potential of their ideas in the initial stages of the crowdsourcing process. Therefore, the “idea market” is initially overcrowded with ideas that are less likely to be implemented. However, individuals learn about both their abilities to come up with high-potential ideas as well as the cost structure of the firm from peer voting on their ideas and the firm's response to contributed ideas. We find that individuals learn rather quickly about their abilities to come up with high-potential ideas, but the learning regarding the firm's cost structure is quite slow. Contributors of low-potential ideas eventually become inactive, whereas the high-potential idea contributors remain active. As a result, over time, the average potential of generated ideas increases while the number of ideas contributed decreases. Hence, the decrease in the number of ideas generated represents market efficiency through self-selection rather than its failure. Through counterfactuals, we show that providing more precise cost signals to individuals can accelerate the filtering process. Increasing the total number of ideas to respond to and improving the response speed will lead to more idea contributions. However, failure to distinguish between high- and low-potential ideas and between high- and low-ability idea generators leads to the overall potential of the ideas generated to drop significantly. This paper was accepted by Sandra Slaughter, information systems ."
/doi/10.1287/mnsc.2018.3067," This study examines the effect of psychological contract breach on budgetary misreporting. Psychological contracts are mental models or schemas that govern how employees understand their exchange relationships with their employers. Psychological contract breach leads to feelings of violation and can occur even when employees’ economic contracts are fulfilled. We study the effects of psychological contract breach on three common types of employee participation in budgeting that differ in the degree of employees’ influence over their approved budgets. These include affirmative budgeting (full influence), consultative budgeting (moderate influence), and authoritative budgeting (low influence). When organizations communicate that employees will be involved in budgeting, employees develop psychological contracts of affirmative budgeting. If employees subsequently experience authoritative or consultative budgeting, their psychological contracts are breached. Employees who experience psychological contract breach seek redress through budgetary misreporting. Experimental results indicate that psychological contract breach partially mediates the relation between budgeting type and budgetary misreporting. Results also indicate asymmetry in the effects of psychological contract breach versus repair. Effects of breach on budgetary misreporting persist even after the breach no longer occurs. This paper was accepted by Shivaram Rajgopal, accounting. The online appendix is available at https://doi.org/10.1287/mnsc.2018.3067 ."
/doi/10.1287/inte.1110.0605," The National Security Personnel System (NSPS) was designed to provide greater opportunity for merit-based pay raises than the preexisting civil service system. However, a US Army organization was concerned that over time the accumulation of merit raises would make the total payroll unaffordable. We were asked to investigate the conditions under which this situation would be likely to happen. We used the Vensim ® modeling software to produce a model of the influences and events that would cause payroll to rise or fall from year to year. Our analysis showed that the merit pay policies in place at the start of our study were very likely to lead to undesirable growth in total pay and a concentration of all employees near the maximum allowed salary, defeating the purpose of NSPS—to reward better service with more pay. The most important factors were the fraction of merit pay that was given as merit raises (as opposed to bonuses) and the salaries of people newly hired to replace those who left the organization. We showed how departing senior employees should be replaced with less-senior ones to avoid long-term payroll growth. We used the model to find the combinations of merit pay and new-hire policies that would balance the effect of these two major influences and produce little or no long-term payroll growth. We also showed that some other influences that had concerned the client were actually of small effect compared to the stochastic uncertainty in payroll growth. Another influence, attrition, produced a more substantial effect. The interactive nature of the model allowed real-time exploration of alternatives with the client and helped build confidence in the robustness of the results. These results highlight potential problem areas needing careful analysis in implementing similar merit pay systems."
/doi/10.1287/mnsc.2014.2103," This paper examines how firm investments in technology-based employee monitoring impact both misconduct and productivity. We use unique and detailed theft and sales data from 392 restaurant locations from five firms that adopt a theft monitoring information technology (IT) product. We use difference-in-differences models with staggered adoption dates to estimate the treatment effect of IT monitoring on theft and productivity. We find significant treatment effects in reduced theft and improved productivity that appear to be primarily driven by changed worker behavior rather than worker turnover. We examine four mechanisms that may drive this productivity result: economic and cognitive multitasking, fairness-based motivation, and perceived increases of general oversight. The observed productivity results represent substantial financial benefits to both firms and the legitimate tip-based earnings of workers. Our results suggest that employee misconduct is not solely a function of individual differences in ethics or morality, but can also be influenced by managerial policies that can benefit both firms and employees. This paper was accepted by Serguei Netessine, operations management ."
/doi/10.1287/mnsc.1110.1505," How did the diffusion of the Internet influence research collaborations within firms? We examine the relationship between business use of basic Internet technology and the size and geographic composition of industrial research teams between 1992 and 1998. We find robust empirical evidence that basic Internet adoption is associated with an increased likelihood of collaborative patents from geographically dispersed teams. On the contrary, we find no evidence of such a link between Internet adoption and within-location collaborative patents, nor do we find any evidence of a relationship between basic Internet and single-inventor patents. We interpret these results as evidence that adoption of basic Internet significantly reduced the coordination costs of research teams, but find little evidence that a drop in the costs of shared resource access significantly improved research productivity. This paper was accepted by Lee Fleming, entrepreneurship and innovation."
/doi/10.1287/stsc.2018.0074," The growth and diversity of strategic management research raise important questions about the field’s identity. In response to the increasing breadth of topics addressed in the field, scholars have suggested the need for a more topical focus, improved theoretical and empirical rigor, or additional integrative reviews. We suggest an alternative approach, one that emphasizes the distinctive contributions of strategic management. We highlight three unique characteristics of strategic decisions: interdependence across contemporaneous decisions, across the decisions of other economic actors, and across time. We suggest that this threefold characterization of strategic decisions improves on existing definitions that rely on the perceived importance of decisions or the trade-offs presented by a decision. Our definition of strategic decisions highlights patterns of interdependent choices that are complementary or superadditive to value creation. Moreover, this characterization of strategic decisions offers a ready way for scholars with multiple disciplinary or topical interests to connect with the field’s core. These characteristics of strategic decisions also relate to theories of resource allocation and strategic investment, competitive advantage, and firm boundaries that suggest opportunities to better connect these streams of research."
/doi/10.1287/mnsc.12.10.B433," This paper describes a production smoothing system which combines several known scheduling tools into an operational unit. The input to the system is forecasted customer demand; the outputs are the required production levels, size of labor force, planned overtime and expected inventories of classes and individual end products. Because of varying demand, economic manufacturing quantities are calculated by the method of dynamic programming following Wagner and Whitin. The manufacturing progress function is used to convert units into labor requirements. Finally, the operating schedule is derived by a linear programming formulation which balances payroll costs, the costs of labor fluctuations, and inventory charges. This system has been in operation for several months."
/doi/10.1287/mnsc.2019.3413," We develop a model of oligopolistic firms that produce partially differentiated products and generate pollution as a byproduct. We analyze and compare two types of pollution regulation: Cap-and-Trade and Taxes. Firms can respond to regulation by any combination of pollution abatement, output reduction, emissions trading (under Cap-and-Trade), or payment of pollution taxes (under Taxes). We prove that well-chosen regulation can, besides reducing pollution, actually improve firms’ profits relative to laissez-faire (unregulated markets), and simultaneously improve consumer surplus and welfare. Thus, regulation Pareto-dominates laissez-faire under a wide range of plausible conditions. These results are driven by an unintended consequence of pollution regulation: Competing firms can use the regulation to tacitly (and credibly) collude to reduce production and improve their profits. We show that the degree of competition plays a critical role in determining the economic consequences of pollution regulation. Our results suggest that the regulator’s primary consideration should be the impact of regulation on consumers rather than producers. This paper was accepted by Vishal Gaur, operations management."
/doi/10.1287/mnsc.1120.1560," We study the online market for peer-to-peer (P2P) lending, in which individuals bid on unsecured microloans sought by other individual borrowers. Using a large sample of consummated and failed listings from the largest online P2P lending marketplace, Prosper.com, we find that the online friendships of borrowers act as signals of credit quality. Friendships increase the probability of successful funding, lower interest rates on funded loans, and are associated with lower ex post default rates. The economic effects of friendships show a striking gradation based on the roles and identities of the friends. We discuss the implications of our findings for the disintermediation of financial markets and the design of decentralized electronic markets. This paper was accepted by Sandra Slaughter, information systems."
/doi/10.1287/mnsc.47.2.308.9838," A fundamental problem in managing product development is the optimal timing, frequency, and fidelity of sequential testing activities that are carried out to evaluate novel product concepts and designs. In this paper, we develop a mathematical model that treats testing as an activity that generates information about technical and customer-need related problems. An analysis of the model results in several important findings. First , optimal testing strategies need to balance the tension between several variables, including the increasing cost of redesign, the cost of a test as function of fidelity, and the correlation between sequential tests. Second , a simple form of our model results in an EOQ-like result: The optimal number of tests (called the E conomic T esting F requency or ETF) is the square root of the ratio of avoidable cost and the cost of a test. Third , the relationship between sequential tests can have an impact on optimal testing strategies. If sequential tests are increasing refinements of one another, managers should invest their budgets in a few high-fidelity tests, whereas if the tests identify problems independently of one another it may be more effective if developers carry out a higher number of lower-fidelity tests. Using examples, the implications for managerial practice are discussed and suggestions for further research undertakings are provided."
/doi/10.1287/opre.2018.1817," Antimicrobial resistance is a significant public health threat. In the United States alone, two million people are infected, and 23,000 die each year from antibiotic-resistant bacterial infections. In many cases, infections are resistant to all but a few remaining drugs. We examine the case in which a single drug remains and solve for the optimal treatment policy for a susceptible–infected–susceptible infectious disease model, incorporating the effects of drug resistance. The problem is formulated as an optimal control problem with two continuous state variables: the disease prevalence and drug’s “quality” (the fraction of infections that are drug-susceptible). The decision maker’s objective is to minimize the discounted cost of the disease to society over an infinite horizon. We provide a new generalizable solution approach that allows us to thoroughly characterize the optimal treatment policy analytically. We prove that the optimal treatment policy is a bang-bang policy with a single switching time. The action/inaction regions can be described by a single boundary that is strictly increasing when viewed as a function of drug quality, indicating that, when the disease transmission rate is constant, the policy of withholding treatment to preserve the drug for a potentially more serious future outbreak is not optimal. We show that the optimal value function and/or its derivatives are neither C 1 nor Lipschitz continuous, suggesting that numerical approaches to this family of dynamic infectious disease models may not be computationally stable. Furthermore, we demonstrate that relaxing the standard assumption of a constant disease transmission rate can fundamentally change the shape of the action region, add a singular arc to the optimal control, and make preserving the drug for a serious outbreak optimal. In addition, we apply our framework to the case of antibiotic-resistant gonorrhea."
/doi/10.1287/orsc.11.3.285.12496," In this qualitative field study, I explore how the construction of a cultural institution's identity is related to the construction of strategic capabilities and resources. I investigated the 1996 musicians’ strike at the Atlanta Symphony Orchestra (ASO), which revealed embedded and latent identity conflicts. The multifaceted and specialized identity of the ASO was reinforced by different professional groups in the organization: the ideologies of musicians and administrators emphasized institutional resource allocations consistent with the legitimating values of their professions, i.e., artistic excellence versus economic utility. These identity claims, made under organizational crisis, accounted for variations in the construction of core competencies. I propose a model that explicates how the construction of core capabilities lies at the intersection of identification and interpretive processes in organizations. Implications are discussed for defining firm capabilities in cultural institutions and for managing organizational forms characterized by competing claims over institutional identity, resources, and core capabilities."
/doi/10.1287/mnsc.1090.1099," When designing a sourcing strategy in practice, a key task is to determine the average order rates placed to each source because that affects cost and supplier management. We consider a firm that has access to a responsive nearshore source (e.g., Mexico) and a low-cost offshore source (e.g., China). The firm must determine an inventory sourcing policy to satisfy random demand over time. Unfortunately, the optimal policy is too complex to allow a direct answer to our key question. Therefore, we analyze a tailored base-surge (TBS) sourcing policy that is simple, used in practice, and captures the classic trade-off between cost and responsiveness. The TBS policy combines push and pull controls by replenishing at a constant rate from the offshore source and producing at the nearshore plant only when inventory is below a target. The constant base allocation allows the offshore facility to focus on cost efficiency, whereas the nearshore facility's quick response capability is utilized only dynamically to guarantee high service. The research goals are to (i) determine the allocation of random demand into base and surge capacity, (ii) estimate corresponding working capital requirements, and (iii) identify and value the key drivers of dual sourcing. We present performance bounds on the optimal cost and prove that economic optimization brings the system into heavy traffic. We analyze the sourcing policy that is asymptotically optimal for high-volume systems and present a simple “square-root” formula that is insightful to answer our questions and sufficiently accurate for practice, as is demonstrated with a validation study."
/doi/10.1287/opre.1080.0641," In 2004, Deere & Company's Commercial & Consumer Equipment Division (C&CE) engaged in a new logistics initiative to further enhance its outbound distribution network. The goal was to offer faster and more reliable replenishment to 2,500 North American independent dealers while keeping logistics costs in check by deploying different tactics during the peak (February–July) and offpeak (August–January) selling and shipping seasons. Deere and SmartOps worked together under a shared reward contract based on actual cost reductions accomplished (that are additive over the benefits that may have accrued for other reasons). Through careful analysis, validation, and verification of the data available, the team was able to develop detailed models of the current and alternative distribution systems. By formulating the key replenishment and transportation decisions and constraints as a mixed-integer mathematical program, the team was able to use powerful off-the-shelf solution software to find improvements. The ability to fix variables to perform what-if analysis also helped in the acceptance of the recommendations. Over a period of three years, Deere significantly improved service to 82% of their dealers (without any reduction in service to the other 18%) while reducing logistics costs by over $10 million. The novelty of this work stems from the dynamic seasonal optimization of Deere C&CE distribution network and replenishment decisions as a way of meeting service and cost reduction mandates, the creative use of tactical network optimization operations research models and what-if analysis to meet the implementation goals under time constraints and in the scrutiny given to the results."
/doi/10.1287/isre.1040.0015," Institution-based trust is a buyer’s perception that effective third-party institutional mechanisms are in place to facilitate transaction success. This paper integrates sociological and economic theories about institution-based trust to propose that the perceived effectiveness of three IT-enabled institutional mechanisms–specifically feedback mechanisms, third-party escrow services, and credit card guarantees–engender buyer trust in the community of online auction sellers. Trust in the marketplace intermediary that provides the overarching institutional context also builds buyer’s trust in the community of sellers. In addition, buyers’ trust in the community of sellers (as a group) facilitates online transactions by reducing perceived risk. Data collected from 274 buyers in Amazon’s online auction marketplace provide support for the proposed structural model. Longitudinal data collected a year later show that transaction intentions are correlated with actual and self-reported buyer behavior. The study shows that the perceived effectiveness of institutional mechanisms encompasses both “weak” (market-driven) and “strong” (legally binding) mechanisms. These mechanisms engender trust, not only in a few reputable sellers, but also in the entire community of sellers, which contributes to an effective online marketplace. The results thus help explain why, despite the inherent uncertainty that arises when buyers and sellers are separated in time and in space, online marketplaces are proliferating. Implications for theory are discussed, and suggestions for future research on improving IT-enabled trust-building mechanisms are suggested."
/doi/10.1287/mnsc.1080.0889," We investigate the situation where a customer experiencing an inventory stockout at a retailer potentially leaves the firm's market. In classical inventory theory, a unit stockout penalty cost has been used as a surrogate to mimic the economic effect of such a departure; in this study, we explicitly represent this aspect of consumer behavior, incorporating the diminishing effect of the consumers leaving the market upon the stochastic demand distribution in a time-dynamic context. The initial model considers a single firm. We allow for consumer forgiveness where customers may flow back to the committed purchasing market from a nonpurchasing “latent” market. The per-period decisions include a marketing mix to attract latent and new consumers to the committed market and the setting of inventory levels. We establish conditions under which the firm optimally operates a base-stock inventory policy. The subsequent two models consider a duopoly where the potential market for a firm is now the committed market of the other firm; each firm decides its own inventory level. In the first model, the only decisions are the stocking decisions and in the second model, a firm may also advertise to attract dissatisfied customers from its competitor's market. In both cases, we establish conditions for a base-stock equilibrium policy. We demonstrate comparative statics in all models."
/doi/10.1287/orsc.1090.0498," This paper investigates whether an employee's perception of customer wealth affects his likelihood of engaging in illegal behavior. We propose that envy and empathy lead employees to discriminate in illicitly helping customers based on customer wealth. We test for this hypothesis in the vehicle emissions testing market, where employees have the opportunity to illegally help customers by passing vehicles that would otherwise fail emissions tests. We find that for a significant number of inspectors, leniency is much higher for those customers with standard vehicles than for those with luxury cars, although a smaller group appears to favor wealthy drivers. We also investigate the psychological mechanisms explaining this wealth-based discriminatory behavior using a laboratory study. Our experiment shows that individuals are more willing to illegally help peers when those peers drive standard rather than luxury cars, and that envy and empathy mediate this effect. Collectively, our results suggest the presence of wealth-based discrimination in employee–customer relations and that envy toward wealthy customers and empathy toward those of similar economic status drive much of this illegal behavior. Implications for both theory and practice are discussed."
/doi/10.1287/isre.2019.0909," In lending-based real estate crowdfunding, borrowers are required to pledge their housing properties as collateral to secure the loans. This nascent practice differs from ordinary peer-to-peer lending in that lenders, to make sound investment decisions, need to process additional information other than basic loan attributes. We examine how lender behavior of investing in real-estate-secured loans is shaped by information that is particularly relevant in such an emerging market. We collect and analyze the data from a large lending-based real estate crowdfunding platform, where each loan is secured by either a mortgage (a mortgage-secured or MS loan) or a borrower’s own house (a house-secured or HS loan). Our analysis reveals that lender decisions of how fast to invest and how much to invest are influenced by both on-platform and off-platform information. For on-platform information, we find that lenders as a whole prefer HS loans to MS loans, as reflected in quicker and larger lending transactions. Experienced lenders tend to invest more aggressively, in both time and amount, but exhibit a weaker preference for HS loans as compared with their inexperienced counterparts. As to off-platform information, our results show that a rise in housing prices is associated with quicker investment decisions, and this association is found to be even stronger on HS loans. Further, when stock market volatility is large, lenders tend to slow down their investment behavior; however, we find such a tendency weaker on MS loans. This research contributes to the literature by establishing relationships between crowdfunding activities, housing prices and stock market performance. Our findings also provide implications for managers and platform designers who desire to stimulate and leverage the fundraising momentum."
/doi/10.1287/msom.2020.0883," Problem description : Measuring quality in the service industry remains a challenge. Existing methodologies are often costly and unscalable. Furthermore, understanding how elements of service quality contribute to the performance of service providers continues to be a concern in the service industry. In this paper, we address these challenges in the restaurant sector, a vital component of the service industry. Academic/practical relevance : Our work provides a scalable methodology for measuring the quality of service providers using the vast amount of text in social media. The quality metrics proposed are associated with economic outcomes for restaurants and can help predict future restaurant performance. Methodology : We use text present in online reviews on Yelp.com to identify and extract service dimensions using nonnegative matrix factorization for a large set of restaurants located in a major city in the United States. We subsequently validate these service dimensions as proxies for service quality using external data sources and a series of laboratory experiments. Finally, we use econometrics to test the relationship between these dimensions and restaurant survival as additional validation. Results : We find that our proposed service quality dimensions are scalable, match industry standards, and are correctly identified by subjects in a controlled setting. Furthermore, we show that specific service dimensions are significantly correlated with the survival of merchants, even after controlling for competition and other factors. Managerial implications : This work has implications for the strategic use of text analytics in the context of service operations, where an increasingly large text corpus is available. We discuss the benefits of this work for service providers and platforms, such as Yelp and OpenTable."
/doi/10.1287/opre.1040.0201," We consider facilities that follow a cyclic schedule to replenish the inventory of a set of items through production by a shared resource. We introduce a dynamic produce-up-to policy that recovers the target cyclic schedule after a single disruption, and is also shown to be effective when disruptions are more frequent. Our policy is more flexible than traditional recovery policies in that our policy is able to adjust the amount of idle time observed during recovery in response to disruptions, and yet re-establish the target idle time as the schedule recovers. This results in a policy that not only saves cost and time, but provides better schedule stability than other recovery procedures. Furthermore, unlike simple produce-up-to policies, our policy is anticipatory—replenishments will be speeded up or delayed, whichever is necessary, to help avoid congestion at the shared resource. In addition, recovery is controlled by a single “knob” or parameter that can tune recovery to be fast and aggressive (frequent setups and small batches) or slow and methodical (few setups and larger batches). Finally, our policy is easy to implement, augmenting a traditional produce-up-to policy with a simple set of counters that control replenishment decisions."
/doi/10.1287/mnsc.46.11.1466.12083," To optimize large-scale queuing systems configurations, OR professionals typically use discrete event simulation packages to examine in detail the movement of entities through such systems, assuming stochastic but fixed arrival patterns. Demand aspects are, however, routinely ignored as few attempts are made to capture the feedback effect of queue performance on the arrival process. Econometricians, on the other hand, use a simultaneous equations estimation approach relying on past data, but they typically disregard the technological insights provided by simulation. This paper combines both tools to study the ailing port system of Calcutta, India, and concludes that raising prices will improve both economic and engineering performances. Microeconomic models of shipowner behavior are constructed to explain the nature of the empirical findings. Finally, full-equilibrium demand elasticities are calculated using the dual prices from an appropriate nonlinear program, which are then compared to the benchmark value expected of profit-maximizing behavior."
/doi/10.1287/orsc.13.3.321.2779," Drawing on an empirical study on automakers' management of supplier involvement in product development in Japan, this paper shows that when the design of a component is outsourced to a supplier, how much automakers know about the component matters for them to gain a better outcome. While the actual tasks of designing and manufacturing components could be out-sourced, automakers should retain the relevant knowledge to obtain better component design quality. The paper argues that knowledge partitioning should be distinguished from task partitioning, and provides some implications for the knowledge-based theory of the firm. The results indicate that effective pattern of knowledge partitioning differs by the nature of component development project in terms of technological newness. For regular projects, it is more important for the automaker to have a higher level of architectural knowledge (how to coordinate various components for a vehicle) than of component-specific knowledge, which is supposed to be provided by the supplier. However, when the project involves new technology for the supplier, it is important for the automaker to have a higher level of component-specific knowledge to solve unexplored engineering problems together with the supplier. In innovative projects, effective knowledge partitioning seems to demand some overlap between an automaker and a supplier, rather than efficient and clear-cut boundaries that are optimal for regular projects. Such “fluid” nature of knowledge boundaries contingent on the project types poses a challenge for firms seeking both technological leadership and efficiency in established products. Developing and maintaining knowledge about an outsourced component is by no means easy. When the actual design tasks are outsourced, automakers miss substantial opportunities to gain relevant knowledge through learning by doing. Also, obtained knowledge may be diffused among competitors through shared suppliers. Another problem for automakers is that component-specific knowledge is important for only limited cases (innovative projects). Even worse, component-specific knowledge has a trade off relationship with architectural knowledge. Such an inherent dilemma of managing knowledge, however, may provide some automakers with the opportunity to achieve sustainable competitive advantage. Additional analysis shows that one automaker managed both types of knowledge better than others in a manner that deals effectively with the dilemma. Its organizational mechanisms include career development policies, extensive documentation of technological information, internal training programs, and incentive schemes. The difficulty in implementing those mechanisms in a consistent and complementary manner seems to explain why there was a significant variance among automakers in knowledge level, even when the actual tasks were carried out by a shared supplier."
/doi/10.1287/mnsc.48.3.364.7731," Explaining how entrepreneurs overcome information asymmetry between themselves and potential investors to obtain financing is an important issue for entrepreneurship research. Our premise is that economic explanations for venture finance, which do not consider how social ties influence this process, are undersocialized and incomplete. However, we also argue that organization theoretic arguments, which draw on the concept of social obligation, are oversocialized. Drawing on the organizational theory literature, and in-depth fieldwork with 50 high-technology ventures, we examine the effects of direct and indirect ties between entrepreneurs and 202 seed-stage investors on venture finance decisions. We show that these ties influence the selection of ventures to fund through a process of information transfer."
/doi/10.1287/orsc.1090.0489," This article examines how multiple ownership changes unfold in international equity joint venture (IEJV) evolution and how such repeated changes impact short-term performance and long-term survival. By theorizing a new concept—the trap of continual change—in the IEJV context, we challenge the adaptive viewpoint assumed in alliance dynamics research. We propose that partners sometimes respond to an initial dissatisfaction with the venture result with a dysfunctional repetition of rearranging the ownership control structure. This continual change locks the organization into bad choices and sends it into a downward spiral. Acknowledging the mixed motive nature of inter-partner relationships, we incorporate cooperative versus competitive dynamics manifested in shared control arrangements. We propose that shared ownership control lends stability to the IEJV until the initial IEJV agreement is renegotiated; this stability is a result of the cooperative forces of mutual interdependence and mutual forbearance between the partners. However, when the power balance breaks down, the potential for inter-partner conflict increases. When the ownership control structure of the IEJV is restructured, especially multiple times, shared control arrangements become increasingly unstable as behavioral, cultural, and managerial differences are amplified."
/doi/10.1287/mnsc.1090.1136," Discrete choice models estimated using hypothetical choices made in a survey setting (i.e., choice experiments) are widely used to estimate the importance of product attributes in order to make product design and marketing mix decisions. Choice experiments allow the researcher to estimate preferences for product features that do not yet exist in the market. However, parameters estimated from experimental data often show marked inconsistencies with those inferred from the market, reducing their usefulness in forecasting and decision making. We propose an approach for combining choice-based conjoint data with individual-level purchase data to produce estimates that are more consistent with the market. Unlike prior approaches for calibrating conjoint models so that they correctly predict aggregate market shares for a “baseline” market, the proposed approach is designed to produce parameters that are more consistent with those that can be inferred from individual-level market data. The proposed method relies on a new general framework for combining two or more sources of individual-level choice data to estimate a hierarchical discrete choice model. Past approaches to combining choice data assume that the population mean for the parameters is the same across both data sets and require that data sets are sampled from the same population. In contrast, we incorporate in the model individual characteristic variables, and assert only that the mapping between individuals' characteristics and their preferences is the same across the data sets. This allows the model to be applied even if the sample of individuals observed in each data set is not representative of the population as a whole, so long as appropriate product-use variables are collected that can explain the systematic deviations between them. The framework also explicitly incorporates a model for the individual characteristics, which allows us to use Bayesian missing-data techniques to handle the situation where each data set contains different demographic variables. This makes the method useful in practice for a wide range of existing market and conjoint data sets. We apply the method to a set of conjoint and market data for minivan choice and find that the proposed method predicts holdout market choices better than a model estimated from conjoint data alone or a model that does not include demographic variables."
/doi/10.1287/isre.1080.0193," Consumer-generated product reviews have proliferated online, driven by the notion that consumers' decision to purchase or not purchase a product is based on the positive or negative information about that product they obtain from fellow consumers. Using research on information processing as a foundation, we suggest that in the context of an online community, reviewer disclosure of identity-descriptive information is used by consumers to supplement or replace product information when making purchase decisions and evaluating the helpfulness of online reviews. Using a unique data set based on both chronologically compiled ratings as well as reviewer characteristics for a given set of products and geographical location-based purchasing behavior from Amazon, we provide evidence that community norms are an antecedent to reviewer disclosure of identity-descriptive information. Online community members rate reviews containing identity-descriptive information more positively, and the prevalence of reviewer disclosure of identity information is associated with increases in subsequent online product sales. In addition, we show that shared geographical location increases the relationship between disclosure and product sales, thus highlighting the important role of geography in electronic commerce. Taken together, our results suggest that identity-relevant information about reviewers shapes community members' judgment of products and reviews. Implications for research on the relationship between online word-of-mouth (WOM) and sales, peer recognition and reputation systems, and conformity to online community norms are discussed."
/doi/10.1287/mnsc.1040.0343," The diffusion of Internet technology among firms is widely considered to be one of the primary factors behind the rapid economic growth of the 1990s. However, little systematic study has examined the variation in firm decisions to adopt the Internet. I explore the sources of this variation by examining Internet adoption decisions in a very large sample of organizations in the finance and services sector in 1998. I show how prior information technology (IT) investments and workplace organization decisions affect the returns to adopting simple and complex Internet technologies. I show that recent investments in client/server (C/S) networking applications have competing effects on the likelihood of Internet adoption. Such investments can slow adoption by acting as a short-run substitute or by creating “switching costs.” Geographic dispersion of employees is complementary with Internet adoption, suggesting that Internet technology lowered internal coordination costs. Increases in organization size and external pressure also increase the likelihood of adoption."
/doi/10.1287/opre.1080.0640," We consider a single-product revenue management problem where, given an initial inventory, the objective is to dynamically adjust prices over a finite sales horizon to maximize expected revenues. Realized demand is observed over time, but the underlying functional relationship between price and mean demand rate that governs these observations (otherwise known as the demand function or demand curve) is not known. We consider two instances of this problem: (i) a setting where the demand function is assumed to belong to a known parametric family with unknown parameter values; and (ii) a setting where the demand function is assumed to belong to a broad class of functions that need not admit any parametric representation. In each case we develop policies that learn the demand function “on the fly,” and optimize prices based on that. The performance of these algorithms is measured in terms of the regret : the revenue loss relative to the maximal revenues that can be extracted when the demand function is known prior to the start of the selling season. We derive lower bounds on the regret that hold for any admissible pricing policy, and then show that our proposed algorithms achieve a regret that is “close” to this lower bound. The magnitude of the regret can be interpreted as the economic value of prior knowledge on the demand function, manifested as the revenue loss due to model uncertainty."
/doi/10.1287/inte.9.3.58," Accountability is becoming an increasingly significant concept in the management literature. The traditional association of accountability with budgetary control and hence an economic perspective has been expanded to include responsibility for the worth of the individual as advocated in human resource accounting. The demand for accountable performance has also increased. Although the term accountability is becoming more visible, there has been relatively little work done in understanding the psychological processes involved. Two questions arise. First, what establishes accountability? Second, what motivates accountable behavior? This paper attempts to provide a basic groundwork for answering these questions."
/doi/10.1287/orsc.2020.1419," This paper addresses the recognized need for connecting scholarship on materiality and evaluation by conceptualizing how materiality provides grounds for “valuation entrepreneurship.” It extends the scope of materiality scholarship by considering an ignored organizational outcome while offering stronger evidence for the role of supply-side factors in social evaluation. The theoretical model posits that materiality affords opportunities for identity construction and social organization that can lead to the emergence of a new theory of value contesting the evaluative regime. This framework is applied to the reanalysis of a famous case: Impressionism. The analysis shows that new materials and methods of painting served as a “focus” for the social organization of artists with a shared identity of craftsmen. These artists espoused a new theory of value that advocated the “unfinishedness” of artworks and used natural perception as an objective basis for contestation of the “subjective” evaluative regime at the salons. The contestation had political overtones, drawing on cultural resources and scientific tenets to justify the valorization of individuality and decentralization of art appraisal. An endogenous account of culture in action presents materiality as a natural counterpoint to the emphasis on conceptualization."
/doi/10.1287/inte.9.4.146," A comment about [Peterson, R., E. A. Silver. 1979. Decision Systems for Inventory Management and Production Planning . John Wiley & Sons, New York, 168.] Pointing out to false mathematical assumption that the costs will be shared equally between the departments involved."
/doi/10.1287/inte.2017.0886," Advertisements are a key source of revenue for companies in the broadcasting and web industries. However, because of increasing competition, advertisers and web publishers have been forced to find innovative ways to increase their profits and gain competitive advantages. Revenue management is a useful operations research and management science tool that may be used to do so. In this paper, we provide an updated review of revenue-management research conducted in the broadcasting and online advertisement industries, highlighting the strategies and techniques adopted to maximize advertising revenue. We also identify mobile advertising as an emerging revenue-management application and review current research on it. We conclude by identifying potential gaps that future research might address."
/doi/10.1287/trsc.34.2.216.12307," We consider a heterogeneous two-dimensional space where a given set of competitive facilities is located. Customers are assumed to be scattered continuously over the space, and each customer is assumed to choose a facility to minimize individual total cost of receiving service. The total cost consists of both the congested travel time to the facility and a cost associated with the congestion externality at the facility. Furthermore, customer demand at any location is assumed to be a function of the total cost of receiving service. Given these assumptions, it is of interest to estimate the market areas and market shares captured by each competitive facility. This problem is formulated here as a calculus of variations problem, and its optimality conditions are shown to be equivalent to the spatial customer choice equilibrium conditions with elastic demand and market externality. The model is solved by an efficient finite element method and illustrated with a numerical example."
/doi/10.1287/opre.1040.0172," We consider a model of a service system that delivers two nonsubstitutable services to a market of heterogenous users. The first service is delivered subject to a “guaranteed” (G) processing rate, and the second is a “best-effort” (BE) type service in which residual capacity not allocated to the guaranteed class is shared among BE users. Users, in turn, are sensitive to both price and congestion-related effects. The service provider’s objective is to optimally design the system so as to extract maximum revenues. The design variables in this problem consist of a pair of static prices for the two services, a policy that controls admission of G users into the system, and the mechanism by which users are informed of the state of congestion in the system. Because these objectives are difficult to address using exact analysis, we pursue approximations that are tractable and lead to structural insights. Specifically, we first solve a deterministic relaxation of the original objective to obtain a “fluid-optimal” solution that is subsequently evaluated and refined to account for stochastic fluctuations. Using diffusion limits, we derive approximations that yield the following structural results: (1) pricing rules derived from the deterministic analysis are “almost” optimal, (2) the optimal operational regime for the system is close to heavy traffic, and (3) real-time congestion notification results in increased revenues. Numerical results illustrate the accuracy of the proposed approximations and validate the aforementioned structural insights."
/doi/10.1287/orsc.11.6.648.12537," Research in organizational communication has examined the structure and content of interaction, but has paid little attention to research traditions outside the organizational sciences that explore the social-psychological interconnections between relationship development and interaction. In this paper we draw upon and extend those traditions to develop a model of how communication relationships develop within organizational dyads. The proposed model examines organization-based communication relationships through a synthesis of theoretical perspectives on communication richness, relational communication, interpersonal attribution, and social expectancy. We also call upon precepts of structuration theory to embed these microlevel processes in an organizational context. The relational outcome in the model is “interactional richness,” a dyad-level construct that assesses the extent to which communication within the dyad is high in shared meaning. Model antecedents are aspects of interaction through which communicators reciprocally define their relationships, including relational message properties, message patterns that emerge over time, and relational perceptions. We propose that these communication properties and behaviors give rise to relationship attributions. We then incorporate processes of expectancy confirmation and violation to explain how specific communication encounters lead individuals to reformulate attributions regarding the status of a given relationship. Research propositions articulate how attribution/expectancy processes mediate between relational communication behavior and relationship development outcomes. We also develop propositions addressing how relational communication behavior is influenced by macrolevel factors, including hierarchy, structure, and culture. In a concluding section we discuss the model's potential contribution to research and practice, address its limitations, and offer recommendations for future research aimed at testing its embedded hypotheses."
/doi/10.1287/mnsc.1120.1587," Because of the health and economic costs of childhood obesity, coupled with studies suggesting the benefits of comprehensive (dietary, physical activity, and behavioral counseling) intervention, the U.S. Preventive Services Task Force recently recommended childhood screening and intervention for obesity beginning at age 6. Using a longitudinal data set consisting of the body mass index of 3,164 children up to age 18 and another longitudinal data set containing the body mass index at ages 18 and 40 and the presence or absence of disease (hypertension and diabetes) at age 40 for 747 people, we formulate and numerically solve—separately for boys and girls—a dynamic programming problem for the optimal biennial (i.e., at ages 2,4,…,16) obesity screening thresholds. Unlike most screening problem formulations, we take a societal viewpoint, where the state of the system at each age is the population-wide probability density function of the body mass index. Compared to the biennial version of the task force's recommendation, the screening thresholds derived from the dynamic program achieve a relative reduction in disease prevalence of 3% at the same screening (and treatment) cost, or—because of the flatness of the disease versus screening trade-off curve—achieves the same disease prevalence at a 28% relative reduction in cost. Compared to the task force's policy, which uses the 95th percentile of body mass index (from cross-sectional growth charts tabulated by the Centers for Disease Control and Prevention) as the screening threshold for each age, the dynamic programming policy treats mostly 16-year-olds (including many who are not obese) and very few males under 14 years old. Although our results suggest that adult hypertension and diabetes are minimized by focusing childhood obesity screening and treatment on older adolescents, the shortcomings in the available data and the narrowness of the medical outcomes considered prevent us from making a recommendation about childhood obesity screening policies. This paper was accepted by Assaf Zeevi, stochastic models and simulation."
/doi/10.1287/orsc.2016.1105," This paper addresses a fundamental conundrum at the heart of meaning making: How is agreement to change achieved amid multiple, coexisting meanings? This challenge is particularly salient when proposing a new strategic initiative as it introduces new meanings that must coexist with multiple prevailing meanings. Yet, prior literature on meaning-making processes places different emphases on the extent to which agreement to a new initiative requires shared meaning across diverse organizational members. We propose the concept of a joint account as the means through which an agreement to change may be achieved that accommodates multiple, coexisting meanings that satisfy diverse constituents’ vested interests. Based on the findings from an ethnographic study of a university’s strategic planning process, we develop a framework that demonstrates two different patterns in the microprocesses of meaning making. These patterns extend our understanding about the way vested interests enable or constrain the construction of a joint account. In doing so, we contribute to knowledge about resistance, ambiguity, and lack of agreement to a proposed change."
/doi/10.1287/orsc.2021.1459," Collaborations between individuals in firms have important implications for the development of relational and human capital. In knowledge-intensive contexts where collaborations are formed to deliver services to clients, collaboration decisions can involve nontrivial tradeoffs between short-term and long-term benefits: individuals and firms must carefully manage the tradeoffs between leveraging existing relational and human capital for the reliable performance of repeat collaboration and creating new relational and human capital through new collaboration. Building from the premise that servicing clients is central to collaboration decisions in human asset–intensive firms, we examine how client-related factors shape collaboration decisions among lawyers (partners) in UK law firms providing M&A legal advisory services. We focus on three key client-related dimensions that we predict govern collaboration decisions: the depth of individual- and firm-level relationships with the focal client, key client attributes that reflect the client’s status and its use of different firms to undertake its outsourced work, and client-driven individual- and firm-level resource constraint . Our empirical findings support our proposition that client-related factors influence the pattern of collaborations between individuals in firms. We also reveal how client-related factors at the individual level can have opposite effects on collaboration decisions from those at the firm level. Overall, our findings contribute to research on relational capital, strategic human capital, team formation, professional service firms, and the microfoundations of strategy."
/doi/10.1287/inte.31.5.104.9653," Linking business students in operations management (OM) directly to real-world operations improvement experiences in manufacturing and service organizations resulted in 70 OM student field projects during the academic year 1998-1999 at Indiana University South Bend. The projects helped client organizations to identify strategies to improve quality, flexibility, and speed of operations, streamline processes, enhance customer satisfaction, reduce costs, and improve profits. Students gained valuable professional experience. The supervising faculty member and the institution enhanced their professional and community participation. The shared experiences should stimulate business schools and industries to implement this totally integrative experiential approach to OM course instruction."
/doi/10.1287/mnsc.1040.0357," Software vulnerability disclosure has become a critical area of concern for policymakers. Traditionally, a Computer Emergency Response Team (CERT) acts as an infomediary between benign identifiers (who voluntarily report vulnerability information) and software users. After verifying a reported vulnerability, CERT sends out a public advisory so that users can safeguard their systems against potential exploits. Lately, firms such as iDefense have been implementing a new market-based approach for vulnerability information. The market-based infomediary provides monetary rewards to identifiers for each vulnerability reported. The infomediary then shares this information with its client base. Using this information, clients protect themselves against potential attacks that exploit those specific vulnerabilities. The key question addressed in our paper is whether movement toward such a market-based mechanism for vulnerability disclosure leads to a better social outcome. Our analysis demonstrates that an active unregulated market-based mechanism for vulnerabilities almost always underperforms a passive CERT-type mechanism. This counterintuitive result is attributed to the market-based infomediary’s incentive to leak the vulnerability information inappropriately. If a profit-maximizing firm is not allowed to (or chooses not to) leak vulnerability information, we find that social welfare improves. Even a regulated market-based mechanism performs better than a CERT-type one, but only under certain conditions. Finally, we extend our analysis and show that a proposed mechanism—federally funded social planner—always performs better than a market-based mechanism."
/doi/10.1287/mksc.1120.0745," Presidential elections provide both an important context in which to study advertising and a setting that mitigates the challenges of dynamics and endogeneity. We use the 2000 and 2004 general elections to analyze the effect of market-level advertising on county-level vote shares. The results indicate significant positive effects of advertising exposures. Both instrumental variables and fixed effects alter the ad coefficient. Advertising elasticities are smaller than are typical for branded goods yet significant enough to shift election outcomes. For example, if advertising were set to zero and all other factors held constant, three states' electoral votes would have changed parties in 2000. Given the narrow margin of victory in 2000, this shift would have resulted in a different president."
/doi/10.1287/mnsc.47.11.1515.10250," We construct a dynamic game to model a monopoly of finitely durable goods. The solution concept is Markov-perfect equilibria with general equilibria embedded in every time period. Our model is flexible enough to simultaneously explain or accommodate many commonly observed phenomena or stylized facts, such as concurrent leasing and selling, active secondary markets for used goods, heterogeneous consumers, endogenous consumption patterns, depreciation, an infinite time horizon, and nontrivial transaction costs. Within our model, consumers have incentives to segment themselves into various consumption classes according to their willingness to pay, and nontrivial transaction costs to sell used goods put strong constraints on consumers' consumption sequences in time. As a direct consequence of the finite durability, the market power of the monopolist remains intact. Leasing manifests itself as a facilitator of price discrimination by debundling the durable good into new and used portions that are naturally bundled together under outright sales. The concurrent leasing and selling reflects the degree of the comparative advantage the monopolist has over consumers in disposing used goods. This comparative advantage, which is partially exploited by the monopolist and partially shared by the consumers, provides a sufficient mechanism to gain Pareto improvement on the market."
/doi/10.1287/mnsc.2014.1904," We consider the problem of choosing, from a set of N potential stock-keeping units (SKUs) in a retail category, K SKUs to be carried at each store to maximize revenue or profit. Assortments can vary by store, subject to a maximum number of different assortments. We view a SKU as a set of attribute levels and also model possible substitutions when a customer's first choice is not in the assortment. We apply maximum likelihood estimation to sales history of the SKUs currently carried by the retailer to estimate the demand for attribute levels and substitution probabilities, and from this, the demand for any potential SKU, including those not currently carried by the retailer. We specify several alternative heuristics for choosing SKUs to be carried in an assortment. We apply this approach to optimize assortments for three real examples: snack cakes, tires, and automotive appearance chemicals. A portion of our recommendations for tires and appearance chemicals were implemented and produced sales increases of 5.8% and 3.6%, respectively, which are significant improvements relative to typical retailer annual comparable store revenue increases. We also forecast sales shares of 1, 11, and 25 new SKUs for the snack cake, tire, and automotive appearance chemical applications, respectively, with mean absolute percentage errors (MAPEs) of 16.2%, 19.1%, and 28.7%, which compares favorably to the 30.7% MAPE for chain sales of two new SKUs reported by Fader and Hardie (1996). This paper was accepted by Yossi Aviv, operations management."
/doi/10.1287/inte.2018.0966," This work presents the development and improvements obtained by the implementation of continuous flow in the cabinet process at Schneider Electric’s manufacturing plant in Tlaxcala, Mexico. The areas involved were the demand-planning, materials-planning, and manufacturing processes and warehouse operations. The implementation process, which consisted of shared planning, analysis of control operations, and synchronization of lean manufacturing techniques, led to increased communication among personnel within departments at the Schneider Tlaxcala plant (STP). In addition, this implementation produced the following benefits: (1) improvement of demand coverage in high-demand seasons without exceeding the production capacity, (2) reduction of shortages and delays in the assembly line with associated savings of approximately $3.5 million, (3) reduction of 17.5% in the overproduction of stamped parts and thus on the daily holding inventory, (4) setup time reduction of 77%, and (5) elimination of product flow between the cabinet process and the warehouse to reduce delivery lead time to the assembly line. The fifth benefit was made possible because STP was able to supply the cabinets directly to the assembly line in two days. As a result, the company released 49 storage spaces and improved its customer service by 5% because it could make the final products available to customers at the appropriate time (i.e., on schedule). After 24 months, these improvements led to total recurring savings of approximately $1 million considering an investment of 1.25%. In addition, Schneider Electric was able to successfully replicate this methodology in similar manufacturing plants in North America."
/doi/10.1287/msom.2019.0856," Problem definition : Buyers can encourage competition among multiple suppliers of goods or services by redistributing their shares of the business based on recent observable performances. This is usually needed because actual efforts or investments by suppliers are typically unobservable, a moral hazard issue. This poses the problem: How should such redistribution be linked to the suppliers’ recent performances to motivate the best supplier performance? Academic/practical relevance : Such performance-based allocations of business are a common practice in supply management when a buyer sources from multiple suppliers. But there is little research on the structural characteristics of the optimal allocation rule in the presence of moral hazard. Methodology : We apply principal-agent theory to model the buyer’s incentive design problem. We analytically derive the optimal allocation rule and conduct numerical experiments to evaluate the effectiveness of several simple heuristics against the optimal rule. Results : Our key result is that the buyer should divide the purchase among suppliers based on their relative performance deviations from the targets set by the buyer. A ratio rule is used for positive deviations, whereas a winner-take-all may be applied when some suppliers underperform. Our result generalizes to a range of environments with different cost structures, distributions of performance outcomes, correlated performance, etc. When the optimal rule is used to benchmark several more practical heuristics, the only really poorly performing heuristic is one that does not use the relative performance deviations. Managerial implications : Buyers’ allocations should reflect suppliers’ relative performance deviations from the desired targets, not absolute performances. Performance deviations can be weighted in some way, but this is a second-order gain. A ratio rule is needed to reward good performance, whereas a winner-take-all rule with more extreme allocations is used for penalizing poor performance."
/doi/10.1287/orsc.1120.0784," This paper develops a parsimonious process-level theory that connects organizational structure to exploration and exploitation. Toward this end, it develops a mathematical model of organizational decision making that combines an information processing approach in the spirit of Sah and Stiglitz [Sah RK, Stiglitz JE (1986) The architecture of economic systems: Hierarchies and polyarchies. Amer. Econom. Rev. 76(4):716–727] with elements from signal detection theory. The model is first used to explore a “design space” of organizations and identify trade-offs and dominance relationships among alternative organization designs. The paper then studies open questions in the organization design literature, such as the extent to which exploration and exploitation can be produced by one organization and what is the effect of organization size on exploration. More broadly, this research speaks to calls for the introduction of more process-level explanations in the organizations literature. The paper concludes with testable hypotheses and managerially relevant insights."
/doi/10.1287/mnsc.1110.1351," The Capital Assistance Program (CAP) was created by the U.S. government in February 2009 to provide backup capital to large financial institutions unable to raise sufficient capital from private investors. Under the terms of the CAP, a participating bank receives contingent capital by issuing preferred shares to the Treasury combined with embedded options for both parties: The bank gets the option to redeem the shares or convert them to common equity, with conversion mandatory after seven years; the Treasury earns dividends on the preferred shares and gets warrants on the bank's common equity. We develop a contingent claims framework in which to estimate market values of these CAP securities. The interaction between the competing options held by the buyer and issuer of these securities creates a game between the two parties, and our approach captures this strategic element of the joint valuation problem and clarifies the incentives it creates. We apply our method to the 18 publicly held bank holding companies that participated in the Supervisory Capital Assessment Program (the stress test) launched together with the CAP. On average, we estimate that compared to a market transaction, the CAP securities carry a net value of approximately 30% of the capital invested for a bank participating to the maximum extent allowed under the terms of the program. We also find that the net value varies widely across banks. We compare our estimates with abnormal stock price returns for the stress test banks at the time the terms of the CAP were announced; we find correlations between 0.78 and 0.85, depending on the precise choice of period and set of banks included. These results suggest that our valuation aligns with shareholder perception of the value of the program, prompting questions about industry reactions and the overall impact of the program. This paper was accepted by Wei Xiong, finance."
/doi/10.1287/mnsc.2014.2073," Social norms involve observation by others and external sanctions for violations, whereas moral norms involve introspection and internal sanctions. To study such norms and their effects, we design a laboratory experiment. We examine dictator choices, where we create a shared understanding by providing advice from peers with no financial payoff at stake. We vary whether advice is given, as well as whether choices are made public. This design allows us to explicitly separate the effects of moral and social norms. We find that choices are in fact affected by a combination of observability and shared understanding. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2014.2073 . This paper was accepted by Teck-Hua Ho, behavioral economics ."
/doi/10.1287/msom.1070.0153," We consider the problem of allocating demand that originates from multiple sources among multiple inventory locations. Demand from each source arrives dynamically according to an independent Poisson process. The cost of fulfilling each order depends on both the source of the order and its fulfillment location. Inventory at all locations is replenished from a shared production facility with a finite production capacity and stochastic production times. Consequently, supply lead times are load dependent and affected by congestion at the production facility. Our objective is to determine an optimal demand allocation and optimal inventory levels at each location so that the sum of transportation, inventory, and backorder costs is minimized. We formulate the problem as a nonlinear optimization problem and characterize the structure of the optimal allocation policy. We show that the optimal demand allocations are always discrete, with demand from each source always fulfilled entirely from a single inventory location. We use this discreteness property to reformulate the problems as a mixed-integer linear program and provide an exact solution procedure. We show that this discreteness property extends to systems with other forms of supply processes. However, we also show that supply systems exist for which the property does not hold. Using numerical results, we examine the impact of different parameters and provide some managerial insights."
/doi/10.1287/mksc.2020.1245," Some firms that operate in multiple product markets use the same brand in different markets, whereas others use different brands in different markets. This research investigates in which product markets a firm should use the same or different brands and how this decision depends on the relatedness of product markets. To answer this question, I propose a framework of market relatedness that characterizes the relationships among distinct product markets from the supply side (e.g., shared production technology) and demand side (e.g., correlated customer preferences). This framework is applied to a model of reputation in which a multiproduct firm’s product quality is jointly determined by its hidden capability type (i.e., adverse selection) and hidden choice of effort level (i.e., moral hazard) in each product market. Consumers obtain noisy information about the firm by observing its track record, that is, product quality produced in the past. Umbrella branding allows consumers to pool the firm’s track record across different product markets and form expectations about the product quality based on market relatedness. The analysis shows that umbrella branding is optimal if supply-side relatedness is high and demand-side relatedness is not too high. However, if the product markets are closely related in both dimensions, then independent branding may be optimal because, as an umbrella brand, the firm faces a temptation to exploit positive information spillover across product markets through its shared brand name. By using different brand names, a firm can credibly commit to investing in all product markets and thereby earn higher profits. Finally, this paper provides implications for an umbrella brand’s customer relationship management strategy whether to serve the same or distinct customer segments with its products."
/doi/10.1287/orsc.2017.1173," We argue that organizations have deep roots in traumatic societal shocks that long preceded their founding. Drawing from the strategic management and social science literatures, we explain how traumatic shocks, such as conflict, disease, and natural disaster, can alter the institutional and cultural paths that determine future business environments. Historical shocks can help clarify the origin of cultural and institutional differences and help provide causal inference about why these differences are correlated with organizational structure and strategy. We explain specific cultural and institutional mechanisms through which historical traumatic shocks persist as well as specific organizational factors influenced by these mechanisms. We also provide guidance on key approaches for empirically linking traumatic shocks with modern firms as well as common identification problems in these methods. Our approach clarifies a path for clarifying theory on how culture and institutions shape firms and how management scholars might anticipate the evolution of market development following emerging traumatic shocks."
/doi/10.1287/mnsc.1060.0563," Our objective is to understand the cross-category effects of marketing activities using aggregate store-level scanner data. For this, we provide a framework derived from household utility maximizing behavior which assumes that a household chooses the “bundle” of products with the highest utility. We use a second-order Taylor series approximation to an arbitrary utility function to represent bundle utility. Aggregate sales or shares in each category are derived under the assumption that households are heterogeneous in their preferences and in their sensitivities to marketing activities. Our estimation accounts for potential price endogeneity in demand. Using store-level scanner data on four product categories—liquid laundry detergents, powdered laundry detergents, liquid fabric softeners, and sheet fabric softeners—we find evidence for a complementary relationship between liquid softeners and both forms of detergents. We also find that the magnitude of cross-category elasticities are brand specific, i.e., different brands in a category have a different price impact on the demand for a brand in another category. The results have implications for retailers in terms of the potential need for cross-category management, as well as for manufacturers such as Procter & Gamble that participate in all four categories. We compare our model with a log-log regression specification on three criteria—estimated elasticities, hold-out sample predictions, and retailer cross-category pricing. We find that the proposed model produces more reasonable estimates relative to the log-log model; it predicts better and is more useful for pricing purposes. Further, in a simulation study, we show that our proposed model can recover the elasticities from a data-generating process that simulates household-level joint outcomes across categories even after these data have been aggregated to brand-level shares within each category. By contrast, the log-log regression model is unable to do so."
/doi/10.1287/mksc.2017.1069," Facing purchase choice involving ambiguity in product quality, consumers behave in a boundedly rational manner. Consumers also exhibit varying degrees of predisposition toward a product. We present a simple model of boundedly rational choice under ambiguity. The model’s key feature is that it captures the interaction between predisposition and ambiguity. We build on the choice model to derive demand curves and the unique equilibrium market outcomes (regarding prices, profits, and market shares) under duopolistic competition. In equilibrium, market shares are proportional to prices. In symmetric competition, higher equilibrium prices obtain when the ambiguity in product quality is high or when the customer base is partisan. For vertically differentiated products, the strategy of a higher-quality firm to marginally reduce ambiguity depends on the ambiguity level inherent in the product–market environment. The presence of informed customers may increase the equilibrium prices and profits of both firms. An understanding of the predisposition–ambiguity interaction may improve the firm’s information and brand management strategy. The online appendix is available at https://doi.org/10.1287/mksc.2017.1069 ."
/doi/10.1287/msom.2.2.144.12353," We investigate a simple dynamic model of firm behavior in which firms compete by investing in capacity that is used to provide a good or service to their customers. There is a fixed total market of customers whose demands for the good or service are random and who divide their patronage between the firms in each period. Periodically, the market shares of the two firms can change based on the realized level of customer service provided in the prior period. We assume that the expected level of customer service can be expressed as a function of the (per customer) capacity of the firms' service delivery systems, and that service declines as the capacity decreases. The firms differ in their customers' willingness to defect when confronted by service failure. The primary issue we address is the firms' capacity decisions in response to customer service concerns and competitive pressure. We provide conditions under which the firms' optimal (i.e., equilibrium) capacity levels in a period are proportional to the size of their respective customer bases in that period. Further, we develop expressions for the value of a firm's customers and the implicit cost of service failure. Results for both single-period and finite-horizon problems are investigated and applied to two examples: (1) competition between Internet service providers who operate systems that we approximate by simple loss-type queueing models, and (2) competition between make-to-stock producers who operate systems that we approximate by newsvendor inventory models. For both examples, solutions are derived and interpreted."
/doi/10.1287/mnsc.1070.0761," We used quantitative genetics techniques to compare the entrepreneurial activity of 870 pairs of monozygotic (MZ) and 857 pairs of same-sex dizygotic (DZ) twins from the United Kingdom. We ran model-fitting analyses to estimate the genetic, shared environmental and nonshared environmental effects on the propensity of people to become entrepreneurs. We found relatively high heritabilities for entrepreneurship across different operationalizations of the phenomenon, with little effect of family environment and upbringing. Our findings suggest the importance of considering genetic factors in explanations for why people engage in entrepreneurial activity."
/doi/10.1287/inte.1110.0582," Blockbuster Inc., a chain of VHS, DVD, Blu-ray, and video game rental stores, has developed a highly specialized distribution network. The company maintains a single distribution center in which it receives products from suppliers, and processes and packs them for shipping to stores across the United States. The volumes of particular products and the number of different products shipped in a week have significant week-to-week volatility. Short lead times are typical because of supplier manufacturing delays and strict in-store due-date requirements. At the distribution center, processing and packing are scheduled through multiple processing departments that compete for use of shared merge conveyors and shared sortation systems. Blockbuster's general processing and packing goal is on-time delivery of products to stores while controlling costs. In this paper, we describe the development and implementation of a mixed-integer programming model to schedule Blockbuster's short-range order-processing operations. Implemented beginning in January 2007, the model has helped Blockbuster to maintain timely shipping, reduce related labor and transportation costs, improve capacity utilization, and attain a better understanding of how to achieve further improvements. Blockbuster's structure, in which multiple processing departments compete for subsequent shared resources, such as merge conveyors and sortation systems, is common in other industries; therefore, we also discuss the relevance of this model to other organizations."
/doi/10.1287/mnsc.1080.0946," We develop a model for the competitive interactions in service industries where firms cater to multiple customer classes or market segments with the help of shared service facilities or processes so as to exploit pooling benefits. Different customer classes typically have distinct sensitivities to the price of service as well as the delays encountered. In such settings firms need to determine (i) the prices charged to all customer classes; (ii) the waiting time standards, i.e., expected steady state waiting time promised to all classes; (iii) the capacity level; and (iv) a priority discipline enabling the firm to meet the promised waiting time standards under the chosen capacity level, all in an integrated planning model that accounts for the impact of the strategic choices of all competing firms. We distinguish between three types of competition: depending on whether firms compete on the basis of their prices only, waiting time standards only, or on the basis of prices and waiting time standards. We establish in each of the three competition models that a Nash equilibrium exists under minor conditions regarding the demand volumes. We systematically compare the equilibria with those achieved when the firms service each market segment with a dedicated service process."
/doi/10.1287/orsc.8.6.593," In recent years, agency theory has substantially influenced research on corporate governance. Organizational sociologists have critiqued the agency theory model of boards as limited and have studied how the functioning of boards is shaped by structural, political, and cognitive contexts. Building on their work, this paper empirically studies the cultural embeddedness of boards in a nonprofit organization called Medlay. It shows how organizational identity—the members' shared beliefs about the central, enduring, and distinctive characteristics of the organization—influences the construction and enactment of the director's role and shapes interactions among board members and managers. The findings demonstrate that the role of the director is shaped by Medlay's Janus-faced identity, as both a volunteer-driven organization and a family of friends; directors see themselves as vigilant monitors and as friendly, supportive colleagues. The findings also portray how some board members' scrutiny of the budget, including “lavish” travel expenditures, surfaces the contradictions in Medlay's identity, and creates conflicts for directors. Should board members take manager to task and thereby exercise vigilance and uphold the ideal of volunteer control, or should they safeguard the principle of friendship and avoid all conflict? An influential subset of directors and top managers resolved the budget issue and preserved Medlay's identity by using different “face-saving” strategies to make directors feel that they had been vigilant, and to affirm sentiments of cooperation. More generally, this study extends the literature on corporate governance by showing how organizational identity influences the construction and enactment of the director's role. It introduces the idea of “conflicts of commitment,” a form of intra-role conflict that arises when directors are besieged by conflicting aspects of the organization's identity. When actions occur that breach the expected role performance of board members, latent contradictions in the organizational identity emerge, and directors are faced with the conflict of upholding one dimension of identity while undermining the other. The study also contributes to research on organizational identity by proposing a model of how organizational and individual identities shape the board role through the processes of identification and action, and how a hybrid identity generates the potential for intra-role conflict."
/doi/10.1287/opre.2019.1877," Majority judgment (MJ) and approval voting (AV) are compared in theory and practice. Criticisms of MJ and claims that AV is superior are refuted. The two primary criticisms have been that MJ is not “Condorcet consistent” and that it admits the “no-show paradox.” That MJ is not Condorcet consistent is a good property shared with AV: the domination paradox shows that majority rule may well err in an election between two. Whereas the no-show paradox is in theory possible with MJ, it is as a practical matter impossible. For those who believe that this extremely rare phenomenon is important, it is proven that MJ with three grades cannot admit the no-show paradox. In contrast, AV suffers from serious drawbacks because voters can only “tick” or “approve” candidates—at best, only Approve or Disapprove each candidate. With AV, voters cannot express their opinions adequately; experiments show that Approve is not the opposite of Disapprove , and although AV does not admit the no-show paradox, it admits the very closely allied no-show syndrome and “insensitivity.” Two are too few. Substantive debate must concern three or more grades."
/doi/10.1287/orsc.14.6.707.24867," This paper develops and tests a model of multilevel experience-based top management team competence and its effects on a firm's capacity of entrepreneurial growth. The model incorporates the individual and additive effects of firm, team, and industry levels of managerial experience and the conflict effects of combining multiple levels of experience. Theoretical arguments are tested in a longitudinal sample of entrepreneurial firms from the medical and surgical instruments industry. The results indicate that founders' participation in the top management team and managers' past experience in the industry contribute to the competence of the team in seizing new growth opportunities. The results also show that, because of conflict effects, the positive effect of founders' participation in the management team on the rate of growth weakens as either the shared team-specific experience or industry-specific managerial experience in the team increases. For practitioners, the most important implication is that for sustained growth, entrepreneurial firms should learn to balance different levels of managerial experience in the top management team. One way to achieve this balance is to retain valuable founder resources in the team while avoiding high levels of shared team-specific experience and industry-specific managerial experience."
/doi/10.1287/orsc.12.5.612.10096," This study describes the image of organizing that underlies a complex organization's ability to incorporate streams of innovation with continuing operations. I argue that a mechanistic organization archetype prevents people from seeing in their minds' eyes—from imagining—how to do the work of innovation organizationwide, but that theorists have failed to articulate an alternative to this archetype in its own terms. The study focuses on two elements of organizing: the differentiation and the integration of work. I build grounded theory for an alternate, innovative archetype of organizing by exploring the shared image of work differentiation and integration in twelve firms that vary in innovative ability. I find a fundamentally different image in innovative organizations that is centered on hands-on practice: People understand value creation as a long-term working relationship with customers, in which they apply the firm's skills to anticipate and solve customer problems. This practice is differentiated into distinct problems in value creation, each of which embodies the integral flow of work like a lateral slice, but which situates those problems in their own contexts. People understand themselves to be organized in an autonomous community of practice that takes charge of one of the problems. The communities of practice are integrated by standards for action: vivid, simple representations of value that frame work and that are reenacted in practice. The analysis details this different image of organizing by describing four autonomous communities of practice and contrasting them with the image of organizing found in noninnovative firms. The paper illustrates how this new image straightforwardly organizes and controls innovative work, and how the noninnovative image of differentiation and integration makes this work unimaginable. I conclude that innovation can be incorporated with continuing operations, provided that managers and theorists reimagine the differentiation and integration of work. I offer preliminary ideas for doing so, and suggest some next steps in this research stream."
/doi/10.1287/orsc.11.2.212.12511," This paper explores the cognitive aspects underlying industries in hypercompetitive environments. Hypercompetition represents a state of competition with rapidly escalating levels of competition and reduced periods of competitive advantage for firms. In hypercompetitive industries member firms act boldly and aggressively to create a state of competitive disequilibrium. In this paper we explore the particular conditions that managers encounter in making sense of hypercompetitive industries and argue that the nature of these conditions is such that conventional sensemaking frameworks will not work. We then describe the “adaptive sensemaking” practices established in the literature for dealing with temporary turbulence and suggest that in hypercompetition those processes continue indefinitely. We argue that these processes can become institutionalized as standard operating procedures within firms, and as shared recipes within industries, which in turn perpetuates hyperturbulent conditions."
/doi/10.1287/mnsc.2021.4113," This paper examines the information content of insider silence, periods of no insider trading. We hypothesize that, to avoid litigation risk, rational insiders do not sell own-company shares when they anticipate bad news; neither would they buy, given unfavorable prospects. Thus, they keep silent. By contrast, insiders sell shares when they do not anticipate significant bad news. Future stock returns are significantly lower following insider silence than following insider net selling, especially among firms with higher litigation risk. We examine two quasinatural experiments where new laws result in changes in shareholder litigation risks for insiders. In both cases, with higher shareholder litigation risks, stocks where insiders stay silent earn significantly lower returns than other stocks. This paper was accepted by Karl Diether, finance."
/doi/10.1287/orsc.2013.0886," To examine what, if any, are the differences in how activities are coordinated within versus between firms, we conducted interviews with 32 project managers regarding 60 projects in the offshore software services industry. Uniquely, our projects were sampled along two dimensions: (1) colocation versus spatial distribution and (2) delivery by groups of individuals from a single firm versus from multiple firms. Our evidence suggests that in colocated projects, the same broad categories of coordination mechanisms are used both within and between firms. However, there is a qualitative difference in how geographically (i.e., spatially) distributed projects are coordinated within versus between firms. Distributed projects conducted within firms rely extensively on tacit coordination mechanisms; such mechanisms are not readily available in between-firm projects that are spatially distributed. This difference may arise because of the lack of shared history and lack of enforcement through common authority in the between-firm context."
/doi/10.1287/isre.10.1.70," The application of fundamental option pricing models (OPMs), such as the binomial and the Black-Scholes models, to problems in information technology (IT) investment decision making have been the subject of some debate in the last few years. Prior research, for example, has made the case that pricing “real options” in real world operational and strategic settings offers the potential for useful insights in the evaluation of irreversible investments under uncertainty. However, most authors in the IS literature have made their cases using illustrative, rather than actual real world examples, and have always concluded with caveats and questions for future research about the applicability of such methods in practice. This paper makes three important contributions in this context: (1) it provides a formal theoretical grounding for the validity of the Black-Scholes option pricing model in the context of the spectrum of capital budgeting methods that might be employed to assess IT investments; (2) it shows why the assumptions of both the Black-Scholes and the binomial option pricing models place constraints on the range of IT investment situations that one can evaluate that are similar to those implied by traditional capital budgeting methods such as discounted cash flow analysis; and (3) it presents the first application of the Black-Scholes model that uses a real world business situation involving IT as its test bed. Our application focuses on an analysis of the timing of the deployment of point-of-sale (POS) debit services by the Yankee 24 shared electronic banking network of New England. This application enables us to make the case for the generalizability of the approach we discuss to four IT investment settings."
/doi/10.1287/orsc.1100.0576," The social networks literature suggests that ties must be maintained to retain value. In contrast, we show that reconnecting dormant ties—former ties, now out of touch—can be extremely useful. Our research prompted Executive MBA students to consult their dormant contacts about an important work project; outcomes compared favorably to those of their current ties. In addition, reconnecting previously strong ties led to all of the four benefits that are usually associated with either weak ties (efficiency and novelty) or strong ties (trust and shared perspective). These findings suggest that dormant relationships—often overlooked or underutilized—can be a valuable source of knowledge and social capital."
/doi/10.1287/stsc.2018.0072," This paper examines when and under what conditions whom to hire is a strategic decision. We identify four mechanisms involved in hiring that add to the “strategicness” of human capital decisions. We posit that, to the degree that human capital outcomes are influenced by these mechanisms, hiring cannot be effectively delegated or treated independently from a firm’s other strategic decisions, nor will a “best athlete” approach to hiring lead to optimal results. We outline the shifts in scholarly attention required for scholars to conceptualize hiring as a strategic process, and we argue that this conceptualization is critical given the increasing use of data science tools by industry to support and automate hiring decisions. By delving into the mechanisms and conditions that make hiring strategic, we contribute to a broader understanding of how and why firms acquire human capital and highlight gaps and opportunities for future research."
/doi/10.1287/stsc.2020.0101," Understanding how firms protect their knowledge from leakage is becoming increasingly important, especially when knowledge is not well protected by legal mechanisms such as patents. The rapid rise in research and development (R&D) activities taking place in offshore locations that only offer weak legal protection for intellectual property provides the ideal context to study this question. Using interview and survey data from captive R&D centers of multinational firms in India, we (1) identify five organizational practices that firms use to protect their knowledge from leaking to competitors in offshore locations, (2) consider whether these practices limit knowledge leakage or limit damage from leaked knowledge, and (3) explore whether they are complements or substitutes."
/doi/10.1287/orsc.1100.0566," We examine the extent to which performance effects of firms' network positions vary with the ages of the ties comprising those positions. Our analysis of Canadian investment banks' underwriting syndicate ties indicates that the performance benefits of closure ties increase with age, whereas benefits of bridging ties decrease with age. We also find that benefits yielded by hybrid network positions, combining elements of both closure and bridging, are greatest when old closure ties are combined with either very young or very old bridging ties. Our findings support the idea that the advantages firms gain (or do not) from their network positions depend on the relational character of the ties comprising them, highlighting the risk of theorizing structural network effects without also considering the relational and temporal dynamics associated with network positions."
/doi/10.1287/orsc.2018.1249," Gendered processes and outcomes are pervasive in organizational life. They shape how individuals perceive their career prospects, which types of opportunities they pursue, how they get work done within organizations, and how they balance this work with the rest of their life. Organizations themselves also shape and are shaped by gender dynamics, from the ways they design jobs and performance evaluation systems to the assumptions managers make about individuals’ preferences and motivations. This virtual special issue collects together 14 papers published in Organization Science that challenge common understandings about the sources of gender differences in career outcomes, the effects of balancing work–life obligations, and the ways that gender dynamics play out in teams and organizations. An important insight that emerges from a comparison of these studies is that demand effects are often confused for supply effects. What looks like a supply problem—we think that women choose not to aspire to top positions or to jobs in top paying fields—might actually be a demand problem—organizations or jobs look unappealing to women because of past histories of not hiring or promoting women into leadership roles or of making work–life balance appear to be impossible. These studies suggest that essentialist explanations that attribute gendered outcomes to inherent characteristics or choices of women might be too simplistic or inaccurate. Instead, future research would benefit from examining the complex interactions between supply-side and demand-side drivers of gender inequality."
/doi/10.1287/orsc.2013.0853," Should responsibility for strategic planning and execution be assigned to the same manager? Should a firm have a chief operating officer with responsibilities distinct from those of the chief executive officer? How does the division of labor affect managerial opportunism? This paper uses a formal agency-theoretic model to address these questions and present a new theory of the division of managerial labor. Building on Penrose’s typology [ Penrose E (2009) The Theory of the Growth of the Firm , 4th ed. (Blackwell, Oxford, UK)], the theory identifies when to assign (i) entrepreneurial services, which relate to strategic planning and the acquisition of resources, and (ii) managerial services, which relate to execution, to the same generalist manager rather than to different specialists. The analysis reveals the critical importance of separability, i.e., whether a supervisor can separately observe the outcomes of entrepreneurial and managerial services. If managers and their supervisor have symmetric information about separability, hiring a generalist dominates because managerial services, which are easier to assess, reduce the scope for opportunism associated with entrepreneurial services, which are harder to assess. Conversely, if managers have better information regarding separability and the probability of separability is low, hiring specialists dominates because hiring a generalist allows the potential for opportunism associated with entrepreneurial services to contaminate the provision of managerial services. Even so, the benefits of hiring a generalist may be restored if the services are sequenced appropriately. An implication of such sequencing is that a firm will grow in fits and starts, giving rise to a “Penrose effect” even if labor market frictions do not impede the assimilation of new managers."
/doi/10.1287/orsc.1110.0718," As the rate of innovation increases, organizational environments are becoming faster and more complex, posing greater challenges for organizations to adapt. This study argues that the concept of coevolution offers a bridge between the prescient adaptationist and ex post selectionist perspectives of organizational change to account for the increasing rates of change. The mutual causal influences in a coevolutionary relationship help explain why competing sets of firms or individual firms can capture dominant shares in product markets. Using a comparative historical method and drawing on evidence from five countries over a 60-year period, this paper inquires how precisely coevolutionary processes work in shaping the evolution of industries and important features of their environments. It identifies—in the context of the synthetic dye industry—three causal mechanisms (exchange of personnel, commercial ties, and lobbying) and suggests how they acted as levers on the fundamental mechanisms of evolution. Understanding the levers is important for managing change in a world that is increasingly becoming coevolutionary, requiring managers to focus more on the emergent, system-level properties of their environments."
/doi/10.1287/moor.2016.0818," We consider the portfolio decision problem of a risky investor. The investor borrows at a rate higher than his lending rate and invests in a risky bond whose market price is correlated with the credit quality of the investor. By viewing the concave drift of the wealth process as a continuous function of the admissible control, we characterize the optimal strategy in terms of a relation between a critical borrowing threshold and solutions of a system of first-order conditions. We analyze the nonlinear dynamic programming equation and prove the singular growth of its coefficients. Using a truncation technique relying on the locally Lipschitz continuity of the optimal strategy, we remove the singularity and show the existence and uniqueness of a global regular solution. Our explicit characterization of the strategy has direct financial implications: it indicates that the investor purchases a high number of bond shares when his borrowing costs are low and the bond sufficiently safe, and reduces the size of his long position or even sells short when his financing costs are high or the bond very risky."
/doi/10.1287/mnsc.1120.1536," Online commercial interactions have increased dramatically over the last decade, leading to the emergence of networks that link the electronic commerce landing pages of related products to one another. Our paper conjectures that the explicit visibility of such “product networks”can alter demand spillovers across their constituent items. We test this conjecture empirically using data about the copurchase networks and demand levels associated with more than 250,000 interconnected books offered on Amazon.com over the period of one year while controlling for alternative explanations of demand correlation using a variety of approaches. Our findings suggest that on average the explicit visibility of a copurchase relationship can lead to up to an average threefold amplification of the influence that complementary products have on each others' demand levels. We also find that newer and more popular products “use” the attention they garner from their network position more efficiently and that diversity in the sources of spillover further amplifies the demand effects of the recommendation network. Our paper presents new evidence quantifying the role of network position in electronic markets and highlights the power of basing (virtual) shelf position on consumer preferences that are explicitly revealed through shared purchasing patterns. This paper was accepted by Pradeep Chintagunta, marketing."
/doi/10.1287/mnsc.1060.0695," Firms in many industries experience protracted periods of pricing power , the ability to successfully enact price increases. In these situations, firms must decide not only whether to raise prices, but to whom. Specifically, in a competitive context, they must determine whether it is more profitable to increase prices across-the-board or to a specific segment of their customer base. While selective price decreases are ubiquitous in practice (e.g., better deals to potential new customers by phone carriers; better deals to current customers by various magazines), to our knowledge selective price increases are relatively rare. We illustrate the benefits of targeted price increases, and, as such, we expand the repertoire of firms' promotional policies. To that end, we explore a scenario where two competing firms must decide whether to increase prices to the entire market or only to a specific segment. Targeted price increases (TPI), i.e., being offered an unchanged price (selectively) when others are subject to price increases, can be offered to Loyals (those who bought from the firm in the previous period) or Switchers (those who did not). The effects of TPIs are estimated through a laboratory experiment and an associated stochastic model, each allowing for both rational (Loyalty, Switching) and behaviorist (Betrayal, Jealousy) effects. We find that TPIs can indeed yield beneficial results (greater retention for Loyals or greater attraction of Switchers) and greater profits in certain circumstances. Results for TPI are additionally benchmarked against those for targeted price decreases and are found to differ. The range of effects stemming from the experiment can be used in a competitive analysis to yield equilibrium strategies for the two firms. In this case, we find that—depending on the magnitude of the price increase, market shares of the two firms, and price knowledge across consumer segments—a firm may wish to embrace targeted price increases in some situations, to institute across-the-board price increases in others, and to not enact any price increases in still others. We show that a firm can sacrifice considerable profit if it settles on a suboptimal pricing strategy (e.g., wrongly instituting an across-the-board increase), favors the wrong segment (e.g., Switchers instead of Loyals), or ignores “behaviorist” effects (Betrayal or Jealousy)."
/doi/10.1287/mnsc.2020.3626," We investigate whether and to what extent shareholder litigation shapes corporate innovation by examining the staggered adoption of universal demand laws in 23 states from 1989 to 2005. These laws impose obstacles against shareholders filing derivative lawsuits, thereby significantly reducing managers’ litigation risk. Using a difference-in-differences design and a matched sample, we find that, following the passage of the laws, firms invested more in research and development, produced more patents in new technological classes and more patents based on new knowledge, generated more patents with significant impacts, and achieved higher patent value. Our findings suggest that the external pressure imposed by shareholder litigation discourages managers from engaging in explorative innovation activities. This paper was accepted by David Simchi-Levi, finance."
/doi/10.1287/mnsc.2021.4053," We provide direct evidence of how dealers’ funding liquidity affects their liquidity provision in securities markets. Worse funding liquidity (higher repo haircuts and rates) leads to larger bid-ask spreads and transaction costs in corporate bonds. We also find that dealers’ relationships with money funds are important determinants of their repo haircuts and rates. Using dealers’ exposure to the 2016 Securities and Exchange Commission (SEC) money fund reform as an instrument, we show that funding liquidity indeed has a causal effect on market liquidity. Finally, dealers with lower funding liquidity tend to have smaller market shares and execute more trades on an agency basis. This paper was accepted by Haoxiang Zhu, finance."
/doi/10.1287/mksc.3.4.327," This paper examines the feasibility, practicality, and predictive ability of the consumer model which was proposed by Hauser and Shugan (Hauser, J. R., S. M. Shugan. 1983. Defensive marketing strategies. Marketing Sci. 2 (4, Fall) 319–360). We report results in two product categories, each representing over $100 million in annual sales. We develop “per dollar” perceptual maps and empirical consumer “taste” distributions. As a first test of the model, we compare the predictive ability of the consumer model in one category to (1) pretest market laboratory measurement models, (2) traditional perceptual mapping procedures, (3) a hybrid model using price as an attribute, and (4) actual market shares in test market cities. In the second product category, we illustrate the application of the quantitative model to augment managerial judgment. Besides developing an empirical version of the “Defender” consumer model, our analyses raise a number of behavioral hypotheses worth further investigation."
/doi/10.1287/opre.2019.1958," We study a competitive partial equilibrium in markets where risk-averse agents solve multistage stochastic optimization problems formulated in scenario trees. The agents trade a commodity that is produced from an uncertain supply of resources. Both resources and the commodity can be stored for later consumption. Several examples of a multistage risked equilibrium are outlined, including aspects of battery and hydroelectric storage in electricity markets, distributed ownership of competing technologies relying on shared resources, and aspects of water control and pricing. The agents are assumed to have nested coherent risk measures based on one-step risk measures with polyhedral risk sets that have a nonempty intersection over agents. Agents can trade risk in a complete market of Arrow-Debreu securities. In this setting, we define a risk-trading competitive market equilibrium and establish two welfare theorems. Competitive equilibrium will yield a social optimum (with a suitably defined social risk measure) when agents have strictly monotone one-step risk measures. Conversely, a social optimum with an appropriately chosen risk measure will yield a risk-trading competitive market equilibrium when all agents have strictly monotone risk measures. The paper also demonstrates versions of these theorems when risk measures are not strictly monotone."
/doi/10.1287/mnsc.2021.4066," We examine a sample of dual-class firms to isolate the magnitude and duration of the demand-driven price effect from stock repurchases. In this novel setting, the non-repurchased class serves as a near-perfect counterfactual to the repurchased class and controls for private information about firm value contained in the repurchases. The average repurchase in our sample, 0.30% of outstanding shares within a month, increases the stock price by 40 to 70 basis points relative to the non-repurchased class of stock. The effect dissipates completely over the subsequent month unless extended by continued repurchases. This small, short-lived price effect leaves little scope for CEOs to benefit from value-destroying repurchases motivated by self-interest. This paper was accepted by Victoria Ivashina, finance."
/doi/10.1287/inte.8.3.91," There is an old saying, “War is too important to be left to the Generals.” To this I would add, “World models are too important to be left to the model builders.” This does not mean that the burden of the model building effort should not be borne by operations researchers and modeling experts. However, as the ultimate stakes are so terribly high, I do mean that the model builders should not do it alone. The possible survival of mankind is simply too much responsibility for one small segment of society to bear. Consequently, any policy model which is constructed needs a socially recognized guarantor. The data and assumptions of a model must be warranted and secured by some person, process or belief. This is an awesome responsibility to be assumed by a lonely model-builder. It is a responsibility that I believe must be shared by the body politic on whose behalf the model is constructed."
/doi/10.1287/opre.2020.2079," We consider two firms selling products to a market of network-connected customers. Each firm is selling one product, and the two products are substitutable. The customers make purchases based on the multinomial logit model, and the firms compete for their purchasing probabilities. We characterize possible Nash equilibria for homogeneous network interactions and identical firms: When the network effects are weak, there is a symmetric equilibrium that the two firms evenly split the market; when the network effects are strong, there exist two asymmetric equilibria additionally, in which one firm dominates the market; interestingly, when the product quality is low and the network effects are neither too weak nor too strong, the resulting market equilibrium is never symmetric, although the firms are ex ante symmetric. We extend these results along multiple directions. First, when the products have heterogeneous qualities, the firm selling inferior product can still retain market dominance in equilibrium due to the strong network effects. Second, when the network effects are heterogeneous, customers with higher social influences or larger price sensitivities are more likely to purchase either product in the symmetric equilibrium. Third, when the network consists of two communities, market segmentation may arise. Fourth, we extend to the dynamic game when the network effects build up over time to explain the first-mover advantage."
/doi/10.1287/msom.3.3.242.9888," We analyze the problem of determining inventory and pricing decisions in a two-period retail setting when an opportunity to refine information about uncertain demand is available. The model extends the newsvendor problem with pricing by allowing for multiple suppliers, the pooling of procurement resources, and more general informational dynamics. One contribution is the solution procedure: We show that all decisions (up to 7 in all, including recourse decisions) can be determined uniquely as a function of a surrogate first-period decision called the stocking factor. Hence, the two-period decision problem with recourse reduces to a search for one decision variable. A second contribution is the policy implications: We find that the cost of learning is (1) a consequence of censored information because, on the margin, learning is free if full information is guaranteed; (2) measured in the form of an increased stocking factor; and (3) shared with the consumer in the form of a higher selling price when demand uncertainty is additive. A third contribution is the application of the results to three motivating examples: a market research problem in which a product is introduced in a test market prior to a widespread launch; a global newsvendor problem in which a seasonal product is sold in two different countries with nonoverlapping selling seasons; and a minimum-quantity commitment problem in which procurement resources for multiple purchases may be pooled."
/doi/10.1287/mnsc.1110.1489," Consumers become satiated with a product when purchasing too much too quickly. How much is too much and how quickly is too quickly depends on the characteristics of the product relative to the time interval between consumption periods. Knowing that, consumers allocate their budget to products that generate less satiation effects. Retailers should then choose to sell products that induce minimal satiation, but usually this is operationally more costly. To study this trade-off, we provide an analytical model based on utility theory that relates customer consumption to price and satiation, in the context of multiple competing retailers. We determine the purchasing pattern over time and provide an explicit expression to determine the consumption level in steady state. We derive market shares and show that they take the form of an attraction model in which the attractiveness depends on price and product satiation. We use this to analyze the competition between firms that maximize long-term average profit. We characterize the equilibrium under three scenarios: (i) price-only competition, (ii) product-only competition, and (iii) price and product competition. The results reveal the interplay between a key marketing lever (price) and the firm's ability to offer products that generate less satiation. In particular, we show that when a firm becomes more efficient at reducing satiation, its competitor may benefit if competition is on product only, but not if it is on price and product. We also find that when satiation effects are not managed, a firm's profit may be significantly reduced while a strategic competitor can largely benefit. This paper was accepted by Yossi Aviv, operations management."
/doi/10.1287/inte.2014.0766," Many end-stage renal disease sufferers who require a kidney transplant to prolong their lives have a relative or friend who has volunteered to donate a kidney to them, but whose kidney is incompatible with the intended recipient. This incompatibility can sometimes be overcome by exchanging a kidney with another incompatible patient-donor pair. Such kidney exchanges have emerged as a standard mode of kidney transplantation in the United States. The Alliance for Paired Donation (APD) developed and implemented nonsimultaneous extended altruistic donor (NEAD) chains, an innovative technique that allows a previously binding constraint (of simultaneity) to be relaxed; thus, it permits longer chains and better-optimized matching of potential donors to patients, greatly increasing the number of possible transplants. Since 2006, the APD has saved more than 220 lives through its kidney exchange program, with more than 75 percent of these achieved through nonsimultaneous chains. Other kidney exchange programs have adopted the technology and methods pioneered by APD, resulting in more than 1,000 lives already saved, with the promise of increasing impact in coming years. In 2013, the percentage of transplants from nonsimultaneous chains reached more than six percent of the number of transplants from living donors. In this paper, we describe the long-term optimization and market design research that supports this innovation. We also describe how a team of physicians and operations researchers worked to overcome the skepticism and resistance of the medical community to the NEAD innovation."
/doi/10.1287/mnsc.2019.3312," Most investor coordination remains undisclosed. I provide empirical evidence on the extent and consequences of investor coordination in the context of hedge fund activism, in which potential benefits and costs from coordination are especially pronounced. In particular, I examine whether hedge fund activists orchestrate “wolf packs”—that is, groups of investors willing to acquire shares in the target firm before the activist’s campaign is publicly disclosed via a 13D filing—as a way to support the campaign and strengthen the activist’s bargaining position. Using a novel hand-collected data set, I develop a method to identify the formation of wolf packs before the 13D filing. I investigate two competing hypotheses: the Coordinated Effort Hypothesis (wolf packs are orchestrated by lead activists to circumvent securities regulations about “groups” of investors) and the Spontaneous Formation Hypothesis (wolf packs spontaneously arise because investors independently monitor and target the same firms at about the same time). A number of tests rule out the Spontaneous Formation Hypothesis and provide support for the Coordinated Effort Hypothesis . Finally, the presence of a wolf pack is associated with various measures of the campaign’s success. This paper was accepted by Brian Bushee, accounting."
/doi/10.1287/inte.2018.0984," In the biomanufacturing industry, production and planning decisions are often challenging because of batch-to-batch variability and uncertainty in the production yield, quality, cost, and lead times. To improve biomanufacturing efficiency, a multidisciplinary team of researchers collaborated over five years to develop a portfolio of decision support tools. The tools developed provide a data-driven, operations research–based approach to reduce biomanufacturing costs and lead times. These decision support tools comprise multiple deterministic and stochastic optimization models to optimize production and planning decisions. To optimize production decisions related to fermentation and protein purification, optimization tools were developed to provide a decision support mechanism that links the underlying biological and chemical processes with business risks and financial trade-offs. To optimize planning decisions, interactive scheduling and capacity planning tools were developed to enable efficient use of expensive and limited resources. Although developed in collaboration with Aldevron, these tools address common industry challenges, and they have been shared with a wider industry community through working group sessions."
/doi/10.1287/serv.2.1_2.126," [Modified excerpt] Systems theory is an interdisciplinary theory about every system in nature, in society and in many scientific domains as well as a framework with which we can investigate phenomena from a holistic approach. Systems thinking comes from the shift in attention from the part to the whole , considering the observed reality as an integrated and interacting unicuum of phenomena where the individual properties of the single parts become indistinct. In contrast, the relationships between the parts themselves and the events they produce through their interaction become much more important, with the result that system elements are rationally connected towards a shared purpose. The systemic perspective argues that we are not able to fully comprehend a phenomenon simply by breaking it up into elementary parts and then reforming it; we instead need to apply a global vision to underline its functioning. Although we can start from the analysis of the elementary components of a phenomenon, in order to fully comprehend the phenomenon in its entirety we have to observe it also from a higher level: a holistic perspective. [ Service Science , ISSN 2164-3962 (print), ISSN 2164-3970 (online), was published by Services Science Global (SSG) from 2009 to 2011 as issues under ISBN 978-1-4276-2090-3.]"
/doi/10.1287/orsc.1120.0742," We provide an analysis of the costs and benefits of blockholding in Europe, where it is a dominant, but certainly not universal, corporate governance strategy for shareholders of publicly listed firms. We find that the effectiveness of blockholding is conditioned by the specific labor institutions that distinguish European countries from the rest of the world, and that these institutional effects involve both competition and cooperation between blockholders and collective labor interests. We also find that relational blockholders are better able to cope with, or benefit from, these institutional effects than arm's-length blockholders. Empirically, we use advanced meta-analytic methods on a total sample of 748,569 firm-year observations, derived from 162 studies covering 23 European countries."
/doi/10.1287/mnsc.2015.2314," The financial sector is unique in being largely self-governed: the majority of financial firms’ shares are held by other financial institutions. This raises the possibility that the monitoring of financial firms is especially undermined by conflicts of interest as a result of personal and professional links between these firms and their shareholders. To investigate this possibility, we scrutinize the aspect of the financial sector’s self-governance that is directly observable: mutual fund companies’ voting on their peers’ stocks. We find that considerations specific to investee firms’ membership in the same industry as their investors do indeed impact voting. This impact is in the direction of supporting the investee’s management. We show that the own-industry effect reduces director efficacy and lowers firm value as a result. We extend our analysis to other financial companies and show that they also tend to vote more favorably when it comes to their peers. Our results suggest that peer support is a corrupting factor in the financial sector’s governance. This paper was accepted by Wei Jiang, finance ."
