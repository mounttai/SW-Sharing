link,abstract
/doi/10.1287/opre.1040.0132," The bullwhip effect (amplification of order variance from a downstream stage in a supply chain to an upstream stage) is widely observed in practice, and is generally considered a major cause of supply chain inefficiencies. But are supply chains always better off with strategies that are designed to dampen the bullwhip effect? This paper considers a model where a single product is sold through multiple retail outlets. The retailers replenish their inventories from a factory, which in turn replenishes its own finished-goods inventory through production. The factory's production capacity is finite, and there are transportation economies of scale in replenishing the retailer inventories. We study two types of replenishment strategies that are widely used in practice, and show that a replenishment strategy that reduces the volatility of orders received by the factory does not necessarily reduce the total costs in the supply chain."
/doi/10.1287/mnsc.13.10.C207," Fifteen years ago hardly anyone had thought seriously about the need of organized management education in India. Soon after Independence in 1947 a high level Commission on Education was appointed by the Government of India with Dr. S. Radhakrishnan (the present President of India) as Chairman to make a comprehensive study of the various aspects of the educational system then existing and make recommendations for its modification for the new requirements of the nation. This commission however, had little to say on management education. But the formulation of a definite program for planned economic development soon after, brought about a complete change in the situation. The need for technically trained and competent administrative personnel became urgent. As a result, during the past one decade there has been an increasing awareness of the need for providing facilities of training in management."
/doi/10.1287/mnsc.2020.3789," We contribute to the growing literature on the effectiveness of corporate boards by examining the effect of two insights that have been largely unexplored in prior studies that use public data. First, since boards’ responsibilities are wide-ranging, more holistic performance measures may better capture the full range of their duties than specific public actions and outcomes (e.g., disclosure of risk management processes, financial restatements, acquisition returns, CEO turnover). And second, because corporate boards share many characteristics of other types of teams, their effectiveness is likely to be influenced by their internal operations. To examine the performance effects of these insights, we use data from 577 directors of U.S. public firms that responded to a survey we conducted in 2015–2016 and qualitative data from interviews of 75 directors. Our study establishes a strong relation between director perceptions of board performance effectiveness and internal board operations. Further, by highlighting the critical role of internal operations, identifying areas of relative strength and weakness in boards’ effectiveness in various activities, and probing director perceptions of their primary responsibilities, we are able to offer concrete suggestions for future research on board effectiveness. This paper was accepted by Shiva Rajgopal, accounting."
/doi/10.1287/orsc.1080.0363," Market mediation literature has been taking primarily a triadic view in studying the role and impact of mediators, actors that occupy a middle position, on supply and demand conditions in markets. Mediating organizations facilitate exchange relationships on a continuous basis between multiple networks of interdependent affiliated actors. An affiliation structure gives rise to the property of duality whereby the behavior and performance of the affiliates affect the behavior and performance of mediators and vice versa. In the context of the banking industry, this study examines how the structural properties of the underlying network of affiliated organizations affect the survival of affiliated organizations and the performance of mediating organizations. The results show that firms that are affiliated with mediating organizations having high customer-set connectedness or with those that are industry specialists increase their chances of survival. Furthermore, mediating organizations having high customer-set connectedness experience lower loan loss than those having lower customer-set connectedness. This study contributes to organizational studies by incorporating a network view to the predominately triadic market mediation literature, explicating the determinants of the effectiveness by which mediating services are rendered, and suggesting new sources of competitive advantage that advance as well as complement established explanations. Implications for the network perspective, organizational technology and the economic theory of the banking firm, as well as firm and mediator management are considered."
/doi/10.1287/mnsc.2020.3658," We propose a discrete-time affine pricing model that simultaneously allows for (i) the presence of systemic entities by departing from the no-jump condition on the factors’ conditional distribution, (ii) contagion effects, and (iii) the pricing of credit events. Our affine framework delivers explicit pricing formulas for default-sensitive securities such as bonds and credit default swaps (CDSs). We estimate a euro-area multicountry version of the model and address economic questions related to the pricing of sovereign credit risk. We find that both frailty (common factors) and contagion phenomena are important to account for the joint dynamics of credit spreads. Our results also provide evidence of credit-event pricing, which is at the source of substantial credit risk premiums, even for short maturities. Finally, we extract measures of depreciation-at-default from CDSs denominated in different currencies. This paper was accepted by Kay Giesecke, finance."
/doi/10.1287/mnsc.2014.2100," Entrepreneurial ventures rely not only on founders but also on “joiners”—start-up employees who are attracted to entrepreneurship, but who do not want to be founders themselves. Drawing on both preference and contextual theories of entrepreneurship, we examine how individuals’ interest in being a founder, a joiner, or neither forms prior to the first career transition. We find that although individuals with founder and joiner interests share similar preferences for entrepreneurial job attributes such as autonomy and risk, their preferences for these attributes also differ in significantly meaningful ways. Contextual factors such as norms, role models, and opportunities exhibit very different relationships with founder and joiner interests. Most interestingly, our results suggest that preferences and context interrelate in unique ways to shape different entrepreneurial interests. In particular, an interest in being a founder is most strongly associated with individuals’ preferences for entrepreneurial job attributes, whereas contextual factors do little to shape a founder interest in individuals who lack these preferences. An interest in being a joiner, on the other hand, is associated with both preferences and context, and this relationship is most pronounced for individuals with preferences that predispose them toward entrepreneurship. This study highlights joiners as a distinct type of entrepreneurial actor and demonstrates the importance of considering the interplay between preferences and context in the study of entrepreneurship. This paper was accepted by David Hsu, entrepreneurship and innovation ."
/doi/10.1287/orsc.1100.0627," We explore the overlooked issue of how certain strategic-level, interindustry diversification options might increase consumer utility. Discussions of inter -industry diversification typically focus on producer synergies obtainable from economies of scope or from skill transfer across business units. Discussions of intra -industry product diversification—generally, the province of marketing—typically focus on synergies obtainable from product bundling, which lowers producer costs or provides convenience for consumers. We take a different tack by linking interindustry diversification and consumer utility. We first separately examine two possible consumer benefits of interindustry diversification: (1) facilitating consumers' accomplishment of two tasks simultaneously or (2) attracting diverse consumer groups to a common platform when intergroup externalities exist. We then assess a simple empirical context that shows potential for simultaneous consumer utilities and two-sided market utility together. We analyze this context and concurrently develop a mathematical model showing how these demand-side synergies can create unique business value. We next introduce asymmetric preferences among consumer subgroups, and we refine our arguments by comparing their conclusions with the empirical data. We learn that combinations of otherwise mundane (i.e., commonplace) assets can create consumer value—“superior” assets are not necessary. Moreover, common ownership is necessary for the pricing flexibility required to deliver (and capture) maximum value through interindustry diversification, especially when consumer groups' preferences may change; the negotiations and settling up required for cooperation through alliances will, without common ownership, increase costs and reduce responsiveness. We discuss the sustainability of demand-side advantages and the implications of these ideas for future research and practice."
/doi/10.1287/mksc.1100.0576," Although advance selling and probabilistic selling differ in both motivation and implementation, we argue that they share a common characteristic—both offer consumers a choice involving buyer uncertainty. We develop a formal model to examine the general economics of purchase options that involve buyer uncertainty, explore the differences in buyer uncertainty created via these two strategies, and derive conditions under which one dominates the other. We show that the seller can address unobservable buyer heterogeneity by inducing sales involving buyer uncertainty via two different mechanisms: (1) homogenizing heterogeneous consumers and (2) separating heterogeneous consumers. Offering advance sales encourages customers to purchase while they are uncertain about their consumption states (more homogeneous), but offering probabilistic goods encourages customers to reveal their heterogeneity via self-selecting whether or not to purchase the uncertain product. The relative attractiveness of these two selling strategies depends on the degree of two types of buyer heterogeneity: (1) Max_Value-Heterogeneity , which is the variation in consumers' valuations for their preferred good, and (2) Strength-Heterogeneity , which is the variation in the strength of consumers' preferences. Neither strategy is advantageous unless the market exhibits sufficient Max_Value-Heterogeneity . However, whereas Strength-Heterogeneity can destroy the profit advantage of advance selling, a mid-range of Strength-Heterogeneity is necessary for probabilistic selling to be advantageous."
/doi/10.1287/orsc.9.3.285," Alliances are volatile key components of many corporations' competitive strategies. They offer fast and flexible means of achieving market access, scale economies, and competence development. However, strategic alliances can encounter difficulties that often lead to disappointing performance. The authors suggest that the way partners manage the collective learning process plays a central role in the success and failure of strategic alliances. Present understanding of interorganizational learning primarily focuses on how the individual organization can be a “good partner” or try to win the internal “race to learn” among the partners. The interorganizational learning dilemma is that (1) being a good partner invites exploitation by partners attempting to maximize their individual appropriation of the joint learning, and (2) such opportunistic learning strategies undercut the collective knowledge development in the strategic alliance. The authors develop a framework for understanding the dilemma through consideration of trade-offs between how collective learning is developed in alliances and how the joint learning outcomes are divided among the partners. They create a typology of five different learning strategies based on how receptive as well as how transparent an organization is in relation to its partners. The strategies are: collaboration (highly receptive and highly transparent); competition (highly receptive and nontransparent); compromise (moderately receptive and transparent); accommodation (nonreceptive and highly transparent); and avoidance (neither receptive nor transparent). Interorganizational learning outcomes are proposed to be the interactive results of the respective partners' type of adopted learning strategy. By synthesizing strategic alliance, organizational learning, collective action, and game theories, the framework contributes to understanding the variety in alliance development, performance, and longevity. Interorganizational learning is likely to be hindered by lack of either motivation or ability to absorb and communicate knowledge between the partner organizations. The dynamics of power, opportunism, suspicion, and asymmetric learning strategies can constitute processual barriers to collective knowledge development. In contrast, prior related interaction between the partners, high learning stakes, trust, and long-term orientation are likely to empower the collective learning process. Comparison of previous case studies and surveys of interorganizational learning provides partial empirical support for the proposed framework. The comparison also indicates several omissions in previous research, such as failure to consider either how receptive or how transparent the partners are, the interaction between their learning strategies, and their dynamic processes over time. Because these omissions are due partly to the methodological limitations of traditional case studies and crosssectional surveys, the authors suggest a bridging case survey design for a more comprehensive test of their interactive, dynamic, and situational framework."
/doi/10.1287/mnsc.2015.2190," Financial institutions use risk measures to calculate the marginal capital cost when expanding the exposure to a certain risk within their portfolio. We reverse this approach by calculating the marginal cost based on economic fundamentals for a profit-maximizing firm and then by identifying the risk measure delivering the correct marginal cost. The resulting measure depends on context. Whereas familiar measures can be recovered in some circumstances, other circumstances yield unfamiliar forms. In all cases, the risk preferences of the institution’s claimants determine how the correct risk measure must weight various default states. Our results demonstrate that risk measures used for pricing and performance measurement should be chosen based on economic fundamentals and may not necessarily adhere to the mathematical properties typically imposed in the literature. This paper was accepted by Jerome B. Detemple, finance ."
/doi/10.1287/mnsc.2019.3333," We analyze the design and impact of bank regulation using a dynamic structural framework. The optimal regulatory policy combines a target capital requirement, the mitigation of underinvestment, an intervention capital requirement to control inefficient risk taking, and recapitalization of distressed banks. The optimal target and intervention capital requirements from our structural estimation are consistent with the substantially higher capital requirements proposed in Basel III and together achieve most of the regulatory benefits by alleviating underinvestment and asset substitution. They are interdependent and respond differently to banks’ asset characteristics, thereby suggesting that regulatory policies should be carefully tuned to the economic environment. This paper was accepted by Kay Giesecke, finance."
/doi/10.1287/mnsc.2021.4166," Policy makers in many developing countries use maximum price or markup policies to control pharmaceutical costs, which represent 20%–60% of their overall healthcare expenditure. We study the price effect of price ceiling policies by exploiting a major policy shift in China: the elimination of longstanding ceilings on retail drug prices. We collect weekly price and characteristics data on more than 4,500 drug stock keeping units (SKUs) from a leading pharmacy chain. By comparing the rate of discontinuous price jumps across drugs with and without price ceilings during the years before and after the policy change, we find that while price ceilings are effective in containing the prices of some drugs, they can lead to higher prices for others, particularly if the ceilings are set at the national level irrespective of local economic conditions. About 5% of nationally controlled drugs (or more than 125 drugs) had inflated prices because of price ceilings, with an average price inflation of 10%. We attribute this perverse price effect to focal point pricing and asymmetric information about production costs. Further supporting this view, we find the perverse price effect most prominent in lower-income regions where the centrally set price ceilings are arbitrarily high considering their poorer economic conditions. Moreover, drugs with highly concentrated production and less elastic demand face heightened risks of inflated prices under price ceilings. Finally, based on a sample of drugs with available price ceiling data, we find that drugs with manufacturer-specific ceilings are 100% more likely to be priced at or near their ceilings and 70% more likely to experience price drops once the ceilings are removed compared with other drugs with regular ceilings. Overall, this paper documents the unintended perverse effect of price ceilings in pharmaceutical markets and sheds lights on the ongoing debate of drug price regulation. This paper was accepted by Stefan Scholtes, healthcare management."
/doi/10.1287/orsc.1050.0137," This paper argues that research in organization theory has seen a shift in orientation from paradigm-driven work to problem-driven work since the late 1980s. A number of paradigms for the study of organizations were elaborated during the mid-1970s, including transaction cost economics, resource dependence theory, organizational ecology, new institutional theory, and agency theory in financial economics. These approaches reflected the dominant trends of the large corporations of their time: increasing concentration, diversification, and bureaucratization. However, subsequent shifts in organizational boundaries, the increased use of alliances and network forms, and the expanding role of financial markets in shaping organizational decision making all make normal science driven by the internally derived questions from these paradigms less fruitful. Instead, we argue that problem-driven work that uses mechanism-based theorizing and research that takes the field rather than the organization as the unit of analysis are the most appropriate styles of organizational research under conditions of major economic change—such as our own era. This sort of work is best exemplified by various studies under the rubric of institutional theory in the past 15 years, which are reviewed here."
/doi/10.1287/stsc.2018.0057," Multinational corporations seeking to overcome the “liability of foreignness” through partnerships with stakeholders in their overseas markets face an important tension in choosing between two alternative strategies. Firms who build relationships with high-status local stakeholders may experience increased cooperation and reduced conflict with other stakeholders. However, the short-term benefits of this status climbing strategy may be difficult to sustain because stakeholders with low-status, particularly those outside the formal political structure, are more likely to increase their conflict as the firm becomes increasingly distant from them. An alternative strategy of bridging structural holes in the host country stakeholder network also faces theoretical limits as access to scarce information and resources is likely to engender conflict with other stakeholders. The status climbing strategy is more attractive where insider connections dominate (i.e., the rule of law is weak) while the bridging strategy is more attractive in countries with stronger rule of law. In other words, we show that as the rule of law strengthens, multinational entrants focus more on what they know rather than whom they know. Our analyses use a hand-coded data set of almost 52,000 media-reported stakeholder interactions that capture the network of relationships among 4,623 stakeholders from 19 gold-mining companies operating 26 mines in 20 countries, and the evolution of these networks from 1993 until 2008."
/doi/10.1287/serv.2015.0123," We explore the role of service coproduction factors, including service level, service variety, and operations-centered and customer-centered cost, in fostering innovation through the effective use of learning-by-doing (LBD) in a customer-facing service context. We hypothesize that service organizations can simultaneously improve along these factors through learning-by-doing. We develop a unique panel data set in the U.S. hotel industry from 2001 to 2011 to test our hypothesis. We find strong direct effects on performance from these coproduction factors. Namely, profitability is negatively associated with operations-centered cost but positively associated with increases in service level, variety, and customer-centered cost. Furthermore, gains from learning-by-doing are higher when operations-centered cost is high or when service level, service variety, and customer-centered cost are low. These findings suggest that service coproduction factors can enhance learning-by-doing simultaneously but their economic impact is subject to certain profitability trade-offs. We close the article by offering theory and managerial implications of the simultaneous learning possibilities and allied profitability trade-offs."
/doi/10.1287/mnsc.2014.1916," The transportation sector's carbon footprint and dependence on oil are of deep concern to policy makers in many countries. Use of all-electric drive trains is arguably the most realistic medium-term solution to address these concerns. However, motorist anxiety induced by an electric vehicle's limited range and high battery cost have constrained consumer adoption. A novel switching-station-based solution is touted as a promising remedy. Vehicles use standardized batteries that, when depleted, can be switched for fully charged batteries at switching stations, and motorists only pay for battery use. We build a model that highlights the key mechanisms driving adoption and use of electric vehicles in this new switching-station-based electric vehicle system and contrast it with conventional electric vehicles. Our model employs results from repairable item inventory theory to capture switching-station operation; we embed this model in a behavioral model of motorist use and adoption. Switching-station systems effectively transfer range risk from motorists to the station operator, who, through statistical economies of scale, can better manage it. We find that this transfer of risk can lead to higher electric vehicle adoption than in a conventional system, but it also encourages more driving than a conventional system does. We calibrate our models with motorist behavior data, electric vehicle technology data, operation costs, and emissions data to estimate the relative effectiveness of the two systems under the status quo and other plausible future scenarios. We find that the system that is more effective at reducing emissions is often less effective at reducing oil dependence, and the misalignment between the two objectives is most severe when the energy mix is coal heavy and has advanced battery technology. Increases in gasoline prices (by imposition of taxes, for instance) are much more effective in reducing carbon emissions, whereas battery-price-reducing policy interventions are more effective for reducing oil dependence. Taken together, our results help a policy maker identify the superior system for achieving the desired objectives. They also highlight that policy makers should not conflate the dual objectives of oil dependence and emissions reductions as the preferred system, and the policy interventions that further that system may be different for the two objectives. This paper was accepted by Yossi Aviv, operations management ."
/doi/10.1287/mnsc.1100.1227," This paper presents a case study of the impact of manufacturing offshore on technology competitiveness in the optoelectronics industry. It examines a critical design/facility location decision being faced by optoelectronic component manufacturers. This paper uses a combination of simulation modeling and empirical data to demonstrate the economic constraints facing these firms. The results show that production location changes the relative production economics of the two competing designs—one emerging, one prevailing—that are currently perfect substitutes for each other on the telecom market, but not necessarily perfect substitutes in other markets in the long term. Specifically, if optoelectronic component firms shift production from the United States to countries in developing East Asia, the emerging designs that were developed in the United States no longer pay. Production characteristics are different abroad, and the prevailing design can be more cost effective in developing country production environments. The emerging designs, however, have performance characteristics that may be valuable in the long term to the larger computing market and to pushing forward Moore's law. This paper concludes by exploring the dilemma this creates for the optoelectronic component manufacturers and recommending a framework based on which the results may be generalized to other industries."
/doi/10.1287/trsc.2015.0598," We consider planning and design of an extended supply chain for bioenergy networks (i.e., networks for multibiomass as well as biofuel logistics) in an integrated fashion while simultaneously addressing strategic and tactical decisions pertaining to location, production, inventory, and distribution in a multiperiod planning horizon setting. In our modeling, we also explicitly incorporate realistic operational parameters, including biomass deterioration rates and transportation economies of scale. For an efficient solution of our model, we suggest a Benders decomposition–based algorithm that can handle realistic size problems for design and analysis purposes. We implement the approach in a particular way using callback functions, in which the master problem is solved only once; we also develop surrogate constraints for enhanced lower bounds to obtain improved convergence especially for large instances. We provide computational results that demonstrate the efficiency of the solution approach on a wide-ranging set of problem instances. Furthermore, we develop a realistic case using data pertaining to the state of Texas and conduct an extensive analysis on the effects of varying input parameters on the design outcomes for a bioenergy supply chain network."
/doi/10.1287/orsc.9.1.16," Analyses of the events that occur in the context of organization process are rapidly advancing. Scholars holding otherwise disparate views share the sense that social actors, including organizations, attend to, interpret, and act upon events. Analyses of events are converging from two theoretical and methodological starting points. Analyses that emphasize human subjectivity and contextual specificity are seeking increased cross-situational learning. Nomothetic analyses are building on their strength in cross-situational learning by striving to represent the way subjects themselves construct events in relation to context. Rather than continuing to analyze classic organizational and environmental dimensions like formalization, general uncertainty, munificence, and stability, scholars are increasingly analyzing the qualities of events and the meanings they are given. They are treating events as elements that social actors abstract from social processes, and social actors as parties who interact to give events meaning. The present paper defines event analyzes its origins and current uses, and indicates how using and going beyond lessons from physics can promote organization studies. These lessons come from the analysis of physical events as particles in relation to waves, fields, and perspectives. The uniquely social element of potential takes us beyond the experience of physical science."
/doi/10.1287/mnsc.2013.1748," This paper brings structural modeling to the literature on financial research in marketing. I propose a dynamic investment-based model to understand the impact of advertising expenditures on stock returns and firm value. In addition, by interpreting advertising expenditures as an investment in brand capital, the approach in this paper provides a novel way to measure brand equity grounded in economic theory. Using the Euler equations from the firm's maximization problem, I derive closed-form expressions for the firm's equilibrium stock returns and market value, which depend on observable firm characteristics. I test the model's predictions by the generalized method of moments and data from a large cross section of publicly traded firms. The model is able to match simultaneously the pattern of average stock returns and firm values of portfolios sorted on advertising expenditures that standard asset pricing models cannot. The estimation results also show that brand equity accounts for a substantial fraction of firm market value (about 23%), and that this value varies substantially across industries. Implications of the findings for research at the intersection of marketing and finance are discussed. This paper was accepted by Wei Xiong, finance."
/doi/10.1287/mksc.2015.0954," This paper addresses the repositioning of Kmart Australia in 2011. It shows how by calibrating emotional as well as cognitive reactions and estimating their impact on purchase intentions, Kmart was able to focus its communications, improving market share. We measure nine key emotions, ranging from surprise to anger. Including these emotions significantly improves our model for likelihood to choose a store. Measuring emotions enabled Kmart’s advertising agency to create a television commercial that tapped into the specific emotions that most strongly predict the likelihood to choose a store; that is, the model drove the development of the advertising creative. The resulting television commercial tested well and was effective when launched. At the individual level, cognitions and emotions changed dramatically. At the aggregate level, an econometric model showed that store visits were significantly enhanced. Kmart’s EBIT (earnings before interest and tax) increased by 30%, whereas Kmart’s main rival had almost no EBIT growth, despite vigorous attempts to counter Kmart’s campaign. One of our key contributions is to incorporate emotions into marketing science models of evaluation and purchase intentions. We also provide a new methodology to measure emotions. The approach enables marketing science to participate in the design of marketing stimuli, rather than just testing preexisting ones. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0954 ."
/doi/10.1287/ijoc.2017.0752," We study a problem that integrates buy-at-bulk network design into the classical facility location problem. We consider a generalization of the facility location problem where multiple clients may share a capacitated network to connect to open facilities instead of requiring direct links. In this problem, we wish to open facilities, build a routing network by installing access cables of different costs and capacities, and route every client demand to an open facility. We provide a path-based formulation and we compare it with the natural compact formulation for this problem. We then design an exact branch-price-and-cut algorithm for solving the path-based formulation. We study the effect of two families of valid inequalities. In addition to this, we present three different types of primal heuristics and employ a hybrid approach to effectively combine these heuristics in order to improve the primal bounds. We finally report the results of our approach that were tested on a set of real world instances, as well as two sets of benchmark instances and evaluate the effects of our valid inequalities and primal heuristics."
/doi/10.1287/serv.3.3.223," This article addresses a relevant question arising in knowledge-intensive industries: modelling productivity ; an important issue of increasing importance in developed economies, (Drucker [Drucker, P. 1999. Knowledge-worker productivity: the biggest challenge. California Management Review .], Neely [Neely, A. 2002. Business Peformance Measurement: Theory and Practice . Cambridge University Press.], Ramirez [Ramirez, et al. 2004. Measuring knowledge worker productivity. Journal of Intellectual Capital .]). According to the findings presented in this paper, service science and related service dominant logic paradigm, is a useful framework upon which to understand and further operationalize productivity in complex servicings . The research approach adopted combines relevant theory in service science and productivity analysis with a real experience as practiced by a worldwide leading healthcare agency to test the validity of the concepts proposed. [ Service Science , ISSN 2164-3962 (print), ISSN 2164-3970 (online), was published by Services Science Global (SSG) from 2009 to 2011 as issues under ISBN 978-1-4276-2090-3.]"
/doi/10.1287/mnsc.2021.4147," In social trading, less experienced investors (followers) are allowed to copy the trades of experts (traders) in real time after paying a fee. Such a copy-trading mechanism often runs into a transparency-revenue tension. On the one hand, social trading platforms need to release traders’ trades as transparently as possible to allow followers to evaluate traders accurately. On the other hand, complete transparency may undercut the platform’s revenue because followers can free ride. That is, followers can manually copy the trades of a trader to avoid paying the following fees. This study addresses this critical tension by optimizing the level of transparency through delaying the release of trading information pertaining to the trades executed by traders. We capture the economic impact of the delay using the notions of profit-gap and delayed-profit . We propose a mechanism that elucidates the economic effects of the profit-gap and delayed-profit on followers and, consequently, the amount of money following a trader: protection effect and evaluation effect. Empirical investigations find support for these two effects. We then develop a stochastic control formulation that optimizes platform revenue, where the control is the optimal delay customized at the trader level and calculated as a function of the current amount of money following a trader and the number of views on the trader’s profile page. The optimized revenue can be incorporated into an algorithm to provide a systematic way to infuse the platform’s goals into the ranking of the traders. A counterfactual study is conducted to demonstrate the performance of the optimal delay policy (versus a constant-delay policy) using data from a leading social trading platform operating in the foreign exchange market. This paper was accepted by Chris Forman, information systems."
/doi/10.1287/mnsc.1060.0673," Neoclassical models of strategic behavior have yielded many insights into competitive behavior, despite the fact that they often rely on a number of assumptions—including instantaneous market clearing and perfect foresight—that have been called into question by a broad range of research. Researchers generally argue that these assumptions are “good enough” to predict an industry’s probable equilibria, and that disequilibrium adjustments and bounded rationality have limited competitive implications. Here we focus on the case of strategy in the presence of increasing returns to highlight how relaxing these two assumptions can lead to outcomes quite different from those predicted by standard neoclassical models. Prior research suggests that in the presence of increasing returns, tight appropriability, and accommodating rivals, in some circumstances early entrants can achieve sustained competitive advantage by pursuing “get big fast” (GBF) strategies: Rapidly expanding capacity and cutting prices to gain market share advantage and exploit positive feedbacks faster than their rivals. Using a simulation of the duopoly case we show that when the industry moves slowly compared to capacity adjustment delays, boundedly rational firms find their way to the equilibria predicted by conventional models. However, when market dynamics are rapid relative to capacity adjustment, forecasting errors lead to excess capacity—overwhelming the advantage conferred by increasing returns. Our results highlight the risks of ignoring the role of disequilibrium dynamics and bounded rationality in shaping competitive outcomes, and demonstrate how both can be incorporated into strategic analysis to form a dynamic, behavioral game theory amenable to rigorous analysis."
/doi/10.1287/mnsc.2014.1909," Statistical aggregation is often used to combine multiple opinions within a group. Such aggregates outperform individuals, including experts, in various prediction and estimation tasks. This result is attributed to the “wisdom of crowds.” We seek to improve the quality of such aggregates by eliminating poorly performing individuals from the crowd. We propose a new measure of contribution to assess the judges' performance relative to the group and use positive contributors to build a weighting model for aggregating forecasts. In Study 1, we analyze 1,233 judges forecasting almost 200 current events to illustrate the superiority of our model over unweighted models and models weighted by measures of absolute performance. In Study 2, we replicate our findings by using economic forecasts from the European Central Bank and show how the method can be used to identify smaller crowds of the top positive contributors. We show that the model derives its power from identifying experts who consistently outperform the crowd. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2014.1909 . This paper was accepted by James Smith, decision analysis ."
/doi/10.1287/opre.2018.1828," Consumers often do not have complete information about the choices they face and, therefore, have to spend time and effort acquiring information. Because information acquisition is costly, consumers trade off the value of better information against its cost and make their final product choices based on imperfect information. We model this decision using the rational inattention approach and describe the rationally inattentive consumer’s choice behavior when the consumer faces alternatives with different information costs. To this end, we introduce an information cost function that distinguishes between direct and implied information. We then analytically characterize the optimal choice probabilities. We find that nonuniform information costs can have a strong impact on product choice, which gets particularly conspicuous when the product alternatives are otherwise very similar. There are significant implications on how a seller should provide information about its products and how changes to the product set impacts consumer choice. For example, nonuniform information costs can lead to situations in which it is disadvantageous for the seller to provide easier access to information for a particular product and to situations in which the addition of an inferior (never chosen) product increases the market share of another existing product (i.e., failure of regularity). We also provide an algorithm to compute the optimal choice probabilities and discuss how our framework can be empirically estimated from suitable choice data."
/doi/10.1287/inte.1090.0448," Shermag Inc. is a vertically integrated furniture company with business units across the supply chain from the forest to the final customer. During the last decade, Shermag has been losing market share to low-cost Asian manufacturers. To reduce the procurement and other significant costs of Shermag's raw material (wood), which constitute a major component of its total furniture cost, we developed a tool to optimize the tactical planning of the company's wood supply chain. We propose an optimization-based approach for coordinating operations at each echelon of the wood supply chain. However, the problem size caused computer-related issues, such as long processing times and computer crashes. In our proposed solution approach, we use decomposition to overcome these issues. Our implementation uses C++, CPLEX (optimization software), and Microsoft Access. In this paper, we present a comparative study of traditional decision making versus optimal decision making. Using Shermag data for 2004 and 2005, we show that our solution reduces total operations costs by more than 22 percent. For any set of parameters, the tool can generate a good, feasible solution. These results convinced Shermag to use our tool for future configurations of its supply chain network."
/doi/10.1287/isre.2019.0843," Although Fogg’s ideas of persuasive technologies are widely accepted, few attempts have been made to test his ideas, particularly in a team context. In this article, we (1) theoretically extend Fogg’s ideas by identifying contexts in which virtual teams are more likely to use persuasive technologies; (2) empirically measure technology visualness, a factor that likely makes technologies more or less persuasive; and (3) assess the association between the use of persuasive technologies, judgment shifts, and forecast performance in a real-world virtual team context. We identify visual representation technologies (VRTs) as a class of technologies used by virtual teams to select, transform, and present data in a rich visual format. We propose that such technologies play a persuasive, as well as diagnostic, role in virtual team decisions. Over a three-year period, we examine the daily chat room discussions and decisions of a virtual team that makes smog forecasts with large economic and health consequences. We supplement regression models of field data with an experiment, interviews with team members, and analyses of imagery processing and group cohesion in team language use. Experiment results show that, relative to non-VRTs, the use of a VRT in a forecasting task increases imagery processing. Field data results show that team members increase their use of VRTs during chat room discussions when initial team consensus is low and the environment is more exacting. Greater use of VRTs in team discussions relates to greater shifts in the initial to final consensus forecasts of the team and greater odds of the team shifting its forecast policy to issue a smog alert. Increased use of VRTs is associated with lower forecast bias but is not significantly associated with forecast accuracy. VRT use is also associated with greater imagery processing and increased group cohesion, as shown through language use."
/doi/10.1287/mnsc.1030.0152," How does competition among economic actors determine the value that each is able to appropriate? We provide a formal, general framework within which this question can be posed and answered, and then provide several results. Chief among them is a condition that is both required for, and guarantees, value appropriation. We apply our methodology to (i) assess the familiar notion that uniqueness, inimitability, and competition imply value appropriation, and (ii) determine the value appropriation possibilities for an innovator whose unique discovery is of use to several others who can compete for the right to use it."
/doi/10.1287/orsc.1090.0429," Technological innovation sometimes requires industry incumbents to shift to a completely new core technology. To successfully navigate a technological transition, firms often face the ambidextrous challenge of “exploiting” existing complementary assets to support the new “exploratory” core technology. We argue that an industry incumbent attempting to make a transition to a new technology requires linkages between organizational units responsible for developing the new technology and units in charge of complementary assets needed to commercialize the innovation. These linkages are critical but overlooked elements of organizational ambidexterity. This paper develops a conceptual framework in which the ability to build and leverage organizational linkages involving the new technology and its complementary assets is essential for a successful technological transition. The framework also highlights the importance of middle management in creating and maintaining these linkages, which are critical to dynamic capabilities in technological transitions. We identify four critical influences—economic, structural, social, and cognitive—on managerial linking activity that enable firms to transition to a new technology while utilizing valuable preexisting capabilities. The technological transitions of IBM and NCR illustrate the importance of organizational linkages and managerial linking activity."
/doi/10.1287/orsc.1050.0135," Organizational fields undergo upheavals. Shifting industry boundaries, new network forms, emerging sectors, and volatile ecosystems have become the stuff of everyday organizational life. Curiously, profound changes of this sort receive scant attention in organization theory and research. Researchers acknowledge fieldwide flux, emergence, convergence, and collapse, but sidestep direct investigations of the causes and dynamic processes, leaving these efforts to political scientists and institutional economists. We attribute this neglect to our field’s philosophical, theoretical, and methodological fealty to the precepts of equilibrium and linearity. We argue that ingrained assumptions and habituated methodologies dissuade organizational scientists from grappling with problems to which these ideas and tools do not apply. Nevertheless, equilibrium and linearity are assumptions of social theory, not facts of social life. Drawing on four empirical studies of organizational fields in flux, we suggest new intellectual perspectives and methodological heuristics that may facilitate investigation of fields that are far from equilibrium. We urge our colleagues to transcend the general linear model, and embrace ideas like field configuration, complex adaptive systems, self-organizing networks, and autocatalytic feedback. We recommend conducting natural histories of organizational fields, and paying especially close attention to turning points when fields are away from equilibrium and discontinuous changes are afoot."
/doi/10.1287/isre.11.2.177.11781," We propose priority pricing as an on-line adaptive resource scheduling mechanism to manage real-time databases within organizations. These databases provide timely information for delay sensitive users. The proposed approach allows diverse users to optimize their own objectives while collectively maximizing organizational benefits. We rely on economic principles to derive priority prices by modeling the fixed-capacity real-time database environment as an economic system. Each priority is associated with a price and a delay, and the price is the premium (congestion toll resulting from negative externalities) for accessing the database. At optimality, the prices are equal to the aggregate delay cost imposed on all other users of the database. These priority prices are used to control admission and to schedule user jobs in the database system. The database monitors the arrival processes and the state of the system, and incrementally adjusts the prices to regulate the flow. Because our model ignores the operational intricacies of the real-time databases (e.g., intermediate queues at the CPU and disks, memory size, etc.) to maintain analytical tractability, we evaluate the performance of our pricing approach through simulation. We evaluate the database performance using both the traditional real-time database performance metrics (e.g., the number of jobs serviced on time, average tardiness) and the economic benefits (e.g., benefits to the organization). The simulation results, under various database workload parameters, show that our priority pricing mechanism not only maximizes organizational benefits but also outperforms in all aspects of traditional performance measures compared to frequently used database scheduling techniques, such as first-come-first-served, earliest deadline first and least slack first."
/doi/10.1287/mksc.2017.1080," Despite the common practice of multiple standards in the high-technology product industry, there is a lack of knowledge on how compatibility between base products and add-ons affects consumer purchase decisions at the brand and/or standard level. We recognize the existence of compatibility constraints and develop a dynamic model in which a consumer makes periodic purchase decisions on whether to adopt/replace a base and/or an add-on product under the expectation of future price, quality, and compatibility. Dynamic and interactive inventory effects are included by allowing consumers to account for the long-term financial implications when planning to switch to a base product that is incompatible with their inventory of add-ons. Applying the model to the consumer purchase history of digital cameras and memory cards from 1998 to 2004, we demonstrate that the inventory of add-ons significantly affects the purchase of base products. This “lock-in” effect is enhanced when future prices of add-ons increase. Interestingly, it is more costly for consumers to switch from Sony to other brands than vice versa. In two policy simulations, we explore the impact of alternative compatibility policies. For example, if Sony had not created its proprietary Memory Stick, the market share of its cameras would have been reduced by 6 percentage points. This result provides important insights that leading brands and early movers should implement a proprietary standard. Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1080 ."
/doi/10.1287/orsc.2019.1341," When outsourcing design tasks, firms want their suppliers to be both efficient and adaptive. Whereas efficiency is necessary to reap economic gains from outsourcing, adaptation is required to deal with interdependencies as the design evolves. Achieving both objectives simultaneously, however, is difficult because procurement contracts require a trade-off between providing incentives for efficiency and facilitating adaptation. In the presence of formal contracts that provide strong incentives for efficiency, ensuring adaptation thus requires effective relational contracts between the buyer and the supplier. But because the focus of prior research has been on dyadic buyer–supplier relationships, it is unclear how the efficiency–adaptation trade-off can be mitigated in the multitier supplier systems that are common in many industries. Addressing this gap, we argue that in hierarchical supplier systems, relational contracts between contractual partners become more important, but at the same time harder to establish, than in single-tier supplier systems. An in-depth case study of the adaptive frictions that arose in the Airbus A350 program allows us to illustrate this challenge of tiered outsourcing. Moreover, we show how Airbus came to resolve the frictions by leveraging skip-level ties—direct informal contacts to lower-level suppliers with which no contractual relationship existed, thus replacing the archetypal notion of a supplier hierarchy by a more complex relationship structure. We discuss the boundary conditions of our findings and suggest propositions for the emergence of skip-level ties in tiered outsourcing."
/doi/10.1287/orsc.1050.0142," We develop a theoretical framework for a specific form of intrafirm competition , namely the extent of overlap between the charters of two or more units in a single organization. This phenomenon is commonly seen in large organizations, e.g., cases of two business units producing competing products, or two product development groups trying to solve the same technological problem, but the existing academic literature provides little insight into the forms intrafirm competition takes, or the conditions under which it is beneficial or harmful to the organization. Building on the concept of an organization charter (Galunic and Eisenhardt 2001), we identify two generic forms of intrafirm competition: the dynamic community model has fluid and frequently changing charter boundaries, and it emerges through the creation of strategic options in the face of a changing environment; the coexistence model has fixed and relatively static charter boundaries, and it owes its existence to economies of scope and differentiation of unit charters to cover multiple market segments. In the body of the paper we develop a theoretical framework to specify the environmental and organizational conditions under which each form of intrafirm competition is expected to occur."
/doi/10.1287/mnsc.1100.1215," This paper studies two fundamentally distinct approaches to opening a technology platform and their different impacts on innovation. One approach is to grant access to a platform and thereby open up markets for complementary components around the platform. Another approach is to give up control over the platform itself. Using data on 21 handheld computing systems (1990–2004), I find that granting greater levels of access to independent hardware developer firms produces up to a fivefold acceleration in the rate of new handheld device development, depending on the precise degree of access and how this policy was implemented. Where operating system platform owners went further to give up control (beyond just granting access to their platforms) the incremental effect on new device development was still positive but an order of magnitude smaller. The evidence from the industry and theoretical arguments both suggest that distinct economic mechanisms were set in motion by these two approaches to opening."
/doi/10.1287/mnsc.1110.1478," We use an incentive-compatible economic experiment and surveys in the field at a large financial services firm to identify the norms for on-the-job behavior among financial advisers and their leaders, and the normative expectations each group has of the other. We examine whistle-blowing on a peer, an incentive clash between serving the client and earning commissions, and a dilemma about fiduciary responsibility to a client. We find patterns of agreement among advisers, among leaders, and between the two groups, that are consistent with company guidelines identified ex ante. However, we also find measurable differences between what leaders expect and the actual norms of advisers. When there is such a mismatch we are able to distinguish miscommunication from ethical disagreement between leaders and advisers. Finally, we show that when advisers' personal ethical opinions do not match group norms, this mismatch is correlated with job dissatisfaction and lying for money in a second experiment. This paper was accepted by Brad Barber, Teck Ho, and Terrance Odean, special issue editors."
/doi/10.1287/mnsc.2017.2791," This paper develops and estimates an economic model of the costs and beliefs required to rationalize household direct stock ownership. In the model, investors believe they can learn information about individual stock returns through costly research. The model identifies the distributions of both research costs and beliefs about the predictability of individual stock returns. Identification depends only on households’ wealth and portfolio choices. The model also provides a novel explanation for many empirical features of household portfolios. Parameter estimates suggest that most households have modest beliefs about the benefits of individual stock research, although a minority must expect extraordinary returns. Data are available at https://doi.org/10.1287/mnsc.2017.2791 . This paper was accepted by Neng Wang, finance."
/doi/10.1287/serv.1120.0028," A product-service system (PSS) is a novel type of business model that integrates products and services in a single system. It provides a strategic alternative to product-oriented economic growth and price-based competition in the global market. This research proposes a methodology to support the generation of innovative PSS concepts, called the PSS concept generation support system. The models and strategies of 118 existing PSS cases were analyzed, and the insights extracted were used to develop the methodology. The methodology consists of various tools and a systematic procedure to support the generation process. It is generic enough to be applied to a variety of PSS contexts. The methodology is demonstrated and verified via case studies on the washing machine and refrigerator industries. The proposed PSS concept generation support system can serve as an efficient and effective aid to PSS designers for new PSS development."
/doi/10.1287/opre.1070.0408," Motivated by one of the leading intermodal logistics suppliers in the United States, we consider an internal pricing mechanism for managing a fleet of service units (shipping containers) flowing in a closed queueing network. Nodes represent geographic locations, and arcs represent travel between them. Customer requests for arcs arrive over time, and the problem is to find an accept/reject policy that maximizes the long-run time average reward rate from accepting requests. We formulate the problem as a semi-Markov decision process and give a simple linear program that provides an upper bound on the optimal reward rate. Using Palm calculus, we derive a nonlinear program that approximately captures queueing and stockout effects on the network. Using its optimal Lagrange multipliers, we construct a simple functional approximation to the dynamic programming value function. The resulting policy is computationally efficient and produces superior economic performance as compared with other policies. Furthermore, it provides a methodologically grounded solution to the firm's internal pricing problem."
/doi/10.1287/opre.1120.1087," We present a framework to analyze the process location and product distribution problem with uncertain yields for a large multinational food processing company. This problem consists of selecting the location of processes, the assignment of products, and the distribution of production quantities to markets in order to minimize total expected costs. It differs from the traditional facility location problem due to characteristics that are inherent to process industry sectors. These include significant economies of scale at high volumes, large switchover times, and production yield uncertainty. We model the problem as a nonlinear mixed-integer program. A challenging aspect of this problem is that the objective function is neither convex nor concave. We develop an exact approach to linearize the objective function. We present heuristics to solve the problem and also construct lower bounds based on a reduction of the constraint set to evaluate the quality of the solutions. This framework has been used to make process choice and product allocation decisions at the food processing company, and the estimated annual cost savings are around 10%, or $50 million. In addition, the insights from the model have had a significant strategic and organizational impact at this company. Our framework and conclusions are relevant to other industrial sectors with similar characteristics, such as pharmaceuticals and specialty chemical manufacturers."
/doi/10.1287/mnsc.1080.0910," Supply chain contracting literature has traditionally focused on aligning incentives for economically rational players. Recent work has hypothesized that social preferences, as distinct from economic incentives, may influence behavior in supply chain transactions. Social preferences refer to intrinsic concerns for the other party's welfare, reciprocating a history of a positive relationship, and intrinsic desires for a higher relative payoff compared with the other party's when status is salient. This article provides experimental evidence that social preferences systematically affect economic decision making in supply chain transactions. Specifically, supply chain parties deviate from the predictions provided by self-interested profit-maximization models, such that relationship preference promotes cooperation, individual performance, and high system efficiency, sustainable over time; whereas status preference induces tough actions and reduces both system efficiency and individual performance."
/doi/10.1287/moor.2015.0743," This paper is concerned with the optimality of a trend following trading rule. The underlying market is modeled like a bull-bear switching market in which the drift of the stock price switches between two states: the uptrend (bull market) and the down trend (bear market). We consider the case when the market mode is not directly observable and model the switching process as a hidden Markov chain. This is a continuation of our earlier study reported in Dai et al. [Dai M, Zhang Q, Zhu Q (2010) Trend following trading under a regime-switching model. SIAM J. Fin. Math. 1:780–810] where a trend following rule is obtained in terms of a sequence of stopping times. Nevertheless, a severe restriction imposed in Dai et al. [Dai M, Zhang Q, Zhu Q (2010) trend following trading under a regime-switching model. SIAM J. Fin. Math. 1:780–810] is that only a single share can be traded over time. As a result, the corresponding wealth process is not self-financing. In this paper, we relax this restriction. Our objective is to maximize the expected log-utility of the terminal wealth. We show, via a thorough theoretical analysis, that the optimal trading strategy is trend following. Numerical simulations and backtesting, in support of our theoretical findings, are also reported."
/doi/10.1287/mnsc.1100.1210," Online personalization services belong to a class of economic goods with a “no free disposal” (NFD) property where consumers do not always prefer more services to less because of the privacy concerns. These concerns arise from the revelation of information necessary for the provision of personalization services. We examine vendor strategies in a market where consumers have heterogeneous concerns about privacy. In successive generalizations, we allow the vendor to offer a fixed level of personalization, variable levels of personalization, and monetary transfers (coupons) to the consumers that depend on the level of personalization chosen. We show that a vendor offering a fixed level of personalization does not offer a coupon unless his marginal value of information (MVI) is sufficiently high, and even when personalization is costless, the vendor does not cover the market. Under a fixed services offering, the vendor serves the same market with or without couponing. Next, we demonstrate that in the absence of couponing, the vendor's optimal variable personalization services contract maximizes surplus for all heterogeneous consumers, which is in contrast to standard results from monopolistic screening. When the vendor can offer coupons that vary according to personalization levels, the optimal contract is not fully revealing unless his MVI is high and he will not offer coupons when this MVI is low. However, a vendor with a moderate MVI (between certain thresholds) offers a bunched contract, wherein consumers with low privacy concerns receive a variable services-coupon contract, those with moderate privacy concerns receive a fixed services-coupon contract, and those with high privacy concerns do not participate in the market. The coupon value is decreasing in privacy sensitivity of consumers."
/doi/10.1287/mnsc.1050.0454," The economical and environmental benefits of product remanufacturing have been widely recognized in the literature and in practice. In this paper, we focus on the interaction between a manufacturer’s reverse channel choice to collect postconsumer goods and the strategic product pricing decisions in the forward channel when retailing is competitive. To this end, we model a direct product collection system, in which the manufacturer collects used products directly from the consumers (e.g., print and copy cartridges) and an indirect product collection system, in which the retailers act as product return points (e.g., single-use cameras, cellular phones). We first examine how the allocation of product collection to retailers impacts their strategic behavior in the product market, and we discuss the economic trade-offs the manufacturer faces while choosing an optimal reverse channel structure. When a direct collection system is used, channel profits are driven by the impact of scale of returns on collection effort, whereas in the indirect reverse channel, supply chain profits are driven by the competitive interaction between the retailers. Subsequently, we show that the buy-back payments transfered to the retailers for postconsumer goods provide a wholesale pricing flexibility that can be used to price discriminate between retailers of different profitability."
/doi/10.1287/msom.2014.0506," We consider an inventory policy that expedites delivery times of open orders if the inventory level drops below a certain threshold. By expediting open orders, back orders can be reduced. Order expediting is costly, and we include various types of expediting costs in the model. We prove structural properties of the model and show how the optimal parameters of the expediting policy can be computed efficiently. The expediting policy is easy to implement and, for situations with variable expediting cost only, the structure of the policy is optimal. For situations with nonvariable expediting costs, the expediting policy that we consider is generally not optimal. The optimal policy can be computed by dynamic programming, but this approach is computationally feasible only for small-problem instances. We conduct numerical experiments that are based on data from the service division of a global equipment manufacturer to evaluate the performances of the expediting policy and the optimal policy. The results show that substantial cost savings can be achieved by order expediting and that the expediting policy realizes a great share of the cost-saving potential offered by order expediting."
/doi/10.1287/mnsc.47.6.787.9808," Unprecedented changes in the economics of interaction, mainly as a result of advances in information and telecommunication technologies such as the Internet, are causing a shift toward more networked forms of organizations such as horizontal alliances—that is, alliances among firms in similar businesses that have positive externalities between them. Because the success of such horizontal alliances depends crucially on aligning individual alliance-member incentives with those of the alliance as a whole, it is important to find coordination mechanisms that achieve this alignment and are simple-to-implement. In this paper, we examine two simple coordination mechanisms for a horizontal alliance characterized by the following features: (i) firms in the alliance can exert effort only in their “local” markets to increase customer demand for the alliance; (ii) customers are mobile and a customer living in a given alliance member's local area may have a need to buy from some other alliance member; and (iii) the coordination rules followed by the alliance determine which firms from a large pool of potential member-firms join the alliance, and how much effort each firm joining the alliance exerts in its local market. In this horizontal alliance setup, we consider the use of two coordination mechanisms: (i) a linear transfer of fees between members if demand from one member's local customer is served by another member, and (ii) ownership of an equal share of the alliance profits generated from a royalty on each member's sales. We derive conditions on the distribution of demand externalities among alliance members to determine when each coordination mechanism should be used separately, and when the mechanisms should be used together."
/doi/10.1287/orsc.1120.0799," Despite increasing interest in transnational fields, transnational commons have received little attention. In contrast to economic models of commons, which argue that commons occur naturally and are prone to collective inaction and tragedy, we introduce a social constructionist account of commons. Specifically, we show that actor-level frame changes can eventually lead to the emergence of an overarching, hybrid “commons logic” at the field level. These frame shifts enable actors with different logics to reach a working consensus and avoid “tragedies of the commons.” Using a longitudinal analysis of key actors’ logics and frames, we tracked the evolution of the global climate change field over 40 years. We bracketed time periods demarcated by key field-configuring events, documented the different frame shifts in each time period, and identified five mechanisms (collective theorizing, issue linkage, active learning, legitimacy seeking, and catalytic amplification) that underpin how and why actors changed their frames at various points in time—enabling them to move toward greater consensus around a transnational commons logic. In conclusion, the emergence of a commons logic in a transnational field is a nonlinear process and involves satisfying three conditions: (1) key actors view their fates as being interconnected with respect to a problem issue, (2) these actors perceive their own behavior as contributing to the problem, and (3) they take collective action to address the problem. Our findings provide insights for multinational companies, nation-states, nongovernmental organizations, and other stakeholders in both conventional and unconventional commons."
/doi/10.1287/inte.2018.0953," This work highlights how mathematical programming modeling supported the decision-making process of locating a biorefinery in Northern Spain. Through close cooperation with relevant stakeholders, we considered a wide range of influencing factors such as supply and storage strategies, transport infrastructure, and individual characteristics of biomass products. The findings of this study provided decision makers with a ranking of potential locations and highlighted the importance of facilitating intermediate storage points for supplying the biorefinery. Currently, the Government of Navarre (Spain) is evaluating the results for its economic sustainability and plans to make the final decision on one of the top-ranked locations in the near future."
/doi/10.1287/mnsc.2015.2239," The practice of detailing in the marketing of prescription drugs is undergoing significant changes. For instance, in September 2013, the Physician Payment Sunshine Act went into full effect. The accompanying transparency requirements have prompted physician practices and hospitals to severely restrict pharmaceutical sales representatives’ direct access to their physicians. Despite all the attention in the popular press, scant scholarly research has investigated how these restrictions on physician access impact physician prescription behavior and competitive detailing to physicians. To analyze the impact of these restrictions, we develop a structural model of how pharmaceutical firms compete dynamically to schedule detailing to physicians. Detailing activities are known to have significant carryover effects that are captured in a first-stage model of physicians’ demand for prescription drugs. We also specify detailing policy functions that describe each firm’s observed detailing actions. In a second stage, we estimate a model that describes costs of detailing, assuming that the observed detailing levels are consistent with a Markov perfect Nash equilibrium. The estimated structural model is used to examine the implications of restrictions on the amount of detailing via counterfactual simulations. We find that restriction policies would increase the market share of a nondrug-treatment-only option but impact firms asymmetrically; firms that are strong in detailing and/or rely more on detailing would be hurt more. Unexpectedly, a policy that imposes a ceiling on detailing frequency could significantly reduce detailing of all firms in the market, including those firms with average detailing levels below the ceiling, and effectively would soften competition between firms and enhance their profits. This paper was accepted by J. Miguel Villas-Boas, marketing ."
/doi/10.1287/isre.1040.0031," This paper presents an approach to organizational modeling that combines both agent-centric and activity-centric approaches. Activity-centric approaches to process modeling capture the mechanistic components of a process (including aspects of workflow, decision, and information), but agent-centric approaches capture specific aspects of the human component. In this paper, we explore an integrative viewpoint in which the transactional aspects of agent-centric concerns—for example, economic incentives for agents to perform—are integrated with decision and informational aspects of a process. To illustrate issues in this approach, we focus on modeling incentive mechanisms in a specific sales process and present results from an extensive simulation experiment. Our results highlight the importance of considering the effects of incentives when decision and informational aspects of a process undergo changes."
/doi/10.1287/mnsc.2018.3246," Dominant platform businesses often develop products in adjacent markets to complement their core business. One common approach used to gain traction in these adjacent markets has been to pursue a tying strategy. For example, Microsoft preinstalled Internet Explorer into Windows, and Apple set Apple Maps as the iOS default. Policymakers have raised concerns that dominant platforms may be leveraging their market power to gain traction for lower quality products when they use a tying strategy. In this paper, we empirically explore this question by examining Google’s decision to tie its new reviews product to its search engine. We experimentally vary the reviews displayed above Google’s organic search results to show either exclusively Google reviews (Google’s current tying strategy) or reviews from multiple platforms determined to be the best-performing by Google’s own organic search algorithm. We find that users prefer the version that does not exclude competitor reviews. Furthermore, looking at observational data on user traffic to Yelp from search engines, we find that Google’s exclusion of downstream competitors may have been effective. The share of Yelp’s traffic coming from Google has declined over this period, relative to traffic from Bing and Yahoo (which do not exclude other companies’ reviews), and Google reviews has grown more quickly than Yelp and TripAdvisor during the period in which they excluded these (and other) reviews providers. Overall, these results suggest that tying has the potential to facilitate entry in complementary markets even when the tied product is of worse quality than competitors. The online appendix is available at https://doi.org/10.1287/mnsc.2018.3246 . This paper was accepted by Joshua Gans, business strategy."
/doi/10.1287/opre.2017.1697," Display advertising has a 39% share of the online advertising market and is its fastest-growing category. In this paper, we consider an online display advertising setting in which a web publisher posts display ads on its website and charges based on the cost-per-impression (CPM) pricing scheme while promising to deliver a certain number of impressions on the ads posted. The publisher faces uncertain demand for advertising slots and uncertain supply of visits from viewers. We formulate the problem as a queueing system, where the advertising slots correspond to service channels with the service rate of each server synchronized with other active servers. We determine the publisher’s optimal price to charge per impression and show that it can increase in the number of impressions made of each ad, which is in contrast to the quantity discount commonly offered in practice. We show that the optimal CPM price may increase in the number of ads rotating among slots. This result is typically not expected because an increase in the number of rotating ads in the system can be interpreted as an increase in the service capacity. However, the capacity increase leads to an increase in the fill rate of the demand (the portion of demand satisfied by the publisher). Hence, the publisher can afford to optimally decrease the arrival rate by increasing the price. The electronic companion is available at https://doi.org/10.1287/opre.2017.1697 ."
/doi/10.1287/mnsc.2021.4123," We analyze the problem of a buyer who purchases a long-term project from one of several suppliers. A changing state of the world influences the costs of the suppliers. We distinguish between complete contracts conditioning on all future realizations of the state of the world and incomplete contracts renegotiated whenever the state of the world changes. We provide conditions such that incomplete contracting does not pose a problem. If the changing state of the world is publicly observable and the buyer cannot switch between suppliers during the lifetime of the project, the buyer achieves the same surplus irrespective of whether contracts are complete or incomplete. An English auction followed by renegotiation whenever the state of the world changes is optimal. To identify conditions when buyers should consider drafting complete contracts, we extend the analysis by considering private information about the changing state of the world and supplier switching. In both cases, incomplete contracting poses a problem. In a survey of procurement consultants, we confirm that publicly observable states of the world via price indexes play an important role in procurement. Moreover, the consultants confirm that supplier switching is infrequent in procurement practice. Thus, incomplete contracting is less of a problem in a considerable share of procurement projects. However, complete contracts are useful and could be used more often. This paper was accepted by Yan Chen, behavioral economics and decision analysis."
/doi/10.1287/orsc.12.3.372.10099," This paper examines how a collaborative effort between the private and public sectors, called the Derivatives Policy Group (DPG), helped shape current regulation of financial innovation. In 1994 and 1995, this group of six large financial firms developed procedures for risk management, internal controls, and reporting for largely unregulated areas of finance, in cooperation with the United States Securities and Exchange Commission and Commodity Futures Trading Commission. The process succeeded despite strong competition among the firms themselves and incentives for both the public and private sectors to resort to adversarial lobbying and legal challenges. The Derivatives Policy Group was a path-setting event in the development of flexible regulation of financial innovation that is now the norm for related policy making. The case is important in and of itself--the financial markets are a major concern of national and international economic policy--but here we treat it as an instance of a larger class of problems. Organizational science constantly encounters settings that involve numerous participants who compete or have histories of conflicts; who are interdependent, and collectively would gain (and even individually gain long term) by cooperating rather than competing on an issue; who fall under different governance systems; and who try as a group to design rules and principles governing their behavior. Four factors appear repeatedly in the research on the success or failure of such arrangements. These are (1) the initial dispositions toward cooperation, (2) the extant issues and incentives, (3) leadership, and (4) the number and variety of organizations involved. This paper focuses on how these factors shaped the development and consequences of the Derivatives Policy Group, and the general implications of this process for interorganizational cooperation."
/doi/10.1287/msom.2019.0861," Problem definition : Although they enjoy low costs in sourcing from emerging economies, global brands also face serious brand and reputation risks from their suppliers’ noncompliance with environmental and labor standards. Such a supplier problem can be viewed as a process quality problem concerning how products are sourced and produced. Academic/practical relevance : Addressing this problem is a key component of many global companies’ responsible sourcing programs. A common approach is to use an audit as an auxiliary supplier screening mechanism. However, in regions with lax law enforcement, an unethical, noncomplying supplier may attempt to bribe an unethical auditor to pass the audit. Such supplier-auditor collusion compromises the integrity of the audit and weakens its effectiveness. Methodology : In this paper, we develop a game-theoretical model to study the effect of supplier-auditor collusion on the buyer’s auditing and contracting strategy in responsible sourcing, as well as various driving factors that help reduce collusion. Results : We show that the buyer’s equilibrium contracting strategy is a shutout contract that takes three different forms, depending on the collusion risk level. We also define and analyze the screening errors and social efficiency loss caused by supplier-auditor collusion. By comparing the cost versus collusion elimination trade-off between a third-party audit and an in-house audit, we offer explanations for why many global brands fully rely on third-party audits and set higher process quality requirements for suppliers located in high-risk countries. The robustness of our insights is verified by two model extensions: one involving additional supplier audit cost and the other allowing for supplier process quality improvement before audit. Managerial implications : These model insights provide useful theoretical support and baseline guidance for the current supplier audit practices in responsible sourcing. Our extended model analysis further demonstrates the importance for global brands to lobby local governments to increase collusion penalties and to promote the ethical level of the third-party auditors located in high-risk countries."
/doi/10.1287/mksc.20.1.23.10201," Our research investigates the competitive ramifications of individual marketing and information management in today's information-intensive marketing environments. The specific managerial issues we address are as follows. First, what kinds of incentive environments do competing firms face when they can only target individual customers imperfectly? Second, does the improvement in an industry's targetability intensify price competition in the industry such that all competing firms become worse off? Third, should a firm share its customer knowledge so as to improve its rival's targetability? Fourth, how should an information vendor sell its information that can improve a firm's targetability? Finally, do competing firms have the same incentives to invest in their own targetability? To answer those questions, we develop a simple model à la Narasimhan (1988), in which each of the two competing firms have their own loyal customers and compete for common switchers. We assume that each firm can classify its own loyal customers and switchers correctly only with a less-than-perfect probability. This means that each firm's perceived customer segmentation differs from the actual customer segmentation. Based on their perceived reality, these two competing firms engage in price competition. As an extension, we also allow the competing firms to make their investment decisions to acquire targetability. We show that when individual marketing is feasible, but imperfect, improvements in targetability by either or both competing firms can lead to win-win competition for both even if both players behave noncooperatively and the market does not expand. Win-win competition results from the fact that as a firm becomes better at distinguishing its price-insensitive loyal customers from the switchers, it is motivated to charge a higher price to the former. However, due to imperfect targetability, each firm mistakenly perceives some price-sensitive switchers as price-insensitive loyal customers and charges them all a higher price. These misperceptions thus allow its competitors to acquire those mistargeted customers without lowering their prices and, hence, reduce the rival firm's incentive to cut prices. This effect softens price competition in the market and qualitatively changes the incentive environment for competing firms engaged in individual marketing. A “prisoner's dilemma” occurs only when targetability in a market reaches a sufficiently high level. This win-win perspective on individual marketing has many managerial implications. First, we show that superior knowledge of individual customers can be a competitive advantage. However, this does not mean that a firm should always protect its customer information from its competitors. To the contrary, we find that competing firms can all benefit from exchanging individual customer information with each other at the nascent stage of individual marketing, when firms' targetability is low. Indeed, under certain circumstances, a firm may even find it profitable to give away this information unilaterally. However, as individual marketing matures (as firms' targetability becomes sufficiently high), further improvements in targetability will intensify price competition and lead to prisoner's dilemma. Therefore, it is not only prudent politics but also a business imperative for an industry to seize the initiative on the issue of protecting customer privacy so as to ensure win-win competition in the industry. Second, we show that the firm with a larger number of loyal customers tends to invest more in targetability when the cost of acquiring targetability is high. However, the firm with a smaller loyal base can, through information investment, acquire a higher level of targetability than the firm with a larger loyal base as long as the cost of acquiring targetability is not too high. As the cost further decreases, competing firms will all have more incentives to increase their investments in targetability until they achieve the highest feasible level. Third, an information vendor should make its information available nonexclusively (exclusively) when its information is associated with a low (high) level of targetability. When the vendor does sell its information exclusively, it should target a firm with a small loyal following if it can impart a high level of targetability to that firm. Finally, our analysis shows that an information-intensive environment does not doom small firms. In fact, individual marketing may provide a good opportunity for a small firm to leapfrog a large firm. The key to leapfrogging is a high level of targetability or customer knowledge."
/doi/10.1287/isre.2019.0881," With the increased penetration of digital technologies into entrepreneurship, the traditional need for proximity to specific locations or large amounts of funding for infrastructure development has diminished. Instead, digital entrepreneurs now pursue locations that provide more opportunities for funding (rounds) and greater social network support. In this paper, we empirically estimate the role of social networks and funding opportunities in a location on entrepreneurs’ decisions to create a start-up in their existing or a new location. We use economic indicators made publicly available by the U.S. government; investment information from CrunchBase and PricewaterhouseCoopers; and demographics, professional histories, and network data from LinkedIn. Analysis of 1,418 entrepreneurs who successfully secured funding suggests that funding rounds per year play a significant and positive role in influencing start-up creation in that location, and local social network density creates stickiness to that location, thus negatively influencing entrepreneurs' willingness to relocate. If entrepreneurs do relocate, we assume that they will create a start-up immediately or delay it only by a short time. In the latter case, our results show that larger social network density (in a new location) affects start-up creation decision. Additionally, we find empirical evidence that midcareer individuals (like millennials) are more likely than early-career or late-career-stage individuals to create a successful technology start-up in the same or a new location. Also, entrepreneurs are less likely to relocate for an Internet-based start-up but are likely to move if they have larger social network density in the new location. This research enhances our understanding of the factors that influence decisions to launch a start-up in a particular location and contributes to the limited literature on technology entrepreneurship in the field of information systems."
/doi/10.1287/orsc.2013.0825," Bank panics attract scholarly interest because they reflect distrust of each bank that experiences a run as a result of diffusion of information; rumors about such bank runs trigger additional runs elsewhere. However, the contagion of bank runs is highly selective for reasons that are unrelated to the financial strength of the individual banks. This presents a puzzle that extant theories on institutions and reputations cannot fully explain. To solve this puzzle, we turn to the characteristics of the community in which the banks operate. We develop theory on how communities with diverse affiliation structures and economic inequality have weaker community cohesion and communication, making such communities less likely to experience widespread distrust and hence bank runs. We test hypotheses on the effects of community ethnic diversity, national origin diversity, religious diversity, and wealth inequality using data from the great bank panic of 1893, and we find strong community effects on bank runs. These findings suggest that the contagion of distrust in organizations following adverse events is channeled by community differences as well as organizational differences."
/doi/10.1287/mnsc.48.2.300.251," In this paper, we examine two information-based supply-chain efforts that are often linked to Vendor-Managed Inventory (VMI) programs. Specifically, we consider a supplier serving multiple retailers located in a close proximity. The first effort uses information on the retailers' inventory positions to coordinate shipments from the supplier to enjoy economies of scale in shipments, such as full truckloads.The second effort uses the same information for eventual unloading of the shipments to the retailers to rebalance their stocking positions. How much benefit do we gain from such initiatives? What are the relative benefits of the two initiatives? What are the drivers of such benefits? This paper seeks answers to these questions."
/doi/10.1287/trsc.2020.1009," Less-than-truckload carriers rely on the consolidation of freight from multiple shippers to achieve economies of scale. Collected freight is routed through a number of transfer terminals at each of which shipments are grouped together for the next leg of their journeys. We study the service network design problem confronted by these carriers. This problem includes determining (1) the number of services (trailers) to operate between each pair of terminals and (2) a load plan , which specifies the sequence of transfer terminals that freight with a given origin and destination will visit. Traditionally, for every terminal and every ultimate destination, a load plan specifies a unique next terminal. We introduce the p -alt model, which generalizes traditional load plans by allowing decision makers to specify a desired number of next-terminal options for terminal–destination pairs using a vector p . We compare a number of exact and heuristic approaches for solving a two-stage stochastic variant of the p -alt model. Using this model, we show that, by explicitly considering demand uncertainty and by merely allowing up to two next-terminal options for terminal–destination pairs in the load plans, carriers can generate substantial cost savings that are comparable to the ones yielded by adopting load plans that allow for any next terminal to be a routing option for terminal–destination pairs. Moreover, by using these more flexible load plans, carriers can generate savings on the order of 10% over traditional load plan designs obtained by deterministic models."
/doi/10.1287/msom.2020.0907," Problem definition : We study sourcing behavior in severe conditions where supply disruptions are rare but carry the potential of wiping out several rounds worth of a firm’s profit. Academic/practical relevance : The tradeoff between scale economies from supplier consolidation and risk mitigation from supplier diversification is at the core of firms’ sourcing strategy and one that is empirically understudied. Methodology : We study supplier diversification through a behavioral lens and test theoretically derived predictions under controlled laboratory conditions. Results : Our data provide strong evidence for under-diversification . We posit that this pattern is partly because of the fact that investing in supplier diversification involves an upfront cost to achieve a delayed, and rarely encountered, benefit. Managerial implications : Under-diversification bias is costly, and its causes are difficult to overcome, presenting firms with the daunting task of devising debiasing mechanisms that reinforce a supplier diversification strategy when the rarity of disruptions almost always render supplier consolidation the ex post preferred strategy."
/doi/10.1287/isre.12.3.260.9713," As demand for online network services continues to grow, service providers are looking to meet this need and avail themselves of business opportunities. However, despite strong growth in demand, providers continue to have difficulty achieving profitability, customer churn remains high, and network performance continues to draw complaints. We suggest that strategic business planning for network services would benefit from a systems thinking approach that analyzes the feedback effects present in the underlying business process. These feedback loops can be complex and have significant impact on business performance. For instance, while the size of a provider's customer base depends on price and network performance, network performance is itself dependent on the size of the customer base. In this paper, we develop a planning model that represents these feedback effects using the finite difference equations methodology of systems dynamics. The model is validated by showing its fit with essential characteristics of the underlying problem domain, and by showing its ability to replicate observed reference mode behaviors. Simulations are then carried out under a variety of scenarios to examine issues important to service providers. Among other findings, the simulations suggest that (a) under flat-rate pricing, lowering price to increase customer base can hurt profitability as well as network performance; (b) under usage-based pricing, lowering price need not necessarily lead to a larger customer base; and (c) in addition to price, the customers' threshold of tolerance for performance degradation plays a significant role in balancing market share with profitability. We briefly present a prototype decision support system based on the systems thinking approach, and suggest ways in which it could be used to help business planning for network services."
/doi/10.1287/isre.1110.0386," E-governments have become an increasingly integral part of the virtual economic landscape. However, e-government systems have been plagued by an unsatisfactory, or even a decreasing, level of trust among citizen users. The political exclusivity and longstanding bureaucracy of governmental institutions have amplified the level of difficulty in gaining citizens' acceptance of e-government systems. Through the synthesis of trust-building processes with trust relational forms, we construct a multidimensional, integrated analytical framework to guide our investigation of how e-government systems can be structured to restore trust in citizen-government relationships. Specifically, the analytical framework identifies trust-building strategies (calculative-based, prediction-based, intentionality-based, capability-based, and transference-based trust) to be enacted for restoring public trust via e-government systems. Applying the analytical framework to the case of Singapore's Electronic Tax-Filing (E-Filing) system, we advance an e-government developmental model that yields both developmental prescriptions and technological specifications for the realization of these trust-building strategies. Further, we highlight the impact of sociopolitical climates on the speed of e-government maturity."
/doi/10.1287/inte.1090.0476," Hewlett-Packard (HP) offers many innovative products to meet diverse customer needs. The breadth of its product offering has helped the company achieve unparalleled market reach; however, it has come with significant costs and challenges. By offering multiple similar products, a manufacturer increases its overall demand volatility, reduces forecast accuracy, and can adversely affect revenue and costs across the entire product life cycle. At HP, these impacts included increases in inventory-driven costs and order-cycle time; liabilities to channel partners; and costs of operations, research and development, marketing, and administration. Furthermore, complexity in HP's product lines confused customers, sales representatives, and channel partners, sometimes driving business to competitors. HP developed two powerful operations research-based solutions for managing product variety. The first, a framework for screening new products, uses custom-built return-on-investment (ROI) calculators to evaluate each proposed new product before introduction; those that do not meet a threshold ROI level are targeted for exclusion from the proposed lineup. The second, HP's Revenue Coverage Optimization (RCO) tool, which is based on a fast, new maximum-flow algorithm, is used to manage product variety after introduction. By identifying a core portfolio of products that are important to order coverage, RCO enables HP businesses to increase operational focus on their most critical products. These tools have enabled HP to increase its profits across business units by more than $500 million since 2005. Moreover, HP has streamlined its product offerings, improved execution, achieved faster delivery, lowered overhead, and increased customer satisfaction and market share."
/doi/10.1287/mnsc.2015.2219," Managers and policy makers frequently face crucial strategic decisions that inevitably rely on judgments about relevant future events. These judgments are often characterized by very high uncertainty and the absence of experience from previous good or bad judgments. Judgments of other experts are oftentimes an important—sometimes the only—source of additional information to reduce uncertainty and improve judgment accuracy. However, in many practical situations, decision makers have very limited means to evaluate the quality of such “advice” from other experts and could tend to ignore this valid source of information. In this paper, we study what leads decision makers to take advice from an expert panel when judging the probability of far-future events with high economic impact. Our analysis is based on a unique data set that comprises more than 15,000 advice-taking decisions made by almost 1,000 experts from different industries. We find that decision makers have a strong tendency to ignore advice, which pronounces even further when conflicts in terms of beliefs, past experiences, or desires arise. This paper was accepted by Yuval Rottenstreich, judgment and decision making."
/doi/10.1287/mnsc.47.11.1441.10249," Professional services firms (e.g., consultants, accounting firms, or advertising agencies) generate and sell business solutions to their customers. In doing so, they can leverage the cumulative experience gained from serving their customer base to either reduce their variable costs or increase the quality of their products/services. In other words, their “production technology” exhibits some form of increasing returns to scale. Growth and globalization, coupled with recent advances in information technology, have led many of these firms to introduce sophisticated knowledge management (KM) systems in order to create sustainable competitive advantage. In this paper, the authors analyze how KM is likely to affect competition among such professional services firms. In particular, they first explore what type (supply-side versus demand-side) of economies of scale are likely to be exploited in KM systems. In the former case, KM's role is to reduce the operating costs of the firm, while in the latter case, its role is to create added value to customers by significantly increasing product quality. Second, the authors analyze the competitive dynamics and market structure that emerge as a result of firms competing with KM systems. The results shed light on the current literature exploring the deployment of KM systems by suggesting that in a competitive setting, when firms' ability to leverage their customer base is high, KM should lead to quality improvement rather than cost reductions. In a dynamic setting, it is also shown that when firms use their KM system to improve product quality, higher ability to leverage the customer base may actually hurt profits and lead to industry shakeout. Beyond normative insights, the results also support a number of recent market trends in management consulting, including the increased emphasis on knowledge-creating activities in modern KM systems, the wave of mergers between consulting firms, and the recent emergence of “retail consulting” services."
/doi/10.1287/isre.13.2.125.89," We propose the Net-Enabled Business Innovation Cycle (NEBIC) as an applied dynamic capabilities theory for measuring, predicting, and understanding a firm's ability to create customer value through the business use of digital networks. The theory incorporates both a variance and process view of net-enabled business innovation. It identifies four sequenced constructs: Choosing new IT, Matching Economic Opportunities with technology, Executing Business Innovation for Growth, and Assessing Customer Value, along with the processes and events that interrelate them as a cycle. The sequence of these theorized relationships for net-enablement (NE) 1 asserts that choosing IT precedes rather than aligns with corporate strategy. The theory offers a logically consistent and falsifiable basis for grounding research programs on metrics of net-enabled business innovation."
/doi/10.1287/orsc.1100.0616," Research in creative destruction has argued that competence-destroying discontinuities result in incumbents' underperformance in research and development (R&D) with respect to entrants, even if complementary assets aid incumbents in retaining market share. In this paper, I propose that attention to the extent of competence destruction is necessary but not sufficient. An analysis of differences in R&D performance through a discontinuity requires assessment not only of old competence destruction but also of new competence access; that is, it requires assessment of both old capability obsolescence and new capability acquisition. I find evidence for this proposition in data from the biotechnology disruption to the anticancer drug market. In particular, my research design is a within-market matched pair of discontinuities: from chemotherapy to small-molecule targeted drugs, and from chemotherapy to large-molecule targeted drugs. Although equally competence destroying, the two discontinuities differ in the access incumbents have to the new capabilities required: whereas all new capabilities are available in the former, one new capability is inaccessible to incumbents (and to many entrants) in the latter. The contrasting results of these two discontinuities support my proposition: in the competence-destroying discontinuity with full access to new capabilities, incumbents did not fall behind entrants; in the other discontinuity, incumbents fell behind only those entrants that owned the difficult-to-access new capability. I close with implications for research in creative destruction, in rational adaptation to environmental change, and in strategic renewal."
/doi/10.1287/trsc.2015.0602," We address the conflict resolution problem in air traffic management. It is widely acknowledged that air traffic controllers’ (ATCs) workload is related to the density of flights. ATCs’ main task is to ensure the safety of flights throughout their trips and consists of ensuring the respect of separation standards. Recently, the concept of subliminal control has emerged as a promising conflict resolution technique that could be used to reduce the impact of conflicts on ATCs’ workload. In this research, we present deterministic conflict resolution models adapted to subliminal speed control. The proposed models are formulated as nonlinear optimization problems that seek to minimize indicators related to ATCs’ workload (total conflict duration, total number of conflicts) using only minor speed adjustments. We introduce a linear approximation of the aircraft separation equations to implement the obtained mixed integer programs on a continental size air traffic network. Specifically, we develop a simulation framework aiming at reproducing realistic navigation conditions and evaluate the robustness of our conflict resolution models using a generic uncertainty model. We show that the impact of conflicts on ATCs’ workload can be significantly reduced using only limited resources, i.e., a narrow speed modulation range, even in the presence of perturbations. Further, we demonstrate that a significant share of the potential conflicts can be resolved without inducing important delays to flights. Finally, we report that our model does not require extensive computational resources as most instances can be solved to global optimality in a few seconds."
/doi/10.1287/orsc.8.6.681," Various determinants have been proposed to explain the persistence of nationally-bound administrative heritages. In this paper, we propose an overall model that integrates these contingent determinants. Our model is based on historical analysis, a method of theory building that first conceptualizes a model and then utilizes it as an exploratory lens for viewing historical evidence. The originality of our model comes from the theory-based bias that we introduce into the model as a way to avoid the trap of multi-causalism. Specifically, we select a nation's educational system as our model's leading determinant, for educational institutions and particularly schools more than other institutions both shape a nation's beliefs about “how things ought to be done,” and transmit those beliefs and practices to successive generations. We draw the theoretical grounding for this bias using a theory of socialization which posits that the schemas that individuals internalize during their early formative stages of development greatly influence the way that they will later construct reality. We found our model to be useful in clarifying the underlying historical reasons why British and French firms rely on different integrative mechanisms when establishing headquarters-subsidiary linkages. Indeed, by analyzing the history of these two nations and the routines and rationalities imposed by their respective set of institutions in the context of our model, we find an overall mosaic, or gestalt, that is fully consistent with their present-day administrative practices. For example, we find that French managers today tend to heavily rely on centralized controls and headquarters intervention because of the influences of a number of key historical elements. These include the way their ancestors dealt with threats of invasion and ethnic diversity; the leadership styles of past French monarchies and elected officials; the manner in which the French colonies were controlled and the French school system organized; and the way the French government responded to economic problems that came from being a late participant in the two industrial revolutions. These also include the philosophies of the revered seventeenth-century French thinkers; the family structures that have existed in France since the middle ages; and the enduring influence of the Catholic ethos. Similarly, we use our model to present a different, yet equally compelling, mosaic image to explain why British managers have tended to rely on an administrative heritage, based on decentralization, “laissezfaire” and autonomy. Finally, and perhaps most important, we show that the explanation we arrived at using this inductive method of theory building is fully consistent with the empirical conclusions we derived using contemporary methods of organization science. In so doing, we demonstrate that these two approaches, when combined, can inform and enrich theory building and thereby lead to a more thorough understanding of organizational phenomena such as administrative heritage."
/doi/10.1287/mnsc.1100.1177," If entrepreneurs are liquidity constrained and not able to borrow to operate on an efficient scale, economic theory predicts that entrepreneurs with more personal wealth should do better than those with less wealth. We test this hypothesis using a novel data set covering a large panel of start-ups from Norway. Consistent with liquidity constraints, we find a positive relation between founder prior wealth and start-up size. The relationship between prior wealth and start-up performance, as measured by profitability on assets, increases in the first three wealth quartiles. In the top wealth quartile, however, profitability drops sharply in wealth. Our findings are consistent with a luxury good interpretation of entrepreneurship and that higher wealth may induce a less alert or a less dedicated management. We conclude that an abundance of resources might do more harm than good for start-ups."
/doi/10.1287/isre.1120.0449," Existing research provides little insight into how social influence affects the adoption and diffusion of competing innovative artifacts and how the experiences of organizational members who have worked with particular innovations in their previous employers affect their current organizations' adoption decision. We adapt and extend the heterogeneous diffusion model from sociology and examine the conditions under which prior adopters of competing open source software (OSS) licenses socially influence how a new OSS project chooses among such licenses and how the experiences of the project manager of a new OSS project with particular licenses affects its susceptibility to this social influence. We test our predictions using a sample of 5,307 open source projects hosted at SourceForge. Our results suggest the most important factor determining a new project's license choice is the type of license chosen by existing projects that are socially closer to it in its inter-project social network. Moreover, we find that prior adopters of a particular license are more infectious in their influence on the license choice of a new project as their size and performance rankings increase. We also find that managers of new projects who have been members of more successful prior OSS projects and who have greater depth and diversity of experience in the OSS community are less susceptible to social influence. Finally, we find a project manager is more likely to adopt a particular license type when his or her project occupies a similar social role as other projects that have adopted the same license. These results have implications for research on innovation adoption and diffusion, open source software licensing, and the governance of economic exchange."
/doi/10.1287/msom.1070.0198," Moving production to low-wage countries may reduce manufacturing costs, but it increases logistics costs and is subject to foreign trade barriers, among others. This paper studies a manufacturer's multimarket facility network design problem and investigates the offshoring decision from a network capacity investment perspective. We analyze a firm that manufactures two products to serve two geographically separated markets using a common component and two localized final assemblies. The common part can be transported between the two markets that have different economic and demand characteristics. Two strategic network design questions arise naturally: (1) Should the common part be produced centrally or in two local facilities? (2) If a centralization strategy is adopted, in which market should the facility be located? We present a transportation cost threshold that captures costs, revenues, and demand risks, and below which centralization is optimal. The optimal location of commonality crucially depends on the relative magnitude of price and manufacturing cost differentials but also on demand size and uncertainty. Incorporating scale economies further enlarges the centralization's optimality region."
/doi/10.1287/msom.2018.0735," Contract farming is a growing practice in developing countries and first-world economies alike. It generates necessary guarantees to sustain the continued operations of vulnerable farmers while enabling the manufacturers to manage the aggregate supply and price risk. We consider a single manufacturer who owns several manufacturing plants, each with a random demand for the crop. The manufacturer selects a set of farmers to offer a menu of contracts, which is exogenously specified or endogenously determined. Each “selected” farmer chooses a contract from this menu in advance of the growing season. After the growing season, under known demands and supplies, the manufacturer minimizes the distribution costs from the selected farmers to the production facilities. We formulate this problem as a Stackelberg game with asymmetric information, where the manufacturer is the leader and the farmers are the followers. The manufacturer’s problem is a two-stage stochastic planning program for which we develop two solution approaches. We have applied our model to problem instances anchored on data from a large manufacturer of potato chips contracting with thousands of small farmers in India. We report on the performance of the solution methods compared with a lower bound based on the Lagrangean dual of the problem and show that the optimality gap is below 1%, for problem instances with 1,000 potential farmers. We also show how our model can be used to gain various managerial insights. As an example, when constructing the contract menu endogenously, often a small number of contract options suffices, depending on the degree of heterogeneity among the farmer pool. Thus, relatively simple menus often suffice. The online appendices are available at https://doi.org/10.1287/msom.2018.0735 . This paper has been accepted for the Manufacturing & Service Operations Management Special Issue on Value Chain Innovations in Developing Economies."
/doi/10.1287/mnsc.2016.2707," Extensive debate exists among policy makers and economists about the employment of highly skilled immigrants in the United States. It remains unclear whether these immigrants perform complementary tasks in addition to their substitutive role relative to native graduates. Empirical studies examining these questions in a focused setting are scarce, principally because of the nonavailability of data. We examine these questions using the audit industry as a setting because of the availability of client, city, and office characteristic data at each audit office. This setting also allows us to answer whether immigration can address the growing human capital constraint in the audit industry. We find evidence of a complementary role of highly skilled immigrants. In addition, our results indicate a reputational spillover of client restatements at the audit office level to the labor markets. Our findings have immigration and education policy implications. This paper was accepted by Shivaram Rajgopal, accounting."
/doi/10.1287/orsc.2017.1190," This paper explains and tests empirically why people employed in product promotion are less willing to trust others. Product promotion is a prototypical setting in which employees are mandated to express attitudes that are often not fully sincere. On the basis of social projection theory, we predicted that organizational agents mandated to express insincere attitudes project their self-perceived dishonesty onto others and thus become more distrustful. An initial large-scale, multi-country field study found that individuals employed in jobs requiring product promotion were less trusting than individuals employed in other jobs—particularly jobs in which honesty is highly expected. We then conducted two experiments in which people were tasked with promoting low-quality products and either were allowed to be honest or were asked to be positive (as would be expected of most salespeople). We found that mandated attitude expression reduced willingness to trust, and this effect was mediated by a decrease in the perceived honesty of the self, which, in turn, reduced the perceived honesty of other people. Our research suggests that the widely used practice of mandating attitude expression has the effect of undermining an essential ingredient of economic functioning—trust."
/doi/10.1287/msom.2016.0599," We examine the impact of information provision policies on farmer welfare in developing countries where farmers lack relevant and timely information for making informed decisions regarding which crop to grow and which market to sell in. In addition to heterogeneous farmers, we consider the case when farmers are price takers and yet the price of each crop (or the price in each market) is a linearly decreasing function of the total sales quantity. When market information is offered free of charge, we show that (a) providing information is always beneficial to farmers at the individual level and (b) providing information to all farmers may not be welfare maximizing at the aggregate level. To maximize farmer welfare, it is optimal to provide information to a targeted group of farmers who are located far from either markets. However, to overcome perceived unfairness among farmers, we show that the government should provide information to all farmers at a nominal fee so that the farmers will adopt the intended optimal provision policy willingly. We extend our analysis to examine different issues including information leakage, social welfare, precision of market information, and information dissemination via a for-profit company. This paper has been accepted for the Manufacturing & Service Operations Management Special Issue on Value Chain Innovations in Developing Economies."
/doi/10.1287/mksc.18.3.435," We organize the existing theoretical pricing research into a new two-level framework for industrial goods pricing. The first level consists of four pricing situations: New Product, Competitive, Product Line, and Cost-based. The second level consists of the pricing strategies appropriate for a given situation. For example, within the new product pricing situation, there are three alternative pricing strategies: Skim, Penetration, and Experience Curve pricing. There are a total of ten pricing strategies included in the framework. We then identified a set of cost, product, market, and information conditions which determine what pricing situation(s) a firm is facing as well as which strategies are appropriate within a given situation. Some of these determinant conditions are common to many pricing strategies (e.g., highly elastic demand) while others are unique to a given strategy within a particular pricing situation. For example, within the product line situation, the profitability of supplementary sales is a unique determinant of the Complementary Product pricing strategy (razor-and-blade pricing). Using this framework as a basis for an empirical study, we examined how well current industrial pricing practice matches the prescriptions from the existing research. Our sample consisted of 270 respondents (27% response rate). Of these, more than 50% indicated that they used more than one pricing strategy in formulating their most recent pricing decision for a high-value industrial product sold in the United States. As in previous research, Cost-Plus pricing was the most often cited pricing strategy (56% of the respondents). Since the respondents were able to indicate their use of more than one pricing strategy, the data are of the “pick k from n ” variety. In order to model the managers' pricing strategy choices, we constructed a “stacked” binary logit with a separate observation for each strategy within a given pricing situation. The signs of the determinant variables were estimated as interaction terms. The new product pricing strategies (skim, penetration, experience curve) were used for new models in the market. Skim pricing was used in markets with high levels of product differentiation by firms at a cost disadvantage due to scale. Penetration pricing was used by firms with a cost advantage due to scale in markets with high level of overall elasticity but low brand elasticity. Experience curve pricing was used for minor innovations by firms with low capacity utilization in markets with a high level of differentiation. The competitive pricing strategies (Leader, Parity, and Low-price Supplier) were used in mature markets. Parity pricing was used by firms in a poor competitive situation, i.e., high costs, low market share, low product differentiation. These firms were also unable to take advantage of high levels of elasticity since their capacity utilization was high. In contrast, the low-price supplier strategy was used by firms with low costs due to scale advantages. Since they have low utilization, these firms can take advantage of elastic brand demand. None of the determinants were significantly related to the choice of leader pricing. Product line pricing strategies (Bundling, Complementary Product, and Customer Value pricing) were more likely to be used by firms which sell substitute or complementary products. Bundle pricing was used for per-sale/contract pricing in markets with high levels of brand elasticity. Complementary product pricing (razor-and-blade) was used by firms that enjoyed high profitability on its supplementary sales. Using customer value pricing, a firm offers a stripped down version of its current products to appeal to more price sensitive segments or to leverage new distribution channels. This strategy was used to target a narrow segment in high growth markets where price changes are difficult to detect. Cost-based pricing was more likely to be used in markets where demand is very difficult to estimate. In such a situation, cost-based pricing makes a great deal of sense. In general, the results show that the managers' pricing strategy choices are consistent with normative pricing research. However, questions about how managers combine their strategies to arrive at a final price as well as the organizational influences on pricing strategies remain important areas for future research."
/doi/10.1287/msom.2.3.297.12347," The third-party logistics industry has grown rapidly in recent years, accounting for $46 billion of the total $921 billion in logistics spending in the United States during 1999. This figure is expected to grow by 15 to 20% annually as manufacturing firms increasingly partner with third-party logistics providers to cost effectively distribute their products, while meeting increasingly stringent service expectations of customers. These logistics partnerships have introduced a new set of decision requirements to negotiate compensation for distribution services. Based on a collaborative project with a leading building products manufacturer, this paper describes the development and implementation of a novel linear programming model to decide the delivery fees paid to distributors. The model applies to manufacturer-distributor partnerships where distributors are compensated using fee values that depend on delivery weights and distances. It ensures that the expected compensation, considering stochastic demands, is adequate to cover the aggregate distribution costs for each distributor, and permits imposing various consistency conditions to ensure that fee values are credible. The model proved effective in helping the manufacturer develop a new fee table that generated considerable economic savings and provided more equitable compensation to distributors."
/doi/10.1287/orsc.8.5.475," The impact of interorganizational systems (IOSs) on the structure of market networks is analyzed from a management perspective. A research framework is applied to various organizational settings, yielding a range of mixed mode forms in which elements of both market and hierarchy are evident. These forms are more complex than the simple network or hybrid structures postulated in the management and information systems literature. The framework represents a departure from electronic markets theory, questioning its basic predictions that as companies trade electronically there will be proportionately more markets than hierarchies. Instead, IOSs make possible relationships that combine market and hierarchy elements simultaneously. Although economic forces are driving the changes in network structure, economic variables are tempered by individual firm strategies reflecting investment, network structure and IOS choices. The implications for theory are that traditional analyses of network structures and competition in business markets do not describe or explain adequately the structure and dynamics of competition in an electronic trading environment. The implications for managers are that they should consider the effects of mixed mode network structures on their processes for forming and managing business relationships supported by IOSs. The contribution of the paper is to provide a more accurate model of competition in business markets by demonstrating that multiple forms of mixed mode network structures exist. The mixed mode proposition is illustrated with case data from a range of mixed mode network structures."
/doi/10.1287/mnsc.2020.3717," Although researchers often view earnings management as being widespread, measuring the cost and level of earnings management is a nontrivial task. We derive a measure of earnings management cost and the associated equilibrium level of earnings management from the cross-sectional properties of earnings and prices. This approach enables us to separate economic shocks from reporting discretion by modeling the economic tradeoff faced by management. The tradeoff can be easily estimated from a closed-form likelihood function. Consistent with prior studies, the measure suggests more earnings management during seasoned equity offerings, for smaller and growing firms, as well as in industries with more irregularities. This paper was accepted by Suraj Srinivasan, accounting."
/doi/10.1287/opre.2020.2098," Replicating portfolios have emerged as an important tool in the life insurance industry, used for the valuation of companies’ liabilities. This paper describes the replicating portfolio (RP) model used to approximate life insurance liabilities in a large global insurance company. We describe the challenges presented by the latest solvency regimes in Europe and how the RP model enables this company to comply with the Swiss Solvency Test. The model minimizes the L 1 error between the discounted life insurance liability cash flows and the discounted RP cash flows over a multiperiod time horizon for a broad range of different future economic scenarios. A numerical application of the RP model to empirical data sets demonstrates that the model delivers RPs that match the liabilities and perform well for economic capital calculations."
/doi/10.1287/mnsc.1100.1207," This paper studies how firms reorganize following diversification, proposing that firms use outsourcing, or vertical disintegration, to manage diseconomies of scope. We also consider the origins of scope diseconomies, showing how different underlying mechanisms generate contrasting predictions about the link between within-firm task heterogeneity and the incentive to outsource following diversification. We test these propositions using microdata on taxicab and limousine fleets from the Economic Census. The results show that taxicab firms outsource, by shifting the composition of their fleets toward owner-operator drivers, when they diversify into the limousine business. The magnitude of the shift toward driver ownership is larger in less urban markets, where the tasks performed by taxicab and limousine drivers are more similar. These findings suggest that (1) firms use outsourcing to manage diseconomies of scope at a particular point in the value chain and (2) interagent conflicts can be an important source of scope diseconomies."
/doi/10.1287/ited.5.2.64ca," Few terms in the recent literature on innovation management have been as widely used as the phrase disruptive technology . Yet this term is widely misunderstood. As Christensen and Raynor (2003, p. 143) note, “Many readers have equated in their minds the terms disruptive and breakthrough . It is extremely important, for purposes of prediction and understanding, not to confuse the terms.” This case shows how to recognize a potentially disruptive technology: it diffuses via a process of low-end encroachment . The new technology initially imposes little apparent threat because it sells to low-end or new customers, but it eventually encroaches on the current market from the low end upward. The case compares and contrasts low-end encroachment (using the example of a new, smaller disk drive involving the firms Seagate and Quantum) to the case of high-end encroachment , where a new product such as a Pentium IV initially sells to high-end customers. In an oversimplified but concrete way, the case shows how marketing concepts such as conjoint analysis and reservation price tie into the concept of a demand curve, and how operations improvements (via the learning curve, or via product and process design) can lead to market share changes and product diffusion. These all have implications on firm strategy. Thus the case integrates material from multiple disciplines and might be used in MBA, MS, executive, or undergraduate courses that cover topics such as technology management, operations strategy, new product development, marketing strategy, or competitive strategy."
/doi/10.1287/msom.1030.0031," Effective distribution using collaborative fulfillment networks requires coordination among the multiple participating firms at different stages of the supply chain. Acting independently, supply chain partners fail to weigh the cost burden they impose on upstream suppliers when their replenishment order quantities vary from period to period. This paper explores a new approach to coordinate multiple stages in the supply chain by controlling, through appropriate downstream inventory management, the demand variability that is propagated to upstream stages. We propose and analyze a coordinated inventory replenishment policy that uses ""order smoothing"" to reduce order-size variability and thus reduce overall system costs, including both inventory and transportation costs. We characterize the optimal parameter values for smoothing alternatives (such as exponential smoothing and moving weighted average policies), assess their economic benefits, and develop insights regarding supply chain contexts that might benefit most significantly from reducing the variability of orders to upstream stages. Using the distribution network for specialty brand appliances as an illustrative example, we demonstrate the potential cost savings that order-smoothing strategies can yield compared to the uncoordinated case when individual firms separately minimize their costs. The magnitude of savings depends on several factors, including the variability in consumer demand, level of product variety, and degree of inventory aggregation in the distribution system. Based on our analytical results, we develop a framework to assess cost reduction opportunities through variability control for different supply chain scenarios."
/doi/10.1287/mnsc.2019.3411," Consumers regard product delivery as an important service component that influences their shopping decisions on online retail platforms. Delivering products to customers in a timely and reliable manner enhances customer experience and companies’ profitability. In this research, we explore the extent to which customers value a high-quality delivery experience when shopping online. Our identification strategy exploits a natural experiment: a clash between SF Express and Alibaba, the largest private logistics service provider with the highest reputation in delivery quality in China and the largest online retail platform in China, respectively. The clash resulted in Alibaba unexpectedly removing SF Express as a shipping option from Alibaba’s retail platform for 42 hours in June 2017. Using a difference-in-differences design, we analyze the market performance of 129,448 representative stock-keeping units on Alibaba to quantify the economic value of a high-quality delivery service to sales, product variety, and logistics rating. We find that the removal of the high-quality delivery option from Alibaba’s retail platform reduced sales by 14.56% during the clash, increased the contribution of long-tail to total sales—sales dispersion—by 3%, but did not impact the variety and logistics rating of sold products. Furthermore, we also identify product characteristics that attenuate the value of high-quality logistics and find that the removal of SF Express is more obstructive for (1) star products as compared with long-tail products because the same star products are likely to be supplied by competing retail platforms that customers can easily switch to, (2) expensive products because customers need a reliable delivery service to protect their valuable items from damage or loss, and (3) less-discounted products because customers are more willing to sacrifice the service quality over a price markdown. This paper was accepted by Victor Martínez-de-Albéniz, operations management."
/doi/10.1287/mksc.2016.0989," We randomize advertising content motivated by the psychology literature on sympathy generation and framing effects in mailings to about 185,000 prospective new donors in India. We find a significant impact on the number of donors and amounts donated consistent with sympathy biases such as the “identifiable victim,” “in-group,” and “reference dependence.” A monthly reframing of the ask amount increases donors and the amount donated relative to daily reframing. A second field experiment targeted to past donors, finds that the effect of sympathy bias on giving is smaller in percentage terms but statistically and economically highly significant in terms of the magnitude of additional dollars raised. Methodologically, the paper complements the work of behavioral scholars by adopting an empirical researchers’ lens of measuring relative effect sizes and economic relevance of multiple behavioral theoretical constructs in the sympathy bias and charity domain within one field setting. Beyond the benefit of conceptual replications, the effect sizes provide guidance to managers on which behavioral theories are most managerially and economically relevant when developing advertising content. Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0989 ."
/doi/10.1287/isre.1120.0469," Despite the popular use of social media by consumers and marketers, empirical research investigating their economic values still lags. In this study, we integrate qualitative user-marketer interaction content data from a fan page brand community on Facebook and consumer transactions data to assemble a unique data set at the individual consumer level. We then quantify the impact of community contents from consumers (user-generated content, i.e., UGC) and marketers (marketer-generated content, i.e., MGC) on consumers' apparel purchase expenditures. A content analysis method was used to construct measures to capture the informative and persuasive nature of UGC and MGC while distinguishing between directed and undirected communication modes in the brand community. In our empirical analysis, we exploit differences across consumers' fan page joining decision and across timing differences in fan page joining dates for our model estimation and identification strategies. Importantly, we also control for potential self-selection biases and relevant factors such as pricing, promotion, social network attributes, consumer demographics, and unobserved heterogeneity. Our findings show that engagement in social media brand communities leads to a positive increase in purchase expenditures. Additional examinations of UGC and MGC impacts show evidence of social media contents affecting consumer purchase behavior through embedded information and persuasion. We also uncover the different roles played by UGC and MGC, which vary by the type of directed or undirected communication modes by consumers and the marketer. Specifically, the elasticities of demand with respect to UGC information richness are 0.006 (directed communication) and 3.140 (undirected communication), whereas those for MGC information richness are insignificant. Moreover, the UGC valence elasticity of demand is 0.180 (undirected communication), whereas that for MGC valence is 0.004 (directed communication). Overall, UGC exhibits a stronger impact than MGC on consumer purchase behavior. Our findings provide various implications for academic research and practice."
/doi/10.1287/inte.32.5.47.36," Protecting ourselves cost-effectively from such global environmental threats as climate change requires a good understanding of the behavioral underpinnings that drive choice under risk and uncertainty, lead to the design of efficient and effective protocols for cooperative action across nations, and create incentives for different control strategies within nations. Researchers can use experimental economic methods to gain insight into these micromotives of climate policy by testing the robustness of theoretical predictions, and by recognizing new patterns of choice. Experiments can be used to sharpen the best guesses guiding policy by identifying how decision makers can try to get more environmental progress at less cost by accounting for the relevant determinants of behavior."
/doi/10.1287/opre.2018.1806," Stable alliance structures among critical (monopoly) component suppliers in a decentralized assembly system are somewhat well understood. However, when there are competing suppliers for any particular component, less is known about such alliances. The intent of this paper is to address some of the theoretical issues that pose challenges in analyzing stable supplier coalitions in such assembly systems. We examine a simple assembly system in which suppliers sell n distinct complementary components to a downstream assembler, who faces a price-sensitive deterministic demand. We assume that k of these components have multiple competing suppliers and that the remaining n − k suppliers are monopolists. We analyze alliance/coalition formation between suppliers using a two-stage approach that is common in the literature. When some suppliers face competition, predictions on stable supplier alliances are fraught with technical difficulties; we resolve these by showing an asymptotic invariance result. We use this in Stage 1 of the game to predict the structure of the stable supplier coalitions using a dynamic version of stability that accounts for players’ farsightedness."
/doi/10.1287/serv.2015.0114," Service orientation is the prevalent paradigm for modular distributed systems, giving rise to service ecosystems defined by software dependencies, which, at the same time, carry business and economic implications. And as software evolves, so do the business relationships among the ecosystem participants, with corresponding economic impact. Therefore, a more comprehensive model of software evolution is necessary in this context, to support the decision-making processes of the ecosystem participants. In this work, we view the ecosystem as a market environment, with providers offering competing services and developing these services to attract more clients by better satisfying their requirements. Based on an economic model for calculating the costs and values associated with service evolution, we develop a game-theoretic model to capture the interactions between providers and clients and support the providers’ decision-making process. We demonstrate the use of our model with a realistic example of a cloud-services ecosystem."
/doi/10.1287/mnsc.2014.2134," In this paper, we study the impact of consumer-generated quality information (e.g., consumer reviews) on a firm’s dynamic pricing strategy in the presence of strategic consumers. Such information is useful, not only to the consumers that have not yet purchased the product but also to the firm. The informativeness of the consumer-generated quality information depends, however, on the volume of consumers who share their opinions and, thus, depends on the initial sales volume. Hence, via its initial price, the firm not only influences its revenue but also controls the quality information flow over time. The firm may either enhance or dampen the quality information flow via increasing or decreasing initial sales. The corresponding pricing strategy to steer the quality information flow is not always intuitive. Compared to the case without consumer-generated quality information, the firm may reduce the initial sales and lower the initial price. Interestingly, the firm may get strictly worse off due to the consumer-generated quality information. Even when the firm benefits from consumer-generated quality information, it may prefer less accurate information. Consumer surplus can also decrease due to the consumer-generated quality information, contrary to the conventional wisdom that word of mouth should help consumers. We examine extensions of our model that incorporate capacity investment, firm’s private information about quality, alternative updating mechanisms, as well as multiple sales periods, and show that our insights are robust. This paper was accepted by Yossi Aviv, operations management."
/doi/10.1287/mnsc.1060.0609," This paper presents new evidence regarding a firm’s probability for survival, based on the network structure of the firm’s managers. We found that start-ups that have larger informal communication networks increased their chance to survive external shock. Original data have been collected from Israeli software start-ups during the dot-com economic growth. About eight years later, we added information about their ability to survive the burst of the dot-com bubble. From a theoretical point of view, this paper highlights the power of the classic social networks approach in explaining organizational performance. From a practical point of view, these findings offer some guidelines for managers of start-ups. Our results show that the size of informal interfirm networks really matters."
/doi/10.1287/mksc.2015.0975," This paper examines the incentives of firms to invest in socially responsible product innovations. Our analysis connects the existence of socially responsible innovations to the presence of intrinsic and extrinsic social responsibility preferences. In addition to deriving economic value from the product, consumers have heterogeneous intrinsic needs to consume products that are socially responsible. They also have extrinsic social comparison preferences that are based on their meetings with others in social interactions. The frequency of these meetings are endogenous to the consumption choices of consumers. A consumer enjoys a social comparison benefit if her consumption decision is more socially responsible than the consumer that she meets in a social interaction and a social comparison cost if it is less socially responsible. The analysis reveals a nonmonotonic effect of social comparison effects on innovation incentives. When the economic value of a product is relatively small, the incentive to innovate decreases as social comparison effects increase. By contrast, when the economic value of a product is sufficiently large, increases in social comparison effects increase the incentive to innovate. Social comparison benefits and costs have different effects on competition between firms. In particular, social comparison benefits soften price competition, whereas social comparison costs tend to exacerbate price competition. We also identify market conditions where a monopoly invests more or less compared to a firm facing competition."
/doi/10.1287/mksc.2015.0965," The marketing and operations disciplines have increasingly accounted for the presence of strategic consumer behavior. Theory suggests that such behavior exists when consumers are able to consider future distribution of prices, and that this behavior exposes firms to intertemporal competition that results with a downward pressure on prices. However, deriving future distribution of prices is not a trivial task. Online decision support tools that provide consumers with information about future distributions of prices can facilitate strategic consumer behavior. This paper studies whether the availability of such information affects transacted prices by conducting an empirical analysis in the context of the airline industry. Studying the effect at the route level, we find significant price reduction effects as such information becomes available for a route, both in fixed-effects and difference-in-differences estimation models. This effect is consistent across the different fare percentiles and amounts to a reduction of approximately 4%–6% in transactions’ prices. Our results lend ample support to the notion that price prediction decision tools make a statistically significant economic impact. Presumably, consumers are able to exploit the information available online and exhibit strategic behavior. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0965 ."
/doi/10.1287/mksc.22.2.188.16041," Sales takeoff is vitally important for the management of new products. Limited prior research on this phenomenon covers only the United States. This study addresses the following questions about takeoff in Europe: 1) Does takeoff occur as distinctly in other countries, as it does in the United States? 2) Do different categories and countries have consistently different times-to-takeoff? 3) What economic and cultural factors explain the intercountry differences? 4) Should managers use a sprinkler or waterfall strategy for the introduction of new products across countries? We gathered data on 137 new products across 10 categories and 16 European countries. We adapted the threshold rule for identifying takeoff (Golder and Tellis 1997) to this multinational context. We specify a parametric hazard model to answer the questions above. The major results are as follows: 1) Sales of most new products display a distinct takeoff in various European countries, at an average of six years after introduction. 2) The time-to-takeoff varies substantially across countries and categories. It is four times shorter for entertainment products than for kitchen and laundry appliances. It is almost half as long in Scandinavian countries as in Mediterranean countries. 3) While culture partially explains intercountry differences in time-to-takeoff, economic factors are neither strong nor robust explanatory factors. 4) These results suggest distinct advantages to a waterfall strategy for introducing products in international markets."
/doi/10.1287/msom.2017.0673," Problem definition : A large proportion of the world’s population has no access to electricity and so relies on noxious kerosene  for their lighting needs. Solar-based solutions require a large up-front investment and are often unaffordable in this market owing to consumers’ tight  liquidity constraints. As an alternative, there are business models relying on rechargeable light bulbs that are sold at a subsidized price (which renders them  affordable) and require regular micropayments for recharges (which eases liquidity constraints). These bulbs provide a cheaper and healthier light source than kerosene,  yet their adoption is lower than expected, and some consumers continue to use kerosene. This paper explores the potential drivers of such preferences and proposes  strategies to alter them. Academic/practical relevance : Unlike most of the existing operations management literature, which focuses on the  problems in developed economies, our paper studies a problem specific to the poor population. Our novel modeling approach, which incorporates several operational  features of the impoverished regions, could also serve as a template for other potential modeling attempts in similar settings. Methodology : We propose a stylized consumer behavior model that accounts for—in addition to the monetary cost incurred while using a particular light source—the inconvenience cost (resulting from repeated travel to the purchase center) and blackout cost (resulting from liquidity constraints) associated with that source to explain the consumer preference for kerosene and to recommend  strategies that could mitigate that preference. Results : Although kerosene lighting is more expensive than bulbs, consumers who face  either high inconvenience costs or high blackout costs prefer kerosene to bulbs because the former’s flexibility, with regard to quantity, helps reduce  whichever cost is dominating. At the firm level, there is an optimal bulb capacity and recharge price pair that maximizes the firm’s revenue; furthermore, a  firm can reverse the preferences for kerosene by increasing the flexibility of the bulbs (e.g., by allowing partial recharges). Although strategies—such as  price discounts and mobile micropayments—which alleviate liquidity constraints are not in themselves sufficient to ensure higher adoption rates, increased  bulb use becomes more likely when they are combined with inconvenience-reducing strategies. Managerial implications : Our paper sheds  light on the structure of the market in which firms operate by identifying the characteristics of the market segments that prefer kerosene. It also helps the firms make better decisions by evaluating the efficacy of several strategies in terms of increasing the adoption of bulbs. The online appendix is available at https://doi.org/10.1287/msom.2017.0673 . This paper has been accepted for the Manufacturing & Service Operations Management Special Issue on Value Chain Innovations in Developing Economies."
/doi/10.1287/stsc.2017.0037," Great business strategies originate in a top management worldview that seeks to implement bold strategic change in the external world. This worldview generates confident strategic actions capable of producing extreme success. However, it also generates catastrophic failures and lower returns on average. This is because action-oriented managers pay abnormal attention to the observables of sense experience while neglecting the world of absence, a bias I refer to as absence-neglect. Absence-neglect is the cognitive tendency to notice presence more than absence—for example, movement more than stillness and noise more than silence. This paper examines the origins of absence-neglect and its consequences for strategy formation, firm performance, and social welfare. I argue that societies can reduce the economic and moral risks of “great” strategies by cultivating a more balanced discernment of presence and absence."
