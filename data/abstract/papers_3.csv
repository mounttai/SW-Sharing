link,abstract
/doi/10.1287/isre.2017.0740," The Internet has assumed a central role in the global economy facilitating commerce and communication and is thus central to many areas of information systems (IS) research. In particular, IS researchers played a critical role in the academic discourse on net neutrality, which has recently informed new regulatory frameworks in the United States and Europe. We discuss and categorize the various issues and key trade-offs that are still being debated in the context of net neutrality and identify open research questions in this domain. Based on these insights, we argue that net neutrality, which is concerned with a gatekeeper at the infrastructure level, may just be part of a larger debate on data neutrality where the gatekeeper may rather control a software platform. We provide several examples of potential data neutrality issues and generalize the key trade-offs in the context of a proposed four-step framework for identifying and organizing promising areas of IS research on data neutrality."
/doi/10.1287/mnsc.2013.1815," We study how the equilibrium risk sharing of agents with heterogeneous perceptions of aggregate consumption growth affects bond and stock returns. Although credit spreads and their volatilities increase with the degree of heterogeneity, the decreasing risk premium on moderately levered equity can produce a violation of basic capital structure no-arbitrage relations. Using bottom-up proxies of aggregate belief dispersion, we give empirical support to the model predictions and show that risk premia on corporate bond and stock returns are systematically explained by their exposures to aggregate disagreement shocks. This paper was accepted by Jerome Detemple, finance ."
/doi/10.1287/orsc.2016.1099," In this paper, we explore the psychological experience of university-educated local workers from emerging economies striving to enter the global job market for managerial positions. Building on qualitative data from sub-Saharan Africa and the Arab Gulf, we conducted two experimental studies in the Arab Gulf to test whether local job candidates feel inhibited to self-advocate for higher compensation in global employment contexts and whether they believe that such negotiating behavior is less appropriate in global than in local work contexts. We theorize that shifting from local to global employment contexts, university-educated locals experience a decline in their status as workers because of a perceived lack of fit with the cosmopolitan “ideal worker.” We find that the contrasting global and local labor-market experiences of local job candidates are moderated by gender because local men experience a greater shift in status between local and global employment contexts than do women. This research contributes to the study of status and gender effects on negotiation by illuminating differential constraints of status and gender on negotiating behavior. This research also has important practical implications for the integration and advancement of workers from emerging economies into global institutions and for our broader understanding of how intersecting status-linked social identities influence career negotiations."
/doi/10.1287/isre.9.1.85," The impact of information technology (IT) investments on firm performance has been the subject of active research in recent years. However, findings of almost all studies are based on data collected in the United States. Little work has been done elsewhere to validate these results and to see if they are applicable across national boundaries. In this study, we fill this gap by comparing four newly-industrialized economies (NIEs) with regard to the impact of IT capital on business performance. Secondary data collected from various sources are used to assess the impact over the period from 1983 to 1991. Findings based on four business measures and a market valuation model based on Tobin's q are reported. While the current results are consistent with work done in the United States in general, discrepancies among the four NIEs are observed. Combined with findings from previous work, three pieces of evidence seem to emerge that are generally observed across country boundaries. First, IT investment is not correlated with shareholder's return. Second, there is little evidence that the level of computerization is valued by the market in developed and newly-developed countries. Third, there is no consistent measurement of IT investment as indicated by the mixed results across different performance ratios. Modeling and measurement concerns expressed in previous U.S.-based studies are also observed in our comparative study. Our findings provide a starting point to accumulate a body of comparative studies for the development of a theory that links IT investment, firm performance, and macro factors such as national technology policy in an integrated framework."
/doi/10.1287/trsc.2021.1068," With the soaring popularity of ride-hailing, the interdependence between transit ridership, ride-hailing ridership, and urban congestion motivates the following question: can public transit and ride-hailing coexist and thrive in a way that enhances the urban transportation ecosystem as a whole? To answer this question, we develop a mathematical and computational framework that optimizes transit schedules while explicitly accounting for their impacts on road congestion and passengers’ mode choice between transit and ride-hailing. The problem is formulated as a mixed integer nonlinear program and solved using a bilevel decomposition algorithm. Based on computational case study experiments in New York City, our optimized transit schedules consistently lead to 0.4%–3% system-wide cost reduction. This amounts to rush-hour savings of millions of dollars per day while simultaneously reducing the costs to passengers and transportation service providers. These benefits are driven by a better alignment of available transportation options with passengers’ preferences—by redistributing public transit resources to where they provide the strongest societal benefits. These results are robust to underlying assumptions about passenger demand, transit level of service, the dynamics of ride-hailing operations, and transit fare structures. Ultimately, by explicitly accounting for ride-hailing competition, passenger preferences, and traffic congestion, transit agencies can develop schedules that lower costs for passengers, operators, and the system as a whole: a rare win–win–win outcome."
/doi/10.1287/mnsc.45.5.704," This study uses personally collected data from 41 steel production lines to assess the effects of Japanese and U.S. human resource management (HRM) practices on worker productivity. The Japanese production lines employ a common system of HRM practices including: problem-solving teams, extensive orientation, training throughout employees' careers, extensive information sharing, rotation across jobs, employment security, and profit sharing. A majority of U.S. plants now have one or two features of this system of HRM practices, but only a minority have a comprehensive system of innovative work practices that parallels the full system of practices found among the Japanese manufacturers. We find that the Japanese lines are significantly more productive than the U.S. lines. However, U.S. manufacturers that have adopted a full system of innovative HRM practices patterned after the Japanese system achieve levels of productivity and quality equal to the performance of the Japanese manufacturers. This study's evidence helps reconcile conflicting views about the effectiveness of adopting Japanese-style worker involvement schemes in the United States. United States manufacturers that have adopted a definition of employee participation that extends only to problem-solving teams or information sharing do not see large improvements in productivity. However, U.S. manufacturers that adopt a broader definition of participation that mimics the full Japanese HRM system see substantial performance gains."
/doi/10.1287/orsc.2018.1207," Goals and the performance feedback on those goals are fundamental to organizational learning and adaptation. However, most research has focused on single overall, high-level organizational goals while ignoring important operational goals farther down in the goal hierarchy. This paper explores the important issue of interdependent feedback on multiple operational goals with shared task environments. We conjecture about the impact of shared technological task environments on feedback across goals. We then empirically examine these conjectures using panel vector autoregression (PVAR) analysis of performance feedback from three strategically important operational goals with shared technological task environments in the automobile industry. We find that interdependent feedback can lead to severe and misleading confusion regarding learning from feedback on such goals with shared task environments. Then, we discuss the implications of our findings. These include the following: the absolute intractability of the problem of meeting multiple goals with interdependent task environments as the number of goals increases; limits on the modularity of organization structure; and severe challenges in ex post credit assignment and ex ante planning when goals share technological task environments. Finally, we discuss the application of PVAR to interdependent feedback problems in organizations. The online appendices are available at https://doi.org/10.1287/orsc.2018.1207 ."
/doi/10.1287/orsc.1100.0567," This multimethod study investigates the effects of entrepreneurs' interpersonal networking style on the initiation of interorganizational exchange ties. I use inductive theorizing to make a distinction between interpersonal networking actions aimed at adding new contacts (network-broadening actions) versus managing existing contacts (network-deepening actions). I reason that because networking actions alter the cost-benefit calculus of using referrals, the extent to which entrepreneurs rely on referrals when searching for new exchange partners should vary with their networking actions. I then propose that entrepreneurs are likely to add fewer new exchange partners when they rely more on referrals to search. The empirical analysis employs a longitudinal design using data coded from the business cards of new contacts formed over a two-month period by a panel of Indian entrepreneurs operating business-to-business ventures. This study makes a theoretical contribution by identifying decision makers' networking style as a distinct mechanism shaping partner selection for their organization. Specifically, the study shows entrepreneurs using more network-deepening actions initiate fewer new economic exchanges, due (in part) to their increased reliance on referral-based search, whereas entrepreneurs using more network-broadening actions initiate more new economic exchanges due (in part) to their decreased reliance on referral-based search."
/doi/10.1287/msom.2019.0831," Problem definition : To alleviate farmer poverty, governments and nongovernmental organizations (NGOs) are developing different mechanisms for disseminating market information to farmers in developing countries. This paper examines whether a wider dissemination of information will always benefit farmers. Academic/practical relevance : The characterization of the farmers’ equilibrium production decisions and the implications of information provision provide practical guidance for governments and NGOs when developing agricultural information services. Methodology : We develop an “asymmetric” two-stage game to analyze a base model in which n ≥ 4 heterogeneous farmers need to determine their production quantities when the underlying market condition is uncertain. Our base model relies on four key modeling assumptions: (1) farmers engage in Cournot (quantity) competition, (2) the social planner cares about farmers’ total income, (3) yields are deterministic, and (4) information service is publicly funded so that information access is free. We further examine alternative market environments by relaxing or changing these modeling assumptions separately. Results : Through the analysis of our base model, we find that providing information to only one farmer is optimal but providing information to all n farmers can be detrimental. In alternative market environments, we obtain the following results. First, when the information service is self-financed, we find that the “exclusivity” result no longer holds when farmers are heterogeneous (even though it holds for the case when farmers are homogeneous). Second, when yield rates are uncertain, we find it is optimal to disseminate to more farmers when the yield rates become more uncertain (in terms of coefficient of variation). Third, when the central planner only cares about creating economic value for those farmers with information access, it is optimal for the social planner to disseminate information to more farmers. Fourth, when farmers engage in Bertrand (price) competition, it is optimal to disseminate information to all n farmers. Managerial implications : By combining different results obtained under different market environments, we conclude that the optimal information provision policy depends on the competition type (Cournot or Bertrand competition), yield uncertainty, source of funding, and the social planner’s ultimate goal."
/doi/10.1287/mnsc.49.8.1018.16402," This paper considers pricing and capacity sizing decisions, in a single-class Markovian model motivated by communication and information services. The service provider is assumed to operate a finite set of processing resources that can be shared among users; however, this shared mode of operation results in a service-rate degradation. Users, in turn, are sensitive to the delay implied by the potential degradation in service rate, and to the usage fee charged for accessing the system. We study the equilibrium behavior of such systems in the specific context of pricing and capacity sizing under revenue and social optimization objectives. Exact solutions to these problems can only be obtained via exhaustive simulations. In contrast, we pursue approximate solutions that exploit large-capacity asymptotics . Economic considerations and natural scaling relations demonstrate that the optimal operational mode for the system is close to “heavy traffic.” This, in turn, supports the derivation of simple approximate solutions to economic optimization problems, via asymptotic methods that completely alleviate the need for simulation. These approximations seem to be extremely accurate. The main insights that are gleaned in the analysis follow: congestion costs are “small,” the optimal price admits a two-part decomposition, and the joint capacity sizing and pricing problem decouples and admits simple analytical solutions that are asymptotically optimal. All of the above phenomena are intimately related to statistical economies of scale that are an intrinsic part of these systems."
/doi/10.1287/isre.2021.1040," We study a scenario in which a buyer (e.g., Uber) buys cloud capacity from a seller (e.g., Amazon Web services) to run its business. One of the key factors that affects the quality of cloud services is congestion, and it has drawn considerable attention in recent years. Congestion leads to a potential loss of end users (e.g., riders and drivers of Uber), thereby adversely affecting the demand for cloud services. Discount has been a useful mean to stimulate demand and reward customer loyalty. However, in the presence of congestion, the effect of discount on demand is ambiguous. On the one hand, a higher discount leads to higher demand; on the other hand, higher demand can lead to higher congestion, thereby lowering the demand. Given that end users are both price and congestion sensitive, the choice of optimal discount under congestion is, therefore, not straightforward. Using a game-theoretic model, we study the dynamics between congestion and discount and explore how congestion moderates both the buyer’s and seller’s optimal decisions. Our results show that the buyer is not necessarily worse off even when the end users are more intolerant to congestion. In fact, we find that when end users are more congestion sensitive, the demand of cloud services can actually sometimes increase, and the discount offered by the seller can decrease. These findings have important managerial implications on the seller’s pricing and capacity decisions. We also observe that a lower cost of technology can sometimes hurt the buyer, and the buyer can pass on lower benefits to end users. Moreover, given that the cloud services are prone to disruptions, a buyer sources from multiple cloud vendors, which further complicates the matter. We draw useful insights about the buyer’s procurement decisions under congestion in a multicloud setup."
/doi/10.1287/opre.2021.2161," We consider a periodic-review inventory control problem for the Multi-Warehouse Multi-Store system with lost sales. We focus on a time horizon during which the system receives no external replenishment. Specifically, each warehouse has a finite initial inventory at the beginning of the horizon, which is then periodically allocated to the stores in each period in order to minimize the total expected lost-sales costs, holding costs, and shipping costs. This is a hard problem and the structure of its optimal policy is extremely complex. We develop simple heuristics based on Lagrangian relaxation that are easy to compute and implement, and have provably near-optimal performances. In particular, we show that the losses of our heuristics are sublinear in both the length of the time horizon and the number of stores . This improves the performance of existing heuristics in the literature whose losses are only sublinear in the number of stores. Numerical study shows that the heuristics perform very well. We also extend our analysis to the setting of positive delivery lead times."
/doi/10.1287/inte.2019.1003," Overfishing is a prime environmental concern. Catch-share systems are an effective way to combat overfishing, but they introduce economic inefficiencies when the allocation of shares does not align with industry needs. This paper describes the implementation of a market-based approach to reallocating fishing shares in New South Wales (NSW), Australia. The design of the market needed to address several nonstandard requirements, including the possibility of all-or-nothing offers, fair prices, and an endogenously determined subsidy. These features were crucial for the adoption of the reform, but also led to computationally challenging allocation and pricing problems. The market operated from May to July 2017 and successfully reallocated shares from inactive fishers to those who were impacted the most by the introduction of catch shares, with significant savings for the taxpayer. It provides a template for the reallocation of catch shares elsewhere and of resource rights in other applications (e.g., water rights, pollution rights, and environmental offsets). We provide a project description, practical examples, and implementation details of the implemented exchange."
/doi/10.1287/mnsc.2021.3972," We study the impacts of social interactions on competing firms’ quality differentiation, pricing decisions, and profit performance. Two forms of social interactions are identified and analyzed: (1) market-expansion effect (MEE)—the total market expands as a result of both firms’ sales—and (2) value-enhancement effect (VEE)—a consumer gains additional utility of purchasing from one firm based on this firm’s previous and/or current sales volume. We consider a two-stage duopoly competition framework, in which both firms select quality levels in the first stage simultaneously and engage in a two-period price competition in the second stage. In the main model, we assume that each firm sets a single price and commits to it across two selling periods. We find that both forms of social interactions tend to lower prices and intensify price competition for given quality levels. However, MEE weakens the product-quality differentiation and is benign to both high-quality and low-quality firms. It also benefits consumers and improves social welfare. By contrast, VEE enlarges the quality differentiation and only benefits the high-quality firm, but is particularly malignant to the low-quality firm. It further reduces the consumers’ monetary surplus. Such impact is consistent, regardless of whether the VEE interactions involve previous or current consumers. We further discuss several model extensions, including dynamic pricing, combined social effects, and various cost structures, and verify that the aforementioned impacts of MEE and VEE are qualitatively robust to those extensions. Our results provide important managerial insights for firms in competitive markets and suggest that they need to not only be aware of the consumers’ social interactions, but also, more importantly, distinguish the predominant form of the interactions so as to apply proper marketing strategies. This paper was accepted by Matthew Shum, marketing."
/doi/10.1287/trsc.2021.1063," With recent advances in mobile technology, public transit agencies around the world have started actively experimenting with new transportation modes, many of which can be characterized as on-demand public transit. Design and efficient operation of such systems can be particularly challenging, because they often need to carefully balance demand volume with resource availability. We propose a family of models for on-demand public transit that combine a continuous approximation methodology with a Markov process. Our goal is to develop a tractable method to evaluate and predict system performance, specifically focusing on obtaining the probability distribution of performance metrics. This information can then be used in capital planning, such as fleet sizing, contracting, and driver scheduling, among other things. We present the analytical solution for a stylized single-vehicle model of first-mile operation. Then, we describe several extensions to the base model, including two approaches for the multivehicle case. We use computational experiments to illustrate the effects of the inputs on the performance metrics and to compare different modes of transit. Finally, we include a case study, using data collected from a real-world pilot on-demand public transit project in a major U.S. metropolitan area, to showcase how the proposed model can be used to predict system performance and support decision making."
/doi/10.1287/isre.2019.0894," Despite massive investments in transportation infrastructure, traffic congestion remains a major societal and public policy problem. Intelligent transportation systems (ITS) have been proposed as a potential solution to this challenge, but their effectiveness has remained unclear in both research and practice. To understand whether and how ITS affect traffic congestion, we consolidate a unique longitudinal data set on road traffic and the deployment of a large federally supported ITS program in the United States—511 systems—in 99 urban areas between 1994 and 2014. The difference-in-differences estimates show that the adoption of 511 systems is associated with a significant decrease in traffic congestion, saving over $4.7 billion dollars and 175 million hours in travel time annually in U.S. cities. 511 systems also reduce about 53 million gallons of fossil fuel consumption and over 10 billion pounds of CO 2 emissions. We offer two theoretical explanations for this effect: (i) ITS help individual commuters to make better travel decisions, and (ii) ITS help local governments to develop an urban traffic management capability. Empirical evidence supports the underlying theoretical mechanisms and shows that ITS help commuters to schedule travel more efficiently, choose better navigation routes, and optimize their work-trip transportation mode. Second, the effect of ITS is contingent on road supply and public transit services. We also find that the traffic-reducing effect of ITS is larger when commuters use more online services for traffic information and when state governments incorporate more functionalities into their 511 systems. This study contributes to the literature on IT capabilities, public-sector IT value, and the societal impact of IT, while also extending the transportation economics to IT-enabled traffic interventions. Finally, we inform policymakers of ITS as a cost-effective means to mitigating traffic congestion."
/doi/10.1287/isre.2017.0722," Firms, and other agencies, tend to adopt widely used software to gain economic benefits of scale, which can lead to a software monoculture. This can, in turn, involve the risk of correlated computer systems failure as all systems on the network are exposed to the same software-based vulnerabilities. Software diversity has been introduced as a strategy for disrupting such a monoculture and ultimately decreasing the risk of correlated failure. Nevertheless, common vulnerabilities can be shared by different software products. We thus expand software diversity research here and consider shared vulnerabilities between different software alternatives. We develop a combinatorial optimization model of software diversity on a network in an effort to identify the optimal software distribution that best improves network security. We also develop a simulation model of virus propagation based on the susceptible-infected-susceptible model. This model allows calculation of the epidemic threshold, a measure of network resilience to virus propagation. We then test the effectiveness of the proposed software diversity strategies against the spreading of viruses through a series of experiments."
/doi/10.1287/trsc.2021.1042," We consider the problem of an operator controlling a fleet of electric vehicles for use in a ride-hailing service. The operator, seeking to maximize profit, must assign vehicles to requests as they arise as well as recharge and reposition vehicles in anticipation of future requests. To solve this problem, we employ deep reinforcement learning, developing policies whose decision making uses Q -value approximations learned by deep neural networks. We compare these policies against a reoptimization-based policy and against dual bounds on the value of an optimal policy, including the value of an optimal policy with perfect information, which we establish using a Benders-based decomposition. We assess performance on instances derived from real data for the island of Manhattan in New York City. We find that, across instances of varying size, our best policy trained with deep reinforcement learning outperforms the reoptimization approach. We also provide evidence that this policy may be effectively scaled and deployed on larger instances without retraining."
/doi/10.1287/opre.1120.1155," This paper studies a repeated game between a manufacturer and two competing suppliers with imperfect monitoring. We present a principal-agent model for managing long-term supplier relationships using a unique form of measurement and incentive scheme. We measure a supplier's overall performance with a rating equivalent to its continuation utility (the expected total discounted utility of its future payoffs), and incentivize supplier effort with larger allocations of future business. We obtain the vector of the two suppliers' ratings as the state of a Markov decision process, and we solve an infinite horizon contracting problem in which the manufacturer allocates business volume between the two suppliers and updates their ratings dynamically based on their current ratings and the current performance outcome. Our contributions are both theoretical and managerial: we propose a repeated principal-agent model with a novel incentive scheme to tackle a common, but challenging, incentive problem in a multiperiod supply chain setting. Assuming binary effort choices and performance outcomes by the suppliers, we characterize the structure of the optimal contract through a novel fixed-point analysis. Our results provide a theoretical foundation for the emergence of “business-as-usual” (low effort) trapping states and tournament competition (high effort) recurrent states as the long-run incentive drivers for motivating critical suppliers."
/doi/10.1287/moor.2018.0961," We study Cournot competition among firms in a networked marketplace that is centrally managed by a market maker. In particular, we study a situation in which a market maker facilitates trade between geographically separate markets via a constrained transport network. Our focus is on understanding the consequences of the design of the market maker and providing tools for optimal design. To that end, we provide a characterization of the equilibrium outcomes of the game between the firms and the market maker. Our results highlight that the equilibrium structure is impacted dramatically by the market maker’s objective—depending on the objective, there may be a unique equilibrium, multiple equilibria, or no equilibria. Furthermore, the game may be a potential game (as in the case of classical Cournot competition) or not. Beyond characterizing the equilibria of the game, we provide an approach for designing the market maker to optimize a design objective (e.g., social welfare) at the equilibrium of the game. Additionally, we use our results to explore the value of transport (trade) and the efficiency of the market maker (compared with a single aggregate market)."
/doi/10.1287/isre.2017.0744," We study a novel privacy concern, i.e., peer disclosure of sensitive personal information in online social communities. We model peer disclosure as the imposition of a negative externality on other people. Our model encompasses the benefits of posting information, positive externalities in the form of recognition and entertainment benefits due to others’ sharing of information, and heterogeneous privacy preferences. We find that regulation of peer disclosure is necessary. We consider two candidate regulations, i.e., nudging and quotas. Nudging reduces user participation and privacy harm and sometimes improves social welfare. By contrast, imposing a quota often improves user participation, privacy protection, and social welfare. Adding a nudge on top of a quota does not bring additional benefits. We show that any regulation that uniformly controls the disclosure of sensitive and nonsensitive information will not serve the triple objectives of reducing privacy harm, increasing social welfare, and increasing information contribution. We derive a necessary condition for solutions that can fulfill these three objectives. We also compare the incentives of the platform owner and social planner and draw related managerial and policy implications. The online appendix is available at https://doi.org/10.1287/isre.2017.0744 ."
/doi/10.1287/msom.2020.0873," Problem definition : This paper studies the role of seekers’ problem specification in crowdsourcing contests for design problems. Academic/practical relevance : Platforms hosting design contests offer detailed guidance for seekers to specify their problems when launching a contest. Yet problem specification in such crowdsourcing contests is something the theoretical and empirical literature has largely overlooked. We aim to fill this gap by offering an empirically validated model to generate insights for the provision of information at contest launch. Methodology : We develop a game-theoretic model featuring different types of information (categorized as “conceptual objectives” or “execution guidelines”) in problem specifications and assess their impact on design processes and submission qualities. Real-world data are used to empirically test hypotheses and policy recommendations generated from the model, and a quasi-natural experiment provides further empirical validation. Results : We show theoretically and verify empirically that with more conceptual objectives disclosed in the problem specification, the number of participants in a contest eventually decreases; with more execution guidelines in the problem specification, the trial effort provision by each participant increases; and the best solution quality always increases with more execution guidelines but eventually decreases with more conceptual objectives. Managerial implications : To maximize the best solution quality in crowdsourced design problems, seekers should always provide more execution guidelines and only a moderate number of conceptual objectives."
/doi/10.1287/opre.2020.2049," We consider a setting where a platform dynamically allocates a collection of goods that arrive to the platform in an online fashion to budgeted buyers, as exemplified by online advertising systems where platforms decide which impressions to serve to various advertisers. Such dynamic resource allocation problems are challenging for two reasons. (a) The platform must strike a balance between optimizing the advertiser’s own revenues and guaranteeing fairness to the advertiser’s (repeat) buyers, and (b) the problem is inherently dynamic due to the uncertain, time-varying supply of goods available with the platform. We propose a stochastic approximation scheme akin to a dynamic market equilibrium. Our scheme relies on frequent resolves of an Eisenberg-Gale convex program and does not require the platform to have any knowledge about how the goods’ arrival processes evolve over time. The scheme fully extracts buyer budgets (thus maximizing platform revenues) and at the same time provides a 0.64 approximation of the proportionally fair allocation of goods achievable in the offline case, as long as the supply of goods comes from a wide family of (possibly nonstationary) Gaussian processes. We then deal with a multi-objective problem where the platform is concerned with both the proportional fairness and efficiency of the allocation and propose a hybrid algorithm that achieves a 0.3 bicriteria guarantee against fairness and efficiency. Finally, we build a sequence of datasets, one public dataset released by the DSP iPinYou and the second based on real Google AdX data, and use them to test the empirical performance of our schemes. We find that across these datasets there is a surprising relationship between fairness and efficiency that can be used to tune the schemes to nearly optimal performance in practice."
/doi/10.1287/mksc.2020.1280," Although typically overlooked, many purchase datasets exhibit a high incidence of products with zero sales. We propose a new estimator for the Random-Coefficients Logit demand system for purchase datasets with zero-valued market shares. The identification of the demand parameters is based on a pairwise-differencing approach that constructs moment conditions based on differences in demand between pairs of products. The corresponding estimator corrects nonparametrically for the potential selection of the incidence of zeros on unobserved aspects of demand. The estimator also corrects for the potential endogeneity of marketing variables both in demand and in the selection propensities. Monte Carlo simulations show that our proposed estimator provides reliable small-sample inference both with and without selection-on-unobservables. In an empirical case study, the proposed estimator not only generates different demand estimates than approaches that ignore selection in the incidence of zero shares, it also generates better out-of-sample fit of observed retail contribution margins."
/doi/10.1287/mnsc.2015.2309," In this paper, we study how restricting the availability of patches to legal users impacts the vendor’s profits, market share, software maintenance decisions, and welfare outcomes. Prior work on this topic assumes that the hacker’s effort is independent of the vendor’s decision to release the patch freely or not. Clearly, if the patch is not available to everyone, the hacker finds it easier to exploit the vulnerability in the product and, as a result, is likely to alter his effort. To understand the role of a strategic hacker, we build a game-theoretic model, where the hacker’s decision is endogenous. With this model, we find that the hacker’s effort may, on the one hand, decrease the utility that the vendor can extract from the consumers but, on the other hand, may help differentiate the legal version of the product from the pirated version. A vendor can strategically exploit the hacker’s behavior in its pricing and software maintenance decisions. The endogeneity of the hacker’s actions drives several of our findings that have interesting policy implications. For example, the vendor may increase the price and reduce market share to exploit the differentiation. In such a case, there may be more pirates in the restricted-patch case than when the patch is freely available, a result that runs counter to typical arguments provided for restricting patches. This paper was accepted by Chris Forman, information systems ."
/doi/10.1287/mnsc.2019.3316," Dynamic pricing through price promotions has been widely used by online retailers. We study how a promotion strategy, one that offers customers a discount for products in their shopping cart, affects customer behavior in the short and long term on a retailing platform. We conduct a randomized field experiment involving more than 100 million customers and 11,000 retailers with Alibaba Group, one of the world’s largest retailing platform. We randomly assign eligible customers to either receive promotions for products in their shopping cart (treatment group) or not receive promotions (control group). In the short term, our promotion program doubles the sales of promoted products on the day of promotion. In the long term, we causally document unintended consequences of this promotion program during the month after our treatment period. On the positive side, it boosts customer engagement, increasing the daily number of products that customers view and their purchase incidence on the platform. On the negative side, it intensifies strategic customer behavior in the posttreatment period in two ways: (1) by increasing the proportion of products that customers add to their shopping cart conditional on viewing them, possibly because of their intention to get more shopping cart promotions, and (2) by decreasing the price that customers subsequently pay for a product, possibly because of their strategic search for lower prices. Importantly, these long-term effects of price promotions on consumer engagement and strategic behavior spill over to sellers who did not previously offer promotions to customers. Finally, we examine heterogeneous treatment effects across promotion, seller, and consumer characteristics. These findings have important implications for platforms and retailers. This paper was accepted by Vishal Gaur, operations management."
/doi/10.1287/isre.2021.1048," Strategic sellers on some online selling platforms have recently been using a conditional-rebate strategy to manipulate product reviews under which only purchasing consumers who post positive reviews online are eligible to redeem the rebate. A key concern for the conditional rebate is that it can easily induce fake reviews, which might be harmful to consumers and society. We develop a microbehavioral model capturing consumers’ review-sharing benefit, review-posting cost, and moral cost of lying to examine the seller’s optimal pricing and rebate decisions. We derive three equilibria: the no-rebate, organic-review equilibrium; the low-rebate, boosted-authentic-review equilibrium; and the high-rebate, partially-fake-review equilibrium. We find that the seller’s optimal price and rebate decisions critically depend on both the review-posting and moral costs. The seller adopts the no-rebate strategy when the review-posting cost is low but the moral cost is high, the low-rebate strategy when the review-posting cost is high or when the review-posting cost is intermediate and the moral cost is high, and the high-rebate strategy when the review-posting cost is not too high and the moral cost is low. Our results suggest that it is not always profitable for strategic sellers to adopt the conditional-rebate strategy. Even if the conditional-rebate strategy is adopted, it does not always result in fake reviews. Furthermore, we find that, compared with the benchmark of no rebate, conditional rebates do not always hurt consumer surplus or social welfare. When a low (high) rebate is offered, if the review-posting cost is not too low (not very high), the conditional-rebate strategy can even lead to higher consumer surplus and social welfare. Our findings shed new light on the platform-policy debate about the fake-review phenomenon induced by conditional rebates."
/doi/10.1287/stsc.2017.0048," This paper outlines the theory-based view of strategy and markets. We argue that novel or “great” strategies come from theories. Entrepreneurs and managers originate theories and hypotheses about which activities they should engage in, which assets they might buy, and how they will create value. A firm’s strategy, then, represents a set of contrarian beliefs and a theory—a unique, firm-specific point of view—about what problems to solve, and how to organize and govern the overall process of value creation. We outline the cognitive and perceptual, organizational, and economic foundations of the theory-based view of strategy. We also discuss the essential attributes needed for a firm-level theory of strategy. Throughout the paper we offer informal examples of our argument, by briefly discussing the strategies of companies like Apple, Uber, Disney, Wal-Mart, and Airbnb. The theory-based view of strategy and markets also offers important insights for how firms govern themselves (including ownership, boards, and organization design) and how firms interact with capital markets and external evaluators and stakeholders. We conclude with a discussion of the practical and managerial applications of the theory-based view."
/doi/10.1287/isre.1120.0472," Digital technologies have made networks ubiquitous. A growing body of research is examining these networks to gain a better understanding of how firms interact with their consumers, how people interact with each other, and how current and future digital artifacts will continue to alter business and society. The increasing availability of massive networked data has led to several streams of inquiry across fields as diverse as computer science, economics, information systems, marketing, physics, and sociology. Each of these research streams asks questions that at their core involve “information in networks”—its distribution, its diffusion, its inferential value, and its influence on social and economic outcomes. We suggest a broad direction for research into social and economic networks. Our analysis describes four kinds of investigation that seem most promising. The first studies how information technologies create and reveal networks whose connections represent social and economic relationships. The second examines the content that flows through networks and its economic, social, and organizational implications. A third develops theories and methods to understand and utilize the rich predictive information contained in networked data. A final area of inquiry focuses on network dynamics and how information technology affects network evolution. We conclude by discussing several important cross-cutting issues with implications for all four research streams, which must be addressed if the ensuing research is to be both rigorous and relevant. We also describe how these directions of inquiry are interconnected: results and ideas will pollinate across them, leading to a new cumulative research tradition."
/doi/10.1287/opre.2017.1712," To systematically study the implications of additional information about routes provided to certain users (e.g., via GPS-based route guidance systems), we introduce a new class of congestion games in which users have differing information sets about the available edges and can only use routes consisting of edges in their information set. After defining the notion of an information-constrained wardrop equilibrium (ICWE) for this class of congestion games and studying its basic properties, we turn to our main focus: whether additional information can be harmful (in the sense of generating greater equilibrium costs/delays). We formulate this question in the form of an informational Braess’ paradox (IBP), which extends the classic Braess’ paradox in traffic equilibria and asks whether users receiving additional information can become worse off. We provide a comprehensive answer to this question showing that in any network in the series of linearly independent (SLI) class, which is a strict subset of series-parallel networks, the IBP cannot occur, and in any network that is not in the SLI class, there exists a configuration of edge-specific cost functions for which the IBP will occur. In the process, we establish several properties of the SLI class of networks, which include the characterization of the complement of the SLI class in terms of embedding a specific set of networks, and also an algorithm that determines whether a graph is SLI in linear time. We further prove that the worst-case inefficiency performance of ICWE is no worse than the standard Wardrop equilibrium."
/doi/10.1287/trsc.2021.1069," Current mobility services cannot compete on equal terms with self-owned mobility products concerning service quality. Because of supply and demand imbalances, ridesharing users invariably experience delays, price surges, and rejections. Traditional approaches often fail to respond to demand fluctuations adequately because service levels are, to some extent, bounded by fleet size. With the emergence of autonomous vehicles, however, the characteristics of mobility services change and new opportunities to overcome the prevailing limitations arise. In this paper, we consider an autonomous ridesharing problem in which idle vehicles are hired on-demand in order to meet the service-level requirements of a heterogeneous user base. In the face of uncertain demand and idle vehicle supply, we propose a learning-based optimization approach that uses the dual variables of the underlying assignment problem to iteratively approximate the marginal value of vehicles at each time and location under different availability settings. These approximations are used in the objective function of the optimization problem to dispatch, rebalance, and occasionally hire idle third-party vehicles in a high-resolution transportation network of Manhattan, New York City. The results show that the proposed policy outperforms a reactive optimization approach in a variety of vehicle availability scenarios while hiring fewer vehicles. Moreover, we demonstrate that mobility services can offer strict service-level contracts to different user groups featuring both delay and rejection penalties."
/doi/10.1287/orsc.1090.0424," This paper discusses some developments in the theory of the organizational capabilities of the business enterprise. Antecedents are recognized, and some promising new developments and areas for future research are identified. The role of managers in the economic system is highlighted and discussed within the context of economic and organizational research. Suggestions for future developments of dynamic capability research involve employment of evolutionary and behavioral theories."
/doi/10.1287/opre.2019.1931," We provide an approximation algorithm for network revenue management problems. In our approximation algorithm, we construct an approximate policy using value function approximations that are expressed as linear combinations of basis functions. We use a backward recursion to compute the coefficients of the basis functions in the linear combinations. If each product uses at most L resources, then the total expected revenue obtained by our approximate policy is at least 1 / ( 1 + L ) of the optimal total expected revenue. In many network revenue management settings, although the number of resources and products can become large, the number of resources used by a product remains bounded. In this case, our approximate policy provides a constant-factor performance guarantee. Our approximate policy can handle nonstationarities in the customer arrival process. To our knowledge, our approximate policy is the first approximation algorithm for network revenue management problems under nonstationary arrivals. Our approach can incorporate the customer choice behavior among the products, and allows the products to use multiple units of a resource, while still maintaining the performance guarantee. In our computational experiments, we demonstrate that our approximate policy performs quite well, providing total expected revenues that are substantially better than its theoretical performance guarantee."
/doi/10.1287/orsc.13.1.81.542," Competitive advantage is a key concept in strategic management research for a number of reasons—not the least of which is that an avowed consequence of its attainment is held to be superior economic performance. However, few prior empirical studies have directly and systematically documented the incidence or prevalence of persistent superior economic performance. The research reported here is based on empirical studies of a large number of industry samples for which longitudinal data were stratified by levels of performance using a new methodology and then analyzed in terms of their dynamics. This new stratification technique was used in lieu of autoregressive methods employed in prior studies of performance persistence to allow for a true outlier analysis because persistent superior economic performance both has been argued theoretically, and found empirically, to be rare. Detailed results from a sample of 6,772 firms in 40 industries over 25 years are presented to illustrate the findings that: (1) while some firms do exhibit superior economic performance, (2) only a very small minority do so, and (3) the phenomenon very rarely persists for long time frames. These results, while not providing direct support for a particular extant strategic management or economic theory concerning firm performance, are most consonant with the resource-based view of the firm and have implications for significant aspects of other received strategic management and economic theories."
/doi/10.1287/msom.2020.0935," Problem definition : We study the contest duration and the award scheme of an innovation contest where an organizer elicits solutions to an innovation-related problem from a group of agents. Academic/practical relevance : Our interviews with practitioners at crowdsourcing platforms have revealed that the duration of a contest is an important operational decision. Yet, the theoretical literature has long overlooked this decision. Also, the literature fails to adequately explain why giving multiple unequal awards is so common in crowdsourcing platforms. We aim to fill these gaps between the theory and practice. We generate insights that seem consistent with both practice and empirical evidence. Methodology : We use a game-theoretic model where the organizer decides on the contest duration and the award scheme while each agent decides on her participation and determines her effort over the contest duration by considering potential changes in her productivity over time. The quality of an agent’s solution improves with her effort, but it is also subject to an output uncertainty. Results : We show that the optimal contest duration increases as the relative impact of the agent uncertainty on her output increases, and it decreases if the agent productivity increases over time. We characterize an optimal award scheme and show that giving multiple (almost always) unequal awards is optimal when the organizer’s urgency in obtaining solutions is below a certain threshold. We also show that this threshold is larger when the agent productivity increases over time. Finally, consistent with empirical findings, we show that there is a positive correlation between the optimal contest duration and the optimal total award. Managerial implications : Our results suggest that the optimal contest duration increases with the novelty or sophistication of solutions that the organizer seeks, and it decreases when the organizer can offer support tools that can increase the agent productivity over time. These insights and their drivers seem consistent with practice. Our findings also suggest that giving multiple unequal awards is advisable for an organizer who has low urgency in obtaining solutions. Finally, giving multiple awards goes hand in hand with offering support tools that increase the agent productivity over time. These results help explain why many contests on crowdsourcing platforms give multiple unequal awards."
/doi/10.1287/serv.2015.0095," In this paper, we propose a network economic model of cybercrime with a focus on financial services, since such organizations are one of the principal targets of such illicit activity. This multiproduct model is constructed as a layered bipartite network with supply price, transaction cost, and demand price functions linking the networks. A novelty of the new model is the incorporation of average time associated with illicit product delivery at the demand markets with the demand price functions being decreasing functions of such times, as noted in reality. The governing equilibrium conditions are formulated as a variational inequality problem with qualitative properties of the solution presented. An algorithm, with nice features for computations, is then applied to two sets of numerical examples in order to illustrate the model and computational procedure as well as the types of interventions that can be investigated from a policy perspective to make it more difficult for cybercriminals to obtain sensitive data."
/doi/10.1287/mnsc.2019.3485," We consider the problem faced by a firm that receives highly differentiated products in an online fashion. The firm needs to price these products to sell them to its customer base. Products are described by vectors of features and the market value of each product is linear in the values of the features. The firm does not initially know the values of the different features, but can learn the values of the features based on whether products were sold at the posted prices in the past. This model is motivated by applications such as online marketplaces, online flash sales, and loan pricing. We first consider a multidimensional version of binary search over polyhedral sets and show that it has a worst-case regret which is exponential in the dimension of the feature space. We then propose a modification of the prior algorithm where uncertainty sets are replaced by their Löwner-John ellipsoids. We show that this algorithm has a worst-case regret which is quadratic in the dimension of the feature space and logarithmic in the time horizon. We also show how to adapt our algorithm to the case where valuations are noisy. Finally, we present computational experiments to illustrate the performance of our algorithm. This paper was accepted by Yinyu Ye, optimization."
/doi/10.1287/mnsc.2014.2044," Using the news-based measure of Baker et al. [Baker SR, Bloom N, Davis SJ (2013) Measuring economic policy uncertainty. Working paper, Stanford University, Stanford, CA] to capture economic policy uncertainty (EPU) in the United States, we find that EPU positively forecasts log excess market returns. An increase of one standard deviation in EPU is associated with a 1.5% increase in forecasted three-month abnormal returns (6.1% annualized). Furthermore, innovations in EPU earn a significant negative risk premium in the Fama–French 25 size–momentum portfolios. Among the Fama–French 25 portfolios formed on size and momentum returns, the portfolio with the greatest EPU beta underperforms the portfolio with the lowest EPU beta by 5.53% per annum, controlling for exposure to the Carhart four factors as well as implied and realized volatility. These findings suggest that EPU is an economically important risk factor for equities. This paper was accepted by Wei Jiang, finance."
/doi/10.1287/msom.2017.0700," Problem definition : Microretailers in remote rural areas in developing countries face high replenishment costs because of poor road infrastructure and the lack of formal distribution channels. This paper investigates the effectiveness of two innovative replenishment strategies (purchasing cooperatives and nonprofit wholesalers) deployed by nongovernmental organizations to reduce microretailers’ replenishment costs and improve consumer welfare. Academic/practical relevance : The problem is relevant in practice, as travel cost has been documented as a major cost burden that leads to meager earnings for the microretailers, few retailers in the market, and high prices to the consumers (because of less competition). Analyzing how these innovative strategies alleviate the previously described problems enables us to develop practical insights as well as fill an important gap in the literature. Methodology : We adopt a stylized model that captures price competition, consumer welfare, and the market entry decision of microretailers under the replenishment strategies considered. We compare the equilibrium retailer profit, consumer welfare, and the number of retailers entering the market under these strategies. Results : We find that in a regulated market, a nonprofit wholesaler creates supply chain inefficiency, and thus can lead to a higher retail price, which benefits the retailers yet harms the consumers. However, with free market entry (unregulated market), we show that under certain market conditions, both the cooperative and the nonprofit wholesaler strategies can be Pareto improving. Yet between these two strategies, there typically exists a trade-off in both regulated and unregulated markets, that is, the cooperative strategy enhances consumer welfare, while the wholesaler strategy leads to higher retailer profits. Managerial implications : The policy maker needs to be mindful about the market conditions and the relative emphasis between the profit and market participation of microretailers and consumer welfare when choosing and implementing the purchasing cooperative and the nonprofit wholesaler strategies. The online appendix is available at https://doi.org/10.1287/msom.2017.0700 ."
/doi/10.1287/mksc.2014.0890," Many firms operate customer communities online. This is motivated by the belief that customers who join the community become more engaged with the firm and/or its products, and as a result, increase their economic activity with the firm. We describe this potential economic benefit as “social dollars.” This paper contributes evidence for the existence and source of social dollars using data from a multichannel entertainment products retailer that launched a customer community online. We find a significant increase in customer expenditures attributable to customers joining the firm’s community. While self-selection is a concern with field data, we rule out multiple alternative explanations. Social dollars persist over the time period observed and arose primarily in the online channel. To assess the source of the social dollar, we hypothesize and test whether it is moderated by participation behaviors conceptually linked to common attributes of customer communities. Our results reveal that posters (versus lurkers) of community content and those with more (versus fewer) social ties in the community generated more (fewer) social dollars. We found a null effect for our measure of the informational advantage expected to accrue to products that differentially benefit from content posted by like-minded community members. This overall pattern of results suggests a stronger social than informational source of economic benefits for firm operators of customer communities. Several implications for firms considering investments in and/or managing online customer communities are discussed."
/doi/10.1287/mnsc.2013.1806," Data obtained from monthly Gallup/UBS surveys from 1998 to 2007 and from a special supplement to the Michigan Surveys of Consumer Attitudes and Behavior, run in 22 monthly surveys between 2000 and 2005, are used to analyze stock market beliefs and portfolio choices of household investors. We show that the key variables found to be positive predictors of actual stock returns in the asset-pricing literature are also highly correlated with investor's subjective expected returns, but with the opposite sign. Moreover, our analysis of the microdata indicates that subjective expectations of both risk and returns on stocks are strongly influenced by perceptions of economic conditions. In particular, when investors believe macroeconomic conditions are more expansionary, they tend to expect both higher returns and lower volatility. This is difficult to reconcile with the canonical view that expected returns on stocks rise during recessions to compensate household investors for increased exposure or sensitivity to macroeconomic risks. Finally, the relevance of these investors' subjective expectations is supported by the finding of a significant link between their expectations and portfolio choices. In particular, we show that portfolio equity positions tend to be higher for those respondents that anticipate higher expected returns or lower uncertainty. This paper was accepted by Brad Barber, finance."
/doi/10.1287/opre.2020.2017," For online resource allocation problems, we propose a new demand arrival model where the sequence of arrivals contains both an adversarial component and a stochastic one. Our model requires no demand forecasting; however, because of the presence of the stochastic component, we can partially predict future demand as the sequence of arrivals unfolds. Under the proposed model, we study the problem of the online allocation of a single resource to two types of customers and design online algorithms that outperform existing ones. Our algorithms are adjustable to the relative size of the stochastic component; our analysis reveals that as the portion of the stochastic component grows, the loss due to making online decisions decreases. This highlights the value of (even partial) predictability in online resource allocation. We impose no conditions on how the resource capacity scales with the maximum number of customers. However, we show that using an adaptive algorithm—which makes online decisions based on observed data—is particularly beneficial when capacity scales linearly with the number of customers. Our work serves as a first step in bridging the long-standing gap between the two well-studied approaches to the design and analysis of online algorithms based on (1) adversarial models and (2) stochastic ones. Using novel algorithm design, we demonstrate that even if the arrival sequence contains an adversarial component, we can take advantage of the limited information that the data reveal to improve allocation decisions. We also study the classical secretary problem under our proposed arrival model, and we show that randomizing over multiple stopping rules may increase the probability of success."
/doi/10.1287/mnsc.1120.1670," This study reports findings from the first large-scale experiment investigating whether entrepreneurs differ from other people in their willingness to expose themselves to various forms of uncertainty. A stratified random sample of 700 chief executive officers from the Yangzi delta region in China is compared to 200 control group members. Our findings suggest that in economic decisions, entrepreneurs are more willing to accept strategic uncertainty related to multilateral competition and trust. However, entrepreneurs do not differ from ordinary people when it comes to nonstrategic forms of uncertainty, such as risk and ambiguity. This paper was accepted by John List, behavioral economics."
/doi/10.1287/mksc.16.1.1," Retailers in developed countries increasingly provide one-stop shopping. Why? Are they responding to growing demand for time-saving convenience? Or are they responding to economies of scale made possible by new retail technology? And what role, if any, is played by improvements in automotive, refrigeration, and other consumer technology? To examine these questions, we develop a model that can help explain the growth of one-stop shopping, estimate the model with aggregate grocery retail data, and compare the implications of our model with competing explanations. Our model includes several desirable features: (1) The extent of one-shop shopping is endogenously determined. (2) Consumer store choice is determined in an explicit household-production, utility-maximizing context. This choice is based on a tradeoff between time-saving shopping convenience and price. (3) The model is readily amenable to empirical testing. The key endogenous parameter of our model describes the number of categories carried by a store. In the context of grocery retailing, this parameter can distinguish between a single-category specialty store, a simple grocer, a supermarket, and a large superstore. This parameter is determined by the interplay of consumers economizing on shopping time and retail competition in the presence of technological constraints which relate operating cost with assortment size. Our approach may also be viewed as an example of how the boundary between retail service production and household service production can be modeled as a market outcome (in the spirit of Betancourt and Gautchi [Betancourt, Roger, David Gautschi. 1990. Demand complementarities, household production, and retail assortments. Marketing Sci. 9 (2, Spring) 146–161.] and Wernerfelt [Wernerfelt, Birger. 1994. An efficiency criterion for marketing design. J. Marketing Res. 31 (November) 462–470.]). The nature of extant retail formats and other channel-related institutions circumscribes the boundary between retailers and households. With the notable early exception of Baumol and Ide (Baumol, William J., Edward A. Ide. 1956. Variety in retailing. Management Sci. 3 (1, October) 93–101.), few explicit models describe retail format determination as an equilibrating market mechanism. Our model describes the determination of one aspect of retail formats (namely, the extent of one-stop shopping). Our hope is that similar approaches can be used to describe other aspects and types of emergent retail formats. We estimate our model using U.S. aggregate annual data for a 26-year period, and later corroborate parts of the analysis using cross-sectional data for the 48 contiguous states of the U.S. The equilibrium conditions of our model translate into a system of two equations with two endogenous variables. Estimates of the reduced form parameters provide our central empirical finding—that per capita disposable income has had a significant positive effect on both supermarket assortment (as expected from our model) and store operating costs (perhaps somewhat less expected). This suggests that greater prevalence of one-stop shopping has been a response to growing demand for time-saving convenience. The estimates of the underlying structural parameters also provide a measure of the imputed net savings to consumers from supermarket shopping, taking into account reductions in shopping time made possible by supermarkets (savings equivalent to approximately 2.2% of expenditures on grocery products over the 1961–1986 period). We lastly consider competing hypotheses. While a number of explanations can account for increased store assortment, these explanations have divergent implications for retail margins, operating costs, and profits. These implications suggest that retail scale economies were not the drivers of one-stop shopping in the years of our data. Nor was the impetus to increase store size primarily a desire to bring in new higher-margin items. However, we acknowledge that transportation and inventory-holding technologies are prerequisites to one-stop shopping and that improvements in these technologies were particularly important in the early growth period of supermarkets."
/doi/10.1287/serv.2014.0081," One key challenge in the current Internet is the inefficiency of the mechanisms by which technology is deployed and the business and economic models surrounding these processes. Customers' demands are driving the Internet and telecommunication networks toward the provision of quality-based end-to-end services, which need a richer family of performance guarantees. We believe that novel insights into future Internet structures can be obtained from taking into account the associated economic models and equilibrium conditions among providers. This paper develops a basic and a general network economic game theory model of a quality-based service-oriented Internet to study the competition among the service providers (both content and network providers). We derive the governing equilibrium conditions and provide the equivalent variational inequality formulations. An algorithm is proposed that yields closed-form expressions, at each iteration, for the prices and quality levels. To illustrate the modeling framework and the algorithm, we present computed solutions to numerical examples. The results show the generality of the proposed network economic model for a future Internet."
/doi/10.1287/mnsc.44.4.501," This paper is another plea for bridging behavioral and economic approaches to the study of competition in markets and strategy making by firms. The arguments focus on a specific case in point: the behavioral theory of organizational decline and the economic modeling of immediate exit. The arguments come in three steps. First, the literature on organizational decline is reviewed by organizing a framework that summarizes arguments from varying economic and organizational perspectives that have, for the most part, developed independently. Observations from empirical and theoretical studies are combined in order to investigate the causes, conditions, courses, and consequences of organizational downturn. Second, a theoretical argument is developed that explains voluntary exit and chronic failure by introducing a proxy of organizational inertia in a model of strategic Cournot duopoly. The key assumptions, which have a behavioral flavour that seemingly contradicts orthodox economics, are grounded in the theoretical and empirical literatures. The results of the model support the claim that “pure profit maximizing behavior may be at the expense of organizational survival” (D'Aveni 1990, p. 135). Third, by formulating two hypotheses and presenting tentative evidence from the chemical industry, the paper hopes to convincingly argue that such integrative models lead to empirical testing of interesting hypotheses. A key finding here is that inefficient firms may outlast their efficient rivals (cf. D'Aveni 1989a)."
/doi/10.1287/mksc.1060.0211," Consumer choice in surveys and in the marketplace reflects a complex process of screening and evaluating choice alternatives. Behavioral and economic models of choice processes are difficult to estimate when using stated and revealed preferences because the underlying process is latent. This paper introduces Bayesian methods for estimating two behavioral models that eliminate alternatives using specific attribute levels. The elimination by aspects theory postulates a sequential elimination of alternatives by attribute levels until a single one, the chosen alternative, remains. In the economic screening rule model, respondents screen out alternatives with certain attribute levels and then choose from the remaining alternatives, using a compensatory function of all the attributes. The economic screening rule model gives an economic justification as to why certain attributes are used to screen alternatives. A commercial conjoint study is used to illustrate the methods and assess their performance. In this data set, the economic screening rule model outperforms the EBA and other standard choice models and provides comparable results to an equivalent conjunctive screening rule model."
/doi/10.1287/mnsc.2016.2453," A critical component of both economic and perceptual decision making under uncertainty is the belief-formation process. However, most research has studied belief formation in economic and perceptual decision making in isolation. One reason for this separate treatment may be the assumption that there are distinct psychological mechanisms that underlie belief formation in economic and perceptual decisions. An alternative theory is that there exists a common mechanism that governs belief formation in both domains. Here, we test this alternative theory by combining a novel computational modeling technique with two well-known experimental paradigms. We estimate a drift-diffusion model (DDM) and provide an analytical method to decode prior beliefs from DDM parameters. Subjects in our experiment exhibit strong extrapolative beliefs in both paradigms. In line with the common mechanism hypothesis, we find that a single computational model explains belief formation in both tasks and that individual differences in belief formation are correlated across tasks. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2016.2453 . This paper was accepted by Yuval Rottenstreich, judgment and decision making ."
/doi/10.1287/mnsc.2016.2585," Using a data set of approximately 3,000 commercial banks from more than 100 countries, I examine the impact of financial consumer protection policies on the cost of financial intermediation. I find evidence that the existence of internal mechanisms for handling complaints, requirements for fair treatment, supervisory power related to consumer protection, and various disclosure requirements reduce the cost of financial intermediation in advanced countries. The results are different in the case of developing countries, where I observe that most financial consumer protection policies increase the cost of intermediation, suggesting that banks pass on regulatory burdens to their customers. This paper was accepted by Amit Seru, finance ."
/doi/10.1287/isre.2013.0474," Open source software (OSS) communities live and die with the continuous contributions of programmers who often participate without direct remuneration. An intriguing question is whether such sustained participation in OSS projects yields economic benefits to the participants. Moreover, as participants engage in OSS projects, they take on different roles and activities in the community. This raises additional questions of whether different forms of participation in OSS communities are associated with different economic rewards and, if so, in which contexts. In this paper, we draw upon theories of signaling and job matching to hypothesize that participants who possess “proof” of their skills in OSS projects are financially rewarded for their activities in the labor market. More specifically, we distinguish between participation in OSS communities that is associated with a signaling value for unobserved productivity characteristics and an additional value that accrues to participants whose OSS roles and activities match those in their paid employment. Following a cohort of OSS programmers over a six-year period, we empirically examine the wages and OSS performance of participants in three of the foremost OSS projects operating within the Apache Software Foundation. Controlling for individual characteristics and other wage-related factors, our findings reveal that credentials earned through a merit-based ranking system are associated with as much as an 18% increase in wages. Moreover, we find that participants who have OSS project management responsibilities receive additional financial rewards if their professional job is in IT management. These findings suggest that rank within an OSS meritocracy is a credible and precise signal of participants' productive capacity and that participants' roles and activities in an OSS community have additional financial value when aligned with their paid employment."
/doi/10.1287/mnsc.2015.2179," Newly public companies tend to exhibit abnormally high accruals in the year of their initial public offering (IPO). Although the prevailing view in the literature is that these accruals are caused by opportunistic misreporting, we show that these accruals do not appear to benefit managers and instead result from the normal economic activity of newly public companies. In particular, and in contrast to the notion that managers benefit from inflating accruals through an inflated issue price, inflated post-IPO equity values, and increased insider trading profits, we find no evidence of a relation between abnormal accruals and these outcomes. Instead, consistent with these accruals resulting from normal economic activity, we find that these accruals are attributable to the investment of IPO proceeds in working capital and that controlling for the amount of IPO proceeds invested in working capital produces a more powerful accrual-based measure of misreporting. This paper was accepted by Gérard Cachon, accounting ."
/doi/10.1287/mnsc.2013.1723," Four distinct theoretical programs have examined market entry decisions of multiunit firms, advancing different explanations for the relationship between a firm's likelihood of entry into a geographical market and the number of rivals that are already present in the target market. Within the strategy literature, theory of strategic interactions explains that firms will want to establish a foothold in markets where their multimarket competitors are scarce, but avoid markets where there are many multimarket competitors. Within economic geography, positive externalities such as increase in demand explain firms' desire to locate close to their rivals whereas negative externalities such as competition explain their desire to avoid them. Within the ecological tradition, density dependence theory explains this relationship in terms of legitimation of an organizational form in a particular market and subsequently increased competition for resources there. Within new institutional theory, the presence of rivals is seen as a signal that a particular market is suitable for entry. Although generally quoted and mentioned in the literature, these four explanations have not been sufficiently separated to indicate whether these four mechanisms all operate simultaneously or whether one of them might account for the often found inverse-U-shaped relationship. Distinguishing firms with different strategies and using various moderators, we test the four explanations jointly and demonstrate their scope of operation. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2013.1723 . This paper was accepted by Jesper Sørensen, organizations."
/doi/10.1287/orsc.1080.0354," In this paper we develop and test predictions regarding the impact of CEO status on the economic outcomes of top management team members. Using a unique data set incorporating Financial World's widely publicized CEO of the Year contest, we found that non-CEO top management team members received higher pay when they worked for a high-status CEO. However, star CEOs themselves retained most of the compensation benefits. We also show that there is a “burden of celebrity” in that the above relationships were contingent on how well a firm performs. Last, we found that, when compared with the subordinates of less-celebrated CEOs, members of top management teams who worked for star CEOs were more likely to become CEOs themselves through internal or external promotions."
/doi/10.1287/opre.1110.0917," In response to Assembly Bill 32, the state of California considered three types of carbon emissions trading programs for the electric power sector: load-based, source-based, and first-seller. They differed in terms of their point of regulation and in whether in-state-to-out-of-state and out-of-state-to-in-state electricity sales are regulated. In this paper, we formulate a market equilibrium model for each of the three approaches, considering power markets, transmission limitations, and emissions trading, and making the simplifying assumption of pure bilateral markets. We analyze the properties of their solutions and show the equivalence of load-based, first-seller, and source-based approaches when in-state-to-out-of-state sales are regulated under the cap. A numeric example illustrates the emissions and economic implications of the models. In the simulated cases, “leakage” eliminates most of the emissions reductions that the regulations attempt to impose. Furthermore, “contract reshuffling” occurs to such an extent that all the apparent emissions reductions resulting from changes in sources of imported power are illusory. In reality, the three systems would not be equivalent because there will also be pool-type markets, and the three systems provide different incentives for participating in those markets. However, the equivalence results under our simplifying assumptions show that load-based trading has no inherent advantage compared to other systems in terms of costs to consumers, contrary to claims elsewhere."
/doi/10.1287/mnsc.2017.2829," Studies of bond return predictability find a puzzling disparity between strong statistical evidence of return predictability and the failure to convert return forecasts into economic gains. We show that resolving this puzzle requires accounting for important features of bond return models such as volatility dynamics and unspanned macro factors. A three-factor model comprising a forward spread, a weighted combination of forward rates, and a macro factor generates notable gains in out-of-sample forecast accuracy compared with a model based on the expectations hypothesis. Such gains in predictive accuracy translate into higher risk-adjusted portfolio returns after accounting for estimation error and model uncertainty. Consistent with models featuring unspanned macro factors, our forecasts of future bond excess returns are strongly negatively correlated with survey forecasts of short rates. The online appendix is available at https://doi.org/10.1287/mnsc.2017.2829 This paper was accepted by Gustavo Manso, finance."
/doi/10.1287/mksc.1120.0760," In this paper we quantify the economic worth of celebrity endorsements by studying the sales of endorsed products. We do so with the use of two unique data sets consisting of monthly golf ball sales and professional golfer (celebrity) rankings. In particular, we examine the impact Tiger Woods had on sales of Nike golf balls. Our identification of the causal effect of a celebrity is grounded in the celebrity's random performance over time. Using two different approaches, reduced form and structural, we find that there are substantial celebrity endorsement effects. From our structural model, we determine that endorsements not only induce consumers to switch brands, a business stealing effect, but also have a primary demand effect. We determine that from 2000 to 2010, the Nike golf ball division reaped an additional profit of $103 million through the acquisition of 9.9 million in sales from Tiger Woods' endorsement effect. Moreover, having Tiger Woods' endorsement led to a price premium of roughly 2.5%. As a result, approximately 57% of Nike's investment in Woods' $181 million endorsement deal was recovered just in U.S. golf ball sales alone."
/doi/10.1287/mksc.1080.0380," We quantify the economic value of hardware base warranties in the U.S. server market to manufacturers, channel intermediaries, and customers. We further decompose the value of a warranty into its insurance value and its price discrimination value, which are the two main rationales for warranty provision in the server market. We use structural modeling and counterfactual experiments to accomplish the empirical task. We derive our demand model from utility maximization, which accounts for a customer's risk aversion behavior and heterogeneity. We obtain our pricing model from the profit maximization behavior of manufacturers and downstream firms in indirect channels, accounting for the institutional realities in the server market. Our empirical analysis uses quarterly data from 1999 to 2004 on server wholesale prices, retail prices, and sales for direct and indirect channels in the U.S. market. We find that manufacturers and downstream firms benefit from warranty provision and from sorting across heterogeneous customers by offering a menu of warranties. Customers also benefit from manufacturer warranty provision as well as from the menu of warranties offered. The insurance value of warranties increases and the price discrimination value of warranties decreases with warranty duration."
/doi/10.1287/isre.2017.0710," This study addresses the economic impacts of information technology (IT) overinvestment and underinvestment decisions. Based on the view of Red Queen competition in conjunction with institutional theory, we hypothesize that overinvestment and underinvestment in IT have nonlinear performance impacts. Drawing on the idea of management control mechanisms, we further hypothesize that the performance impacts are conditional on ownership concentration. Using a sample of S&P 500 firms, we find that, on average, there is a positive relationship between a firm’s overinvestment in IT and Tobin’s q , although that relationship attenuates at higher levels of overinvestment. However, there is, on average, no relationship between a firm’s underinvestment in IT and its Tobin’s q . Importantly, the payoff for underinvestment becomes positive for companies with founding-family ownership. Implications for research and practice are discussed. The online appendix is available at https://doi.org/10.1287/isre.2017.0710 ."
/doi/10.1287/orsc.1040.0089," Firms often delegate elements of strategic decisions to outside experts who promise objective assessments, which are especially valuable in unstable environments. However experts themselves may be prone to skewed decision making as the stability of their own industry environment changes and as their positioning within the industry shifts. We examine this possibility in the context of expert credit-rating agencies (“agencies”) and their risk ratings of emerging-market sovereign borrowers (“ratings”) published from 1987 to 1998, a period that includes both industry stability (1987–1996) and industry turbulence set off by financial crises in several emerging-market countries (1997–1998). After controlling for macroeconomic and related objective risk factors linked to the sovereigns themselves, we find several points: (1) agency ratings during crisis-induced industry turbulence are negatively skewed, indicating undue pessimism among these experts, in line with decision-making perspectives predicting negative reaction by experts in an effort to retain legitimacy with salient stakeholders, in this case, investors and public regulators; (2) this negative shift is greater for incumbent firms and regionally focused firms, possibly because of the loss of previous informational advantages; and (3) this negative shift during crisis-induced turbulence is greater as industry rivalry among these experts increases in particular market segments, possibly indicating the development of competitive bandwagons among experts. Together, our results suggest that objective assessments by expert organizations are vulnerable to substantial distortion from the confluent effects of industry instability and expert positioning within the industry, particularly positioning affecting rivalry among experts. Ironically, experts may be most likely to mislead clients in unstable industry environments when experts command greater attention and should show greater fidelity to disinterested objectivity."
/doi/10.1287/isre.2019.0856," This study examines the team-level effects of pair programming by developing a research model that accounts for mediators and moderators of the relationship between pair programming and team performance. We hypothesize that pair programming helps software development teams establish backup behavior by strengthening the shared mental models among developers. In turn, backup behavior attenuates the negative effect of task novelty on team performance. We collect data from the software developers, Scrum masters, and product owners of 62 software development teams in a global enterprise software firm and find broad support for our research model. The study makes important contributions by shifting attention to the team-level effects of pair programming and by explicating mediating and moderating mechanisms related to the roles of shared mental models, backup behavior, and task novelty. The results underline the importance of viewing pair programming as a context-specific practice that helps establish backup behavior in teams. In terms of implications for practitioners, our results show that pair programming can be a valuable element of team governance to create shared mental models and backup behavior and to achieve high team performance when teams face high levels of task novelty."
/doi/10.1287/mnsc.47.7.881.9802," There are many materials for which the quantity needed by a firm is at best indirectly related to the quantity of final product produced by that firm, such as solvents in manufacturing processes or office supplies. For any such “indirect” materials, an inescapable incentive conflict exists: The buyer wishes to minimize consumption of these indirect materials, while the supplier's profits depend on increasing volume. Both buyer and supplier can exert effort to reduce consumption, hence making the overall supply chain more efficient. However, no supplier will voluntarily participate unless contract terms are fundamentally revised. This can be done through a variety of “shared-savings” contracts, where both parties profit from a consumption reduction. This paper analyzes several such contracts currently in use for chemicals purchasing. We show that such contracts can always increase supply-chain profits but need not lead to reduced consumption. We analyze equilibrium effort levels, consumption, and total profits, and show how these change with the contract parameters. We find that the goals of maximizing joint profits and minimizing consumption are generally not aligned. Also, surprisingly, a decrease in a cost parameter can lead to a decrease in profits; it may be necessary (but is always possible) to renegotiate the shared-savings contract to reap the benefits of a cost decrease."
/doi/10.1287/trsc.2017.0796," Our study involves the decision-making problems that railway infrastructure managers face in a rail network with dedicated tracks and shared-use corridors. We will analyze the consolidation strategy for shared-use corridors, where the track serves passenger and freight trains. In the stochastic demand case, we will provide an analytical model for the railway infrastructure manager to compute the expected long-term profit using a consolidation system. We will pinpoint the different characteristics of passenger and freight trains, and analytically derive the optimum track allocation and consolidation time, together with the optimum price, in all such cases, using two different model structures, i.e., the additive and the multiplicative forms. We will extend our model further to consider the due-date requirements and volume incentives for railway operators. Our experiments will use realistic parameter values, based on the Dutch railway system."
/doi/10.1287/msom.2016.0585," We use a unique empirical setting to investigate the spillover of quality knowledge across supply chains and to the examine contingencies that affect such spillover. We analyze the quality performance of 191 suppliers, who utilize the same facilities to manufacture similar products for two distinct businesses: one that makes cars and the other that makes commercial vehicles. From 2006 to 2009, the car business undertook 2,121 quality improvement initiatives at these suppliers, while the commercial vehicles business did not undertake any such initiatives. We find that the quality knowledge developed through the quality improvement initiatives undertaken by the car business does not easily spill over to benefit the commercial vehicles business. Quality knowledge spills over under three conditions: (1) when quality improvement efforts are focused on organizational members, as opposed to when they focus on routines or technology; (2) when quality improvement efforts focus on the output activities of suppliers, not when they focus on the input or in-process activities; and (3) when quality knowledge is developed at suppliers with low complexity in their operations. Our results provide insights on managing quality at shared suppliers."
/doi/10.1287/isre.2018.0788," As organizations increasingly use digital platforms to facilitate innovation, researchers are seeking to understand how platforms shape business practices. Although extant literature offers important insights into platform management from a platform-owner perspective, we know little about how organizations manage industry platforms provided by external parties to generate opportunities and overcome challenges in relation to their infrastructure and work processes. As part of larger ecosystems, these digital platforms offer organizations bundles of digital options that they can selectively invest in over time. At the same time, organizations’ previous investments in digital infrastructure and work processes produce a legacy of digital debt that conditions how they manage their digital platforms over time. Against this backdrop, we investigate how digital options and digital debt were implicated in a large Scandinavian media organization’s management of a news production platform over nearly 17 years. Drawing on extant literature and the findings from this case, we theorize the progression of and interactions between digital options and digital debt during an organization’s digital platform management in relation to its infrastructure and work processes. The theory reveals the complex choices that organizations face in such efforts: While they may have to resolve digital debt to make a platform’s digital options actionable, hesitancy to plant digital debt may equally well prevent them from realizing otherwise attractive digital options. Similarly, while identified digital options may offer organizations new opportunities to resolve digital debt, eagerness to realize digital options may just as easily lead to unwise planting of digital debt."
/doi/10.1287/mnsc.2020.3659," We review past research and discuss future directions on how the vibrant research areas of market design and behavioral economics have influenced and will continue to impact the science and practice of management in both the private and public sectors. Using examples from various auction markets, reputation and feedback systems in online markets, matching markets in education, and labor markets, we demonstrate that combining market design theory, behavioral insights, and experimental methods can lead to fruitful implementation of superior market designs in practice. This paper was accepted by David Simchi-Levi, Special Section of Management Science: 65th Anniversary."
/doi/10.1287/serv.2016.0133," This paper studies a monopoly telecom operator’s decision on the adoption of shared data plans. A shared data plan allows sharing data quota among multiple devices or users, while conventional single device data plans only allow the use of a single device. We develop analytical models and compare a simple shared data plan (i.e., bundling pricing) to single device data plans (i.e., partitioned pricing). We aim to identify key factors that drive the optimal pricing policies under the two pricing plans and the value of data shared plans. We first consider a base model with independent consumer valuations (usages) of different devices. We find a threshold on the unit usage cost below which the shared data plan yields more profits than single device data plans. The optimal price for the shared data plan is less than the sum of the single device data plans. The disparity of the devices’ mean usages reduces the relative value of the shared data plan against the single device data plans. We also show that shared data plans increases the social welfare and consumer surplus when it yields a higher profit. We then examine the effects of complementarity and substitution between consumer valuations of the devices on the value of data shared plans. Two commonly used approaches to modeling the complementarity and substitution in the literature are considered: utility scaling and correlation. We examine how the utility scaling factor and correlation affect the value of shared data plans, respectively. We find that a higher degree of complementarity (or a lower degree of substitution) may not necessarily increase the value of shared data plan, which is counter to our common intuition that shared data plans are more attractive for complementary products. We provide the rationale and managerial insights. We also examine the value of bundling under a quadratic usage cost function."
/doi/10.1287/msom.2017.0653," When suppliers (i.e., contract manufacturers) fail to comply with health and safety regulations, buyers (retailers) are compelled to improve supplier compliance by conducting audits and imposing penalties. As a benchmark, we first consider the independent audit-penalty mechanism in which the buyers conduct their respective audits and impose penalties independently. We then examine the implications of two new audit-penalty mechanisms that entail a collective penalty. The first is the joint mechanism under which buyers conduct audits jointly, share the total audit cost incurred, and impose a collective penalty if the supplier fails their joint audit. The second is the shared mechanism in which each buyer conducts audits independently, shares its audit reports with the other buyers, and imposes a collective penalty if the supplier fails any one of the audits. Using a simultaneous-move game-theoretic model with two buyers and one supplier, our analysis reveals that both the joint and the shared mechanisms are beneficial in several ways. First, when the wholesale price is exogenously given, we establish the following analytical results for the joint mechanism in comparison with the independent mechanism: (a) the supplier’s compliance level is higher; (b) the supplier’s profit is lower while the buyers’ profits are higher; and (c) when the buyers’ damage cost is high, the joint audit mechanism creates supply chain value so the buyers can offer an appropriate transfer payment to make the supplier better off. Second, for the shared audit mechanism, we establish similar results but under more restrictive conditions. Finally, when the wholesale price is endogenously determined by the buyers, our numerical analysis shows that the key results continue to hold. The online appendix is available at https://doi.org/10.1287/msom.2017.0653 ."
/doi/10.1287/msom.2016.0606," We study the pricing and capacity allocation problem of a service provider who serves two distinct customer classes. Customers in each class are inherently heterogeneous in their willingness to pay for service, but their utilities are also affected by the presence of other customers in the system. Specifically, customer utilities depend on how many customers are in the system at the time of service as well as who these other customers are. We find that if the service provider can price discriminate between customer classes, pricing out a class, i.e., operating an exclusive system, can sometimes be optimal and depends only on classes’ perceptions of each other. If the provider must charge a single price, an exclusive system is even more likely. We extend our analysis to a service provider who can prevent class interaction by allocating separate capacity segments to the two customer classes. Under price discrimination, allocating capacity is optimal if the “net appreciation” between classes, as defined in the paper, is negative. However, under a single-price policy, allocating capacity can be optimal even if this net appreciation is positive. We describe in detail how the nature of asymmetry in classes’ perception of each other determines the optimal strategy. The online appendix is available at https://doi.org/10.1287/msom.2016.0606 ."
/doi/10.1287/trsc.2013.0488," This paper presents a price-endogenous, dynamic, mixed-integer nonlinear programming (MINLP) model to determine the biofuel feedstock supply response in U.S. agriculture and future biorefinery locations that meet the mandated cellulosic biofuel production targets. With a large number of supply units and potential processing locations involved, the problem could not be solved directly using MINLP solvers. We developed a sequential two-stage solution procedure to cope with this computational difficulty. The original MINLP model is decomposed into a price-endogenous agricultural sector model that solves the supply response and equilibrium in agricultural product markets, and a dynamic linear mixed-integer programming (MIP) model that solves the optimum facility location and supply chain network. The two models are solved sequentially with feedback from each other. Because of the large number of binary variables involved, computational difficulty was also encountered when solving the MIP model. We employed a heuristic backward-recursive technique to cope with this difficulty. Using moderately large test problems, we demonstrate that the heuristic solution procedures are computationally convenient and produce near-optimal solutions. We then applied this method to solve the full-scale model where nearly 3,000 U.S. counties were considered both as spatial supply units and potential refinery locations over the 2007–2022 planning horizon. Empirical results show that: (i) the U.S. biofuel mandates would lead to a significant increase in food commodity prices; (ii) regional comparative advantage in producing biofuel feedstocks would be more important than proximity to biofuel demand locations when determining the optimum refinery locations; and (iii) incorporating biofuel refinery locations in land-use decisions makes a considerable difference in the regional biomass production pattern."
/doi/10.1287/stsc.2020.0120," We study agents who distill the complex world around them using cognitive frames. We assume that agents share the same frame and analyze how the frame affects their collective performance. In one-shot and repeated interactions, the frame causes agents to be either better or worse off than if they could perceive the environment in full detail: it creates a fog of cooperation or a fog of conflict. In repeated interactions, the frame is as important as agents’ patience in determining the outcome: for a fixed discount factor, when all agents choose what they perceive as their best play, there remain significant performance differences induced by different frames. A low-performing team conducting a site visit to observe a high-performing team will be mystified, sometimes observing different actions than they expected or being given unexpected reasons for the actions they expected. Finally, we distinguish between incremental versus radical changes in frames, and we develop a model of category formation to analyze challenges faced by a leader who seeks to improve the agents’ collective performance."
/doi/10.1287/isre.1070.0142," We study the question of whether a software vendor should allow users of unlicensed (pirated) copies of a software product to apply security patches. We present a joint model of network software security and software piracy and contrast two policies that a software vendor can enforce: (i) restriction of security patches only to legitimate users or (ii) provision of access to security patches to all users whether their copies are licensed or not. We find that when the software security risk is high and the piracy enforcement level is low, or when tendency for piracy in the consumer population is high, it is optimal for the vendor to restrict unlicensed users from applying security patches. When piracy tendency in the consumer population is low, applying software security patch restrictions is optimal for the vendor only when the piracy enforcement level is high. If patching costs are sufficiently low, however, an unrestricted patch release policy maximizes vendor profits. We also show that the vendor can use security patch restrictions as a substitute to investment in software security, and this effect can significantly reduce welfare. Furthermore, in certain cases, increased piracy enforcement levels can actually hurt vendor profits. We also show that governments can increase social surplus and intellectual property protection simultaneously by increasing piracy enforcement and utilizing the strategic interaction of piracy patch restrictions and network security. Finally, we demonstrate that, although unrestricted patching can maximize welfare when the piracy enforcement level is low, contrary to what one might expect, when the piracy enforcement level is high, restricting security patches only to licensed users can be socially optimal."
/doi/10.1287/opre.1060.0288," Transit and peering arrangements among Internet backbone providers (IBPs) are essential for the global delivery of communication services on the Internet. In addition, to support delay-sensitive applications (e.g., streaming and multimedia applications) it is important for IBPs to maintain high service quality even if the network is congested. One promising approach is to establish interconnection agreements among providers to dynamically trade network capacity. To make such interconnections possible in a competitive setting, we propose a pricing scheme that considers factors such as network utilization, link capacity, and the cost structure of the interconnecting participants. Our analyses show that the common sender keeps all (SKA) mode of settlement does not provide adequate incentives for collaboration; rather, the provider that delivers the packets should be suitably compensated at an equilibrium price. Two price equilibria are identified: The first favors slower IBPs, whereas the other is congestion based and can be more beneficial for faster IBPs. When cost asymmetries exist, the lower cost IBP needs to offer a price discount to induce participation. We show that a usage-based, utilization-adjusted interconnection agreement could align the costs and revenues of the providers while allowing them to meet more stringent quality of service requirements."
/doi/10.1287/mnsc.2021.4027," The role of education and enforcement in ensuring compliance with a law or policy has been debated for more than a century now. We reopen this debate in the context of security circumvention by employees, currently a leading cause of information security and privacy breaches. Drawing on prior literature, we develop a microeconomic framework that captures employees’ circumventing behavior in the face of security controls. This allows us to obtain interesting insights that have implications for how an organization should employ anticircumvention measures. First, unless circumvention is rampant, education and enforcement often work better in combination, and not in isolation. Second, there are incentives for an organization to tolerate circumvention to an extent, even when education and enforcement are cheap. Finally, education and enforcement may be strategic complements or substitutes in different parts of the parameter space. When they are complements, if a change in cost parameters compels the organization to increase one, it would also require an increase in the other in lockstep. In contrast, when they are substitutes, an increase in one is associated with a decrease in the other. This paper was accepted by Chris Forman, information systems."
/doi/10.1287/mnsc.2014.2030," Examining changes in two different retail formats, we show that consumers alter their purchases depending on the retail environment. In both settings, the change in behavior coincides with a reduction in the interpersonal interaction required to complete a transaction. As such, we contend that the format changes reduced a “social friction” that would otherwise inhibit consumers due to an implicit cost associated with ordering certain items in social settings. This paper was accepted by Pradeep Chintagunta, marketing ."
/doi/10.1287/opre.46.3.368," In this paper we present the theoretical foundations for one of the methods used to achieve convergence in the National Energy Modeling System (NEMS). NEMS is a large model with several component models that are built and operated by different branches in the organization and is an example of a system without a hierarchical structure that cannot be solved by traditional equation solving methods. Some of the component models use linear programs to construct supply and demand curves. The discontinuities that result lead to oscillations in the standard relaxation algorithms. We explain where the convergence problems lie and how the convergence theory with step functions links to the convergence theory with continuous functions. To achieve convergence within the entire system, a set of ad hoc techniques were developed to implement a decomposition strategy that allows the individual models to be run separately. We present the theoretical justification for one of them here. The technique presented here has the potential to allow an organization to use operational models for planning without resorting to aggregation. It also facilitates decentralized computing over Internet."
/doi/10.1287/mksc.2020.1251," We empirically investigate the impact of category captaincy , an arrangement where the retailer works exclusively with a manufacturer to manage both the manufacturer’s and his rivals’ products. Using a unique data set that contains information on category captaincy as well as SKU-store-level sales and price across 24 retail chains and eight local markets in the United States for a frozen food category, we quantify the impact of captaincy on prices, assortments, profits, and consumer welfare. Interestingly, our estimates suggest that captaincy can lead to welfare gains for consumers, which argues against a purely negative view of captaincy by policy makers."
/doi/10.1287/mnsc.2020.3676," This study contributes to our understanding of what accruals capture and how they relate to the distribution of future returns. It examines the past and future growth components of accruals and shows that, whereas past growth is negatively associated with idiosyncratic skewness, future growth is positively associated with it. In addition, although both past and future growth are negatively associated with future returns, the association is more pronounced for past growth when volatility is lower, but for future growth when volatility is higher. The study also shows that the association between the future growth component and future returns reverses in the long run, whereas the association between past growth and future returns does not. This paper was accepted by Shivaram Rajgopal, accounting."
/doi/10.1287/mnsc.2019.3301," This study seeks to determine the role of audit firms’ quality control (QC) system deficiencies, as measured by the Public Company Accounting Oversight Board (PCAOB) inspection program, on audit quality and profitability. Using a unique data set of firmwide QC deficiencies identified by the PCAOB during its inspections of audit firms, I find a negative association between QC deficiencies, mainly performance related, and audit quality. Furthermore, audits conducted by larger audit firms with more organization-level deficiencies appear less profitable, evidenced by more hours worked on the engagements, leading to lower fees per hour. These results appear to be partly explained by deficiencies in the tone at the top (a proxy for culture) and the audit methodology. Further evidence suggests that a lack of remediation of QC system deficiencies has a negative influence on audit quality. This paper was accepted by Suraj Srinivasan, accounting."
/doi/10.1287/isre.2016.0641," Internet service providers (ISPs) are experimenting with a business model that allows content providers (CPs) to subsidize Internet access for end consumers. In this study, we develop a game-theoretical model to analyze the effects of this sponsorship of consumer data usage. We find that the ISP’s optimal network management choice of data sponsorship crucially depends on market conditions, such as the revenue rates of CPs and the fit cost of consumers. If the fit cost is low, the ISP will either allow both CPs to subsidize consumers’ Internet access, or will allow only the more competitive CP to subsidize, depending on the per-consumer revenue generation rates of CPs. If the fit cost is high, it is in the ISPs interest not to allow any subsidization. We also identify conditions under which the ISP’s network management choices of data sponsorship deviate from social optimum. These results should be of interest to the telecom industry as it searches additional revenue models, and to online CPs competing for customer loyalty. It should also be of interest to policymakers investigating into this issue."
/doi/10.1287/mksc.1090.0505," According to Jacobson and Mizik [Jacobson, R., N. Mizik. 2009. The financial markets and customer satisfaction: Reexamining possible financial market mispricing of customer satisfaction. Marketing Sci. 28 (5) 810–819], excess stock portfolio returns for firms with strong customer satisfaction are small and statistically insignificant, and if there is any above-market performance at all, it is due to a small set of firms in the computer and Internet industries. But their data seem to suggest the opposite. The returns are actually both exceptionally large and significant. Using monthly data, their portfolio consisting of strong American Customer Satisfaction Index (ACSI) firms outperformed the market by 0.0053, corresponding to 6.4% cumulative risk-adjusted above-market returns on an annual basis over a 10-year period—a performance that would beat at least 99% of all large-cap U.S. stock funds tracked by Morningstar. Using a different treatment of risk, their annualized risk-adjusted return is a whopping 8.4% better than market. After eliminating computer, Internet, and utility companies, they find that the monthly risk-adjusted abnormal returns drop to 0.0045, which corresponds to an annual above-market return of 5.4%. This too is better than 99% of all actively managed stock funds in the population. Yet Jacobson and Mizik conclude that these returns are not statistically significant and that there is no evidence that stock returns from firms with strong customer satisfaction outperform the market over the long run. The failure to reject the null hypothesis is probably due to a lack of statistical power in Jacobson and Mizik's analysis. We discuss why this is likely the case and then present new data updating the results from our original article [Fornell, C., S. Mithas, F. Morgeson III, M. S. Krishnan. 2006. Customer satisfaction and stock prices: High returns, low risk. J. Marketing 70 (1) 3–14]. The above-market returns persist and are both economically and statistically significant."
/doi/10.1287/mnsc.2020.3591," Organizations often require agents’ private information to achieve critical goals such as efficiency or revenue maximization, but frequently it is not in the agents’ best interest to reveal this information. Strategy-proof mechanisms give agents incentives to truthfully report their private information. In the context of matching markets, they eliminate agents’ incentives to misrepresent their preferences. We present direct field evidence of preference misrepresentation under the strategy-proof deferred acceptance in a high-stakes matching environment. We show that applicants to graduate programs in psychology in Israel often report that they prefer to avoid receiving funding, even though the mechanism preserves privacy and funding comes with no strings attached and constitutes a positive signal of ability. Surveys indicate that other kinds of preference misrepresentation are also prevalent. Preference misrepresentation in the field is associated with weaker applicants. Our findings have important implications for practitioners designing matching procedures and for researchers who study them. This paper was accepted by Axel Ockenfels, decision analysis."
/doi/10.1287/mnsc.2020.3826," This paper studies whether euphemisms obfuscate the content of earnings conference calls and cause investors to underreact. I argue that managers’ use of euphemisms can alleviate the impact of bad news and delay the market reaction to adverse information. Using a dictionary of corporate euphemisms, I find that their use by managers—but not by analysts—is negatively associated with both immediate and future abnormal returns, and their frequency moderates the negative market reaction to bad earnings news. Finally, stock underreaction is more pronounced on busy earnings announcement dates, when investor attention is distracted. This paper was accepted by Brian Bushee, accounting."
/doi/10.1287/opre.1070.0446," We describe a model of the market for petroleum tank vessels used for planning by Maritrans, Inc. This model is an enhanced version of an earlier model and more closely approximates the market for transportation services. Because of the better representation, we found that the market, which is defined around an index for transportation services, has the potential for multiple equilibria. We present how the model has been used in making major decisions at Maritrans and show how the index design leads to an anomaly where demand could increase with increasing prices, leading to the potential for multiple equilibria. We have not observed this phenomenon in the market. However, with the advent of forward markets for transportation services, known as freight-forward markets, if multiple equilibria do appear, it could become profitable for a player to move a market from one equilibrium to another."
/doi/10.1287/serv.2013.0047," This paper develops models for the analysis of a cloud brokering platform under conditions of risk and demand uncertainty, focusing on controlling the risk of not delivering the quality of service required by users. Such risk can occur as a result of inherent limitations of the best-effort connectivity. We take the approach of modern portfolio theory and show how the trade-off between risk and profit can be chosen by selecting efficient connectivity portfolios that combine the best-effort connectivity of different grades with premium-grade connectivity. We provide theoretical analysis of connectivity portfolio models and related insights delivered by numerical experiments that utilize the measurements of Internet traffic."
/doi/10.1287/isre.1110.0347," Cooperative caching is a popular mechanism to allow an array of distributed caches to cooperate and serve each others' Web requests. Controlling duplication of documents across cooperating caches is a challenging problem faced by cache managers. In this paper, we study the economics of document duplication in strategic and nonstrategic settings. We have three primary findings. First, we find that the optimum level of duplication at a cache is nondecreasing in intercache latency, cache size, and extent of request locality. Second, in situations in which cache peering spans organizations, we find that the interaction between caches is a game of strategic substitutes wherein a cache employs lesser resources towards eliminating duplicate documents when the other caches employs more resources towards eliminating duplicate documents at that cache. Thus, a significant challenge will be to simultaneously induce multiple caches to contribute more resources towards reducing duplicate documents in the system. Finally, centralized decision making, which as expected provides improvements in average latency over a decentralized setup, can entail highly asymmetric duplication levels at the caches. This in turn can benefit one set of users at the expense of the other, and thus will be challenging to implement."
/doi/10.1287/isre.2019.0893," This paper investigates the strategy for product recommendation. Specifically, we analyze a platform-based market where consumers search and purchase products that potentially differ in quality. In addition, consumers have idiosyncratic tastes for a product, and the extent of this heterogeneity may vary from one product to another. In other words, there may be products with low taste dispersion (products for which there is less heterogeneity among consumers), as well as products with high taste dispersion. Our modeling framework elucidates how platform recommendation influences the market-level equilibrium outcomes, thereby informing the optimal recommendation strategy. We find that the quality and taste-dispersion dimensions can interact to affect the overall effectiveness of product-recommendation strategies. Conditioning on taste dispersion, recommending high-quality products increases both producer profits and consumer surplus. Conditioning on quality, recommending high-taste-dispersion products may, however, increase or decrease producer profits, depending on the joint effect of profit margin and purchase probability. The direction of change in consumer surplus is also uncertain—recommending a high-taste-dispersion product is more likely to increase (decrease) consumer surplus if the quality is low (high). Importantly, we show that when the platform cannot discern product types, recommendation strategies based on observed price or sales signals cannot guarantee the optimal outcome in the general case."
/doi/10.1287/mnsc.1080.0949," Ranking and selection procedures are standard methods for selecting the best of a finite number of simulated design alternatives based on a desired level of statistical evidence for correct selection. But the link between statistical significance and financial significance is indirect, and there has been little or no research into it. This paper presents a new approach to the simulation selection problem, one that maximizes the expected net present value of decisions made when using stochastic simulation. We provide a framework for answering these managerial questions: When does a proposed system design, whose performance is unknown, merit the time and money needed to develop a simulation to infer its performance? For how long should the simulation analysis continue before a design is approved or rejected? We frame the simulation selection problem as a “stoppable” version of a Bayesian bandit problem that treats the ability to simulate as a real option prior to project implementation. For a single proposed system, we solve a free boundary problem for a heat equation that approximates the solution to a dynamic program that finds optimal simulation project stopping times and that answers the managerial questions. For multiple proposed systems, we extend previous Bayesian selection procedures to account for discounting and simulation-tool development costs."
/doi/10.1287/opre.49.5.690.10610," This paper develops an operational risk management model for evaluating production efforts in manufacturing and mining industries where the resource to be exploited is nonhomogenous. Using a contingent claims methodology now commonly encountered in financial applications, we formulate a production control model in an environment characterized by market and process uncertainty. In our analysis, market risk is depicted by the output price while process uncertainty is captured by the random variability inherent in the output's yield. In this light, adjustments to the rate of production are viewed as a sequence of (nested) real options affording operating flexibility. We account for an optimal sequence of production adjustments, over a preestablished production horizon, by taking the production rate as an adapted positive real-valued process. Accordingly, techniques of stochastic control theory and contingent claims analysis (CCA) are employed to ensure value maximizing production policies are rendered in a manner consistent with an equilibrium price structure."
/doi/10.1287/mnsc.1090.1051," This paper utilizes an understudied but often utilized aspect of initial public offerings (IPOs), secondary shares, to examine whether the knowledge conditions of firms give rise to agency problems that limit the ability of founders and venture capitalists to sell equity at IPO. In an analysis of 2,190 IPOs spanning from 1992 through 2002, we find that private owners are less likely to be observed selling their equity at IPO in ventures that are highly dependent on technical specialized knowledge as measured by the count of individuals with Ph.D.s in the top management team and board of directors. However, we find that this limit on the financial liquidity of founders and venture capitalists is alleviated when the venture's output has received greater market acceptance. Hence, the findings suggest that the financial liquidity of individuals involved in entrepreneurship is likely to be influenced by the knowledge conditions of their venture."
/doi/10.1287/mksc.1050.0118," Many emerging technologies exhibit path-dependent demands driven by positive network feedback. Such network effects profoundly impact marketing strategists' thinking in today's network economy. However, the significant network externalities expected by many people often fail to materialize in the emerging technology market. We analyze this phenomenon in the context of a technology distribution channel. By studying cheap-talk strategies under information asymmetry, we show that incentive-compatible contracts are essential for achieving credible information transmission. In our model, the better-informed technology vendor has an incentive to inflate the retailer's ex ante belief of network externalities when a wholesale price contract is adopted. When properly termed revenue-sharing contracts are implemented, there are information-efficient cheap-talk equilibria where truthful information transmission is mutually beneficial. When the vendor's information is imperfect, even revenue-sharing contracts cannot guarantee credible information transmission if there is significant prior belief disparity between the vendor and the retailer. This study demonstrates how information-inefficient equilibria (e.g., information blockage) arise because of the conflict of interest or the conflict of opinion among channel members. It also explores the role of cheap talk in facilitating channel coordination."
/doi/10.1287/inte.33.6.18.25187," Costs related to tires are about three percent of the operating costs of a transport fleet. Fleet operators have been trying to reduce these costs by extending the tires’ useful life and by increasing their use of retreaded tires. A program to reduce tire wear can pay off only if the fleet operator and the retreader cooperate. However, the typical contract between the two leads to conflicting incentives. We devised a service contract with shared savings and cautiously chosen parameters for McGriff Treading Company, Inc., that better aligns the incentives for reducing tire costs. Apart from the optimal choice of contract parameters, managerial performance metrics and information technology systems to monitor and track costs were the key factors in the company's transition from a product to a service company. McGriff Treading now successfully uses such contracts for its intermodal and trucking clients."
/doi/10.1287/isre.2019.0837," How do multinational corporations (MNCs) and domestic firms compensate technical and managerial skills of knowledge workers within and across geographies? This paper answers this question by examining how developed economy MNCs and emerging economy firms value master of business administration (MBA) education and firm-specific information technology (IT) experience of IT professionals in India and how developed economy MNCs value MBA education and firm-specific IT experience differently across India and the United States. Our analyses of archival data on more than 20,000 IT professionals reveal two important findings. First, for IT professionals in India, the marginal effect of firm-specific IT experience on compensation is greater for developed economy MNCs than for emerging economy firms, and the marginal effect of MBA education is greater for emerging economy firms than for developed economy MNCs. Second, when we compare compensation of IT professionals employed by MNCs across India and the United States, we find a greater marginal effect of firm-specific IT experience in India but a greater marginal effect of MBA education in the United States. These findings suggest that the manner in which MNCs and emerging economy firms value and compensate IT professionals across geographies is consistent with their firm-level strategies and capabilities. The findings provide important insights on compensation of IT professionals within and across geographies based on firm national origin. These contributions are important to understand the broader context of compensation of IT professionals across firm national origin and geographies."
/doi/10.1287/mksc.20.3.284.9768," Marketing scholars and practitioners frequently infer market responses from cross-sectional or pooled cross-section by time data. Such cases occur especially when historical data are either absent or are not representative of the current market situation. We argue that inferring market responses using cross-sections of multimarket data may in some cases be misleading because these data also reflect unobserved actions by retailers. For example, because the (opportunity) costs of doing so do not outweigh the gains, retailers are predisposed against promoting small share brands. As a consequence, local prices and promotion variables depend on local market shares—the higher the local share, the higher the local observed promotion intensity. We refer to this reverse causation as an endogeneity. Ignoring it will inflate response estimates, because both the promotion effects on share as well as the reverse effects are in the same direction. In this paper, we propose a solution to this inference problem using the fact that retailers have trade territories consisting of multiple contiguous markets. This implies that the unobserved actions of retailers cause a measurable spatial dependence among the marketing variables. The intuition behind our approach is that by accounting for this spatial dependence, we account for the effects of the retailer's behavior. In this context, our study hopes to make the following contributions at the core of which lies the above intuition. First, we separate the market response effect from the reverse retailer effect by computing responses to price and promotion net of any spatial—and therefore retailer—influence. Second, underlying this approach is a new variance-decomposition model for data with a panel structure. This model allows to test for endogeneity of prices and promotion variables in the cross-sectional dimension of the data. This test aims to complement the one developed by Villas-Boas and Winer (1999), who test for endogeneity along the temporal dimension. Third, to illustrate the approach, we use Information Resources Inc. (IRI) market share data for brands in two mature and relatively undifferentiated product categories across 64 IRI markets. Whereas we only use data with very short time horizons to estimate price and promotion responses with the spatial model, we do have data over long time windows. We use the latter to validate the approach. Specifically, within-market estimates of price and promotion response are not subject to the same endogeneity because we hold the set of retailers constant. Therefore, comparing within- and across-market estimates of price and promotion responses is a natural way to validate the approach. Consistent with our argument, ignoring the reverse causation in the cross-sectional data leads to inferences of price and promotion elasticities that are farther away from zero than the elasticities obtained from within-market analysis. In contrast, cross-sectional spatial estimates and time-series estimates show convergent validity. From a practical point of view, this means it is possible to obtain reasonable within-market estimates of price and promotion elasticities from (predominantly) cross-sectional data. This may benefit marketing managers. The manager who would act on the inflated elasticities will over-allocate marketing resources to promotions because she ignores retailers' censorship of promotions on the basis of already existing high share. We explore other approaches to correct for the inference bias, and discuss further managerial issues and future research."
/doi/10.1287/isre.1100.0283," In this paper, we analyze a model of usage pricing for digital products with discontinuous supply functions. This model characterizes a number of information technology-based products and services for which variable increases in demand are fulfilled by the addition of blocks of computing or network infrastructure. Such goods are often modeled as information goods with zero variable costs; in fact, the actual cost structure resembles a mixture of zero marginal costs and positive periodic fixed costs. This paper discusses the properties of a general solution for the optimal nonlinear pricing of such digital goods. We show that the discontinuous cost structure can be accrued as a virtual constant variable cost. This paper applies the general solution to solve two related extensions by first investigating the optimal technology capacity planning when the cost function is both discontinuous and declining over time, and then characterizing the optimal costing for the discontinuous supply when it is shared by several business profit centers. Our findings suggest that the widely adopted full cost recovery policies are typically suboptimal."
/doi/10.1287/ijoc.10.3.351," A UNIX macro process scheduler, referred to as the Fair Share Scheduler (FSS), is a UNIX scheduling algorithm that allocates different shares of the processor capacity to different groups of processes. First, the scheduling algorithm is approximated by a generalized processor-sharing discipline. Then, a second approximation is made to derive the mean response times of the different groups and the optimal allocation of shares among the groups. Finally, an application is shown where substantial performance improvements are observed."
/doi/10.1287/mnsc.1040.0241," This paper explores the relationship between wages and the scientific orientation of R&D organizations. Firms that adopt a science-oriented research approach (i.e., “science”) allow their researchers to pursue and publish an individual research agenda. The adoption of science may be associated with a “taste” for science on the part of researchers (a preference effect) and/or as a “ticket of admission” to gain earlier access to scientific discoveries with commercial application (a productivity effect). These two effects differ in their impact on wages. Whereas the preference effect contributes to a negative compensating differential, the productivity effect may result in rent sharing. However, because science may be adopted by firms employing higher-quality researchers, cross-sectional evaluations of wages and science may be biased by unobserved heterogeneity. To overcome this bias, this paper introduces a novel empirical approach. Specifically, prior to accepting a given job, many scientists receive multiple job offers , allowing for the calculation of the wage-science relationship and controlling for differences in salary levels offered to individual researchers. Using a dataset composed of multiple job offers to postdoctoral biologists, the results suggest a negative relationship between wages and science. These findings are robust to restricting the sample to nonacademic job offers, but the findings depend critically on the inclusion of researcher fixed effects. Conditional on perceived ability, scientists do indeed pay to be scientists."
/doi/10.1287/isre.2014.0552," Seizing the latest technological advances in distributed work, an increasing number of firms have set up offshore captive centers (CCs) in emerging economies to carry out sophisticated R&D work. We analyze survey data from 132 R&D CCs established by foreign multinational companies in India to understand how firms execute distributed innovative work. Specifically, we examine the performance outcomes of projects using different technology-enabled coordination strategies to manage their interdependencies across multiple locations. We find that modularization of work across locations is largely ineffective when the underlying tasks are less routinized, less analyzable, and less familiar to the CC. Coordination based on information sharing across locations is effective when the CC performs tasks that are less familiar to it. A key contribution of our work is the explication of the task contingencies under which coordination based on modularization versus information sharing yield differential performance outcomes."
/doi/10.1287/isre.2021.1052," An increasing number of companies have started to implement corporate knowledge-sharing communities. Consistent with the observation in the offline setting, employees are less likely to share knowledge with individuals who have higher job ranks (i.e., higher-ups) in corporate communities such as online wikis and discussion groups. Given the importance of managers’ engagements in the community and the needs for knowledge sharing across the hierarchy, we examine whether such observation persists in the corporate question-and-answer (Q&A) community, another popular type of corporate knowledge-sharing community. On the one hand, as in the offline setting and other types of communities, employees can still be reluctant to share knowledge with the higher-ranked individuals in the Q&A community. On the other hand, a Q&A community has some unique attributes that can potentially motivate employees to engage more with the higher-ups. Using a unique data set from a large corporate Q&A community and a potential-dyads approach, we find that a user is inclined to respond to a knowledge seeker whose job rank is higher than (versus lower than or the same as) the user’s rank in the corporate Q&A community. We further show the causality of the result with a quasi experiment that leverages the promotions announced in our study period. Because these promotions are based on employees’ performances before the existence of the community, the promotion announcements are largely exogenous to our research interest. We also find that knowledge providers exert greater effort when answering questions from the higher-ups. Finally, our analyses show that knowledge providers who post more answers to higher-ranked seekers and who display greater effort in those answers are more likely to get promoted in subsequent years. Given the critical role of knowledge sharing and the increasing prevalence of online communities, our study offers a better understanding of the knowledge-sharing pattern in the corporate Q&A community of the hierarchical organizations and delivers useful managerial implications."
/doi/10.1287/mnsc.2020.3791," Do the rich always get richer by investing in a cryptocurrency for which new coins are issued according to a proof-of-stake (PoS) protocol? We answer this question in the negative: Without trading, the investor shares in the cryptocurrency are martingales that converge to a well-defined limiting distribution and, hence, are stable in the long run. This result is robust to allowing trading when investors are risk neutral. Then, investors have no incentive to accumulate coins and gamble on the PoS protocol but weakly prefer not to trade. This paper was accepted by Kay Giesecke, finance."
/doi/10.1287/orsc.1110.0671," In research and development (R&D) alliances, the partner firms must balance the tension between knowledge sharing and knowledge leakages because knowledge sharing, designed to support the alliance's technology development goals, can often lead to unintended and potentially damaging knowledge leakages. Governance structure is a well-understood knowledge protection strategy designed to reduce knowledge leakage concerns and thereby encourage desired knowledge transfers in two-party R&D alliances. Whether governance structure can be an important balancing mechanism for R&D alliances with multiple partner firms, or multilateral R&D alliances, however, requires further study. Because increasing the number of alliance partners introduces additional complexities to managing an alliance, the appropriate governance mechanism for a multilateral R&D alliance is likely to differ from that for a bilateral alliance. Drawing insights from social exchange theory, we explore governance decisions in multilateral R&D alliances. First, we examine the potential for variance between multilateral and bilateral R&D alliances in governance decisions as a means of knowledge sharing and knowledge protection. Results based on our analysis of 2,423 R&D alliances, 1,690 bilateral and 733 multilateral, are consistent with predictions drawn from social exchange theory. We next focus on three-partner R&D alliances, or trilateral R&D alliances , and compare governance mechanisms between two types of trilateral R&D alliances: chain and net. We find that equity governance structures are more likely to be used in net-based than in chain-based trilateral R&D alliances; we also find that alliance scope moderates the relationship between the type of alliance and governance structure. Finally, we find that multilateral R&D alliances with predicted (aligned) governance structures perform better, in terms of alliance longevity, than those with misaligned structures."
