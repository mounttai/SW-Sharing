link,abstract
/doi/10.1287/mnsc.1030.0184," Traditionally, integration has been studied at the country level. With increasing economic integration, industrial reorganization, and blurring of national boundaries (e.g., European Union (EU)), it is important to investigate global integration at the industry level. We argue that country-level integration (segmentation) does not preclude industry-level segmentation (integration). Indeed, our results suggest that a country is integrated with (segmented from) the world capital markets only if most of her industries are integrated (segmented). We also show that although global industry risk is small, it can be priced for certain industries. Industries that are priced differently from either the world or domestic markets represent incremental opportunities for international diversification."
/doi/10.1287/orsc.1050.0155," The National Science Foundation (NSF) is the primary federal agency funding nonmedical research in the United States. However, relatively few organizational researchers consider approaching the agency, despite the funds it has available. It is easy to understand why: The agency has a reputation for preferring “hard science” and quantitative approaches, and there is no obvious home for organizational studies in the agency. NSF programs appear to recognize psychological and small-group research on the one hand, and sociological research on the other, with little emphasis on organizations. There are, however, several programs that have funded research on organizations, or relevant to organizations. Additionally, the NSF regularly solicits advice and direction from researchers on new directions for focus and attention. We argue that organization researchers should indeed approach NSF with robust research proposals. We provide suggestions for finding an appropriate application “home” within NSF. We also discuss the process of proposal preparation, offering suggestions on how to prepare a persuasive proposal, with comments on how the review process works. Finally, we close with a clarion call for organization researchers to help NSF understand the importance of organizations, and thus of organization research, to the whole array of the agency’s other research interests—to the effective practice of science; to organizing human activity, including economic and educational activity; and to management and governance of human activities in general. As such, we argue, NSF has a stake in organizational research—and organizational researchers have a stake in NSF."
/doi/10.1287/mnsc.2019.3456," We examine the shareholder wealth effects of the adoption of the UK Modern Slavery Act 2015 (MSA). The MSA’s Transparency in Supply Chains clause introduced new reporting requirements mandating certain firms to provide an annual statement outlining how they identify and mitigate modern slavery in their business and supply chain. An event study of stock price reactions of UK firms covered by the MSA to eight events associated with its adoption provides no evidence of abnormal stock returns. We do, however, uncover significant cross-sectional differences in stock price reactions, with results suggesting that the MSA provides a competitive advantage to firms with a demonstrated track record of addressing slavery risk. We find no effects for preregulatory corporate social responsibility disclosure levels on stock price reactions. Our findings highlight the economic value of maintaining socially responsible sourcing practices and inform the current policy debate on the importance of greater transparency in corporate supply chains. This paper was accepted by Gad Allon, operations management."
/doi/10.1287/mnsc.1070.0788," The format of pricing contracts varies substantially across business contexts, a major variable being whether a contract imposes a fixed fee payment. This paper examines how the use of the fixed fee in pricing contracts affects market outcomes of a manufacturer-retailer channel. Standard economic theories predict that channel efficiency increases with the introduction of the fixed fee and is invariant to its framing. We conduct a laboratory experiment to test these predictions. Surprisingly, the introduction of the fixed fee fails to increase channel efficiency. Moreover, the framing of the fixed fee does make a difference: an opaque frame as quantity discounts achieves higher channel efficiency than a salient frame as a two-part tariff, although these two contractual formats are theoretically equivalent. To account for these anomalies, we generalize the standard economic model by allowing the retailer's utilities to be reference dependent so that the up-front fixed fee payment is perceived as a loss and the subsequent retail profits as a gain. We embed this reference-dependent utility function in a quantal response equilibrium framework where the retailer is allowed to make decision mistakes due to computational complexity. The key prediction of this behavioral model is that channel efficiency decreases with loss aversion for sufficiently Nash-rational retailers. Consistent with this prediction, the estimated loss-aversion coefficient is 1.37 in the two-part tariff condition, significantly higher than 1.27 in the quantity discount condition. At the same time, loss aversion dominates contract complexity in explaining the data. Lastly, we conduct a follow-up experiment to confirm the central role of loss aversion as a behavioral driver. In one condition, the retailer becomes less loss averse when we temporally compress the fixed fee payment and the realization of retail profits, which supports the loss aversion theory. In the other condition, the retailer's contract acceptance rate does not decline when we reward the manufacturer a higher cash payment for each experimental point earned, which rules out the competing hypothesis that the retailer rejects contract offers due to fairness concerns."
/doi/10.1287/mnsc.1070.0829," To motivate buyers to increase their order quantity, suppliers often rely on a well-established and widely used approach—they offer quantity discounts. This practice is in large part driven to obtain improved economies in transportation through higher truckload utilization. Recently, transportation rates, which are increasing faster than other costs, have become a larger portion of total net landed cost, placing the traditional quantity-discount practices under scrutiny. Many suppliers are left perplexed as to why their approach is not effective anymore, and some are even concerned that their overall profits may have actually decreased due to their discount parameters. In this paper, we study a multiperiod model, with a buyer facing stochastic end-item demand and a supplier offering an all-units quantity discount to him, to understand better the dynamics of such systems. We provide guidelines and insights on how to set effective discount parameters, and when not to expect much from them. We derive the optimal policy of the buyer, develop insights as to why the policy is complex, study the supplier's profit as a function of her offered quantity-discount scheme (accommodating the buyer's optimal policy), and discover a new phenomenon that is distinct and structurally different from the well-known bullwhip effect."
/doi/10.1287/orsc.2017.1155," What role do social connections play in the labor market? A vast, influential literature has detailed the ways in which ties facilitate the flow of information about job opportunities to workers as well as endorsements or referrals of workers to firms. We propose that this role is not constrained to an informational one alone. Rather, relationships between workers can enable a collective job-matching process that facilitates the transfer of shared human capital from one organization to another. Yet “comobility” has been studied only occasionally and among elite workers in particular industries. This study delivers both fieldwork as well as large-sample analysis among all nongovernmental workers in Denmark, finding a 5.5% wage premium for those who move jointly instead of independently, both in the full sample as well as when applying strict matching or fixed effects for workers or firms. Co-movers whose skills are related but not identical capture a higher premium, and the wage gains associated with comobility are not limited to highly skilled workers but rather obtain across a wide range of occupations. Robustness tests establish that the wage premium associated with comobility is not driven by efforts to capture “prized” workers, by employee referrals, or by aggressive hiring by firms. This study provides the first quantification of the anticipated value of comobility, suggesting that both workers and firms may want to revisit individualistic assumptions about how labor markets function. More broadly, our findings show that social relationships can transfer not only information but jointly held human capital across organizational boundaries."
/doi/10.1287/mnsc.2017.2966," Both theory and empirical evidence suggest that managers’ career concerns can serve as an important source of implicit economic incentives. We examine how incentives for political promotion are related to compensation policy and firm performance in Chinese state-owned enterprises. We find that the likelihood that the CEO receives a political promotion is positively related to firm performance. We also find that CEOs with a higher likelihood of political promotion have lower pay levels and lower pay–performance sensitivity. Overall, the evidence suggests that competition in the political job market helps mitigate weak monetary incentives for CEOs in China. Data are available at https://doi.org/10.1287/mnsc.2017.2966 . This paper was accepted by Lauren Cohen, finance."
/doi/10.1287/mantech.3.1.33," Management, itself, has come to accept, to an increasing extent, a new definition of its functions. Perhaps, in this evolution is to be found the hope of the future, based upon managerial acceptance of new concepts, new interpretations, and rejection of some old concepts—economic, political, sociological, and ethical. (Epigraph by George Fillipetti, Industrial Management in Transition , p. 328.) Management Technology , ISSN 0542-4917, was published as a separate journal from 1960 to 1964. In 1965 it was merged into Management Science ."
/doi/10.1287/mnsc.47.8.1063.10227," The aim of this research is to study the effects of real exchange rates on the long-term ownership strategies of production facilities of firms entering foreign markets. Among the strategies considered are exporting (EXP), joint ventures with local partners (JV), and wholly owned production facilities (WOS) in the foreign country. Our research takes a first step in modeling the influence of exchange rates on the choice and dynamic adjustment of such strategies. The insights obtained from our modeling analysis are then translated into testable hypotheses and empirically verified with the use of firm level data from U.S. multinational corporations (both at the firm and a more aggregate level). An insightful result of our model is the identification of a hysteresis phenomenon that characterizes switching behavior between strategies in the presence of switchover cost. The magnitude of the hysteresis band, which is a measure of the inertia associated with keeping the current ownership structure, is affected by a multiplicity of factors such as exchange rate volatility and market power of the entering firm. Analytical and numerical results on the effects of such factors on the hysteresis band are provided. The four testable hypotheses generated from our modeling analysis are rigorously tested with the use of a multinomial logit model on data obtained from the Harvard Multinational Enterprise database, and a data set maintained by the Bureau of Economic Analysis, the U.S. Department of Commerce. The empirical results strongly support our insights that relatively depreciated real exchange rates (i.e., weak home currency) favor (a) the JV over the WOS and (b) EXP mode over the WOS or JV. Finally, we summarize our results into useful guidelines for global production managers."
/doi/10.1287/orsc.1040.0081," This research elaborated and empirically tested the individual action component of the collective action model as applied to individual contributions to organizational information commons. The model extended prior theory and research by making six elaborations on the classic collective action model based on unique characteristics of information goods compared to material collective goods. The structural equation model was tested via LISREL analyses of data provided by 781 respondents in three high-tech firms who had access to corporate intranets as shared information goods. The results were highly similar across organizations and indicated that (a) level of production, information retrieval, and cost predicted the perceived value of information, (b) information value and cost predicted gain, and (c) information retrieval and gain predicted the level of individual contributions to the commons."
/doi/10.1287/ijoc.1090.0353," Electronic auction markets are economic information systems that facilitate transactions between buyers and sellers. Whereas auction design has traditionally been an analytic process that relies on theory-driven assumptions such as bidders' rationality, bidders often exhibit unknown and variable behaviors. In this paper we present a data-driven adaptive auction mechanism that capitalizes on key properties of electronic auction markets, such as the large transaction volume, access to information, and the ability to dynamically alter the mechanism's design to acquire information about the benefits from different designs and adapt the auction mechanism online in response to actual bidders' behaviors. Our auction mechanism does not require an explicit representation of bidder behavior to infer about design profitability—a key limitation of prior approaches when they address complex auction settings. Our adaptive mechanism can also incorporate prior general knowledge of bidder behavior to enhance the search for effective designs. The data-driven adaptation and the capacity to use prior knowledge render our mechanisms particularly useful when there is uncertainty regarding bidders' behaviors or when bidders' behaviors change over time. Extensive empirical evaluations demonstrate that the adaptive mechanism outperforms any single fixed mechanism design under a variety of settings, including when bidders' strategies evolve in response to the seller's adaptation; our mechanism's performance is also more robust than that of alternatives when prior general information about bidders' behaviors differs from the encountered behaviors."
/doi/10.1287/mksc.1030.0022," Utility maximizing solutions to economic models of choice for goods with either discrete quantities or non-linear prices cannot always be obtained using standard first-order conditions such as Kuhn-Tucker and Roy's identity. When quantities are discrete, there is no guarantee that derivatives of the utility function are equal to derivatives of the budget constraint. Moreover, when prices are nonlinear, as in the case of quantity discounts, first-order conditions can be associated with the minimum rather than the maximum value of utility. In these cases, the utility function must be directly evaluated to determine its maximum. This evaluation can be computationally challenging when there exist many offerings and when stochastic elements are introduced into the utility function. In this paper, we provide an economic model of demand for substitute brands that is flexible, parsimonious, and easy to implement. The methodology is demonstrated with a scanner panel data set of light-beer purchases. The model is used to explore the effects of price promotions on primary and secondary demand, and the utility of product assortment."
/doi/10.1287/mnsc.48.9.1123.178," Online information servers that provide access to diverse databases where users can search for, browse through, and download the information they need have been rapidly increasing in number in the past few years. Online vendors have traditionally charged users for information on the based on the length of the time they were connected to the databases. With hardware and software advances, many online servers have recently started changing their pricing strategies to search-based and/or subscription-fee pricing. This paper examines the various issues involved in pricing these information products, and presents an economic approach to analyze conditions under which the various pricing schemes may prove optimal for the online servers. Our results show that the variation in consumer expertise and valuation of information affects the choice of a pricing strategy by the server. We present general conditions under which subscription-fee pricing is optimal even when consumer demand is inelastic. We also find that, given the cost structures characterizing the market, undifferentiated online servers can compete and coexist in the market each making positive profits. We show that in a competitive setting an increase in costs of online servers can sometimes benefit them by enabling them to differentiate themselves. Our results offer insights into the trends in pricing strategies and may provide an explanation as to why many servers may persist with connect-time strategies."
/doi/10.1287/mnsc.8.4.408," Decision-makers and policy-makers in education face some options to which systems analysts and economists might make contributions. Some of these options are identified within three clusters: how much to invest in education, how to allocate that investment, and what technology and organization to use. Tuition and fees, scholarships, admissions and retention requirements, and counseling policies are viewed as policy settings that can more explicitly encourage educational flows appropriate to development objectives. Education is shown to be a dispersed and difficult “market” for research services."
/doi/10.1287/isre.2021.1062," Healthcare entitlement programs in the United States represent a large and growing financial outlay for taxpayers. In the pursuit of operational efficiencies, program administrators often contract with private managed care organizations (MCOs) to procure insurance for beneficiaries. This, however, encourages MCOs to attract the healthiest beneficiaries and avoid the sickest, a phenomenon known as risk selection. This paper investigates whether risk selection can be mitigated with a mechanism where MCOs bid to enroll each individual beneficiary. Although procurement auctions have been studied extensively in the literature, extant research has rarely discussed individual-level bidding. Digitization can contribute to the development and introduction of efficient market structures and mechanisms for matching beneficiaries with appropriate MCOs. We model demand- and supply-side aspects in a two-sided insurance marketplace to examine three mechanisms, risk adjustment, bidding, and a mix of prospective payment and bidding, with and without reserve prices. Analytical results show that traditional risk adjustment cannot optimally be used to eliminate risk selection, whereas the bidding mechanisms eliminate it entirely. Mixed bidding eliminates risk selection at a strictly lower cost than pure bidding. The proposed mixed bidding approach is a new type of mechanism with preauction offers that strictly dominates the second-price auction without requiring additional assumptions. Numerical analysis shows bidding dominates risk adjustment in 75.1% of simulated parameter sets. Compared with risk adjustment, bidding secures coordinated care for 12.1% more allocated beneficiaries while lowering program costs by 9.2% and largely preserving MCO profits. This would amount to approximately $27.2 billion in Medicaid program savings. Sensitivity analysis reveals that the proposed bidding mechanism dominates in scenarios that closely resemble real-world healthcare entitlement environments. These results show that digital markets that enable individual-level auctions are a promising approach for achieving the dual aim of financial sustainability and expanded access to care for the most vulnerable."
/doi/10.1287/isre.1110.0370," Today, few firms could survive for very long without their computer systems. IT has permeated every corner of firms. Firms have reached the current state in their use of IT because IT has provided myriad opportunities for firms to improve performance and, firms have availed themselves of these opportunities. Some have argued, however, that the opportunities for firms to improve their performance through new uses of IT have been declining. Are the opportunities to use IT to improve firm performance diminishing? We sought to answer this question. In this study, we develop a theory and explain the logic behind our empirical analysis; an analysis that employs a different type of event study. Using the volatility of firms' stock prices to news signaling a change in economic conditions, we compare the stock price behavior of firms in the IT industry to firms in the utility and transportation and freight industries. Our analysis of the IT industry as a whole indicates that the opportunities for firms to use IT to improve their performance are not diminishing. However, there are sectors within the IT industry that no longer provide value-enhancing opportunities for firms. We also find that IT products that provided opportunities for firms to create value at one point in time, later become necessities for staying in business. Our results support the key assumption in our work."
/doi/10.1287/msom.1080.0239," A classic example that illustrates how observed customer behavior impacts other customers' decisions is the selection of a restaurant whose quality is uncertain. Customers often choose the busier restaurant, inferring that other customers in that restaurant know something that they do not. In an environment with random arrival and service times, customer behavior is reflected in the lengths of the queues that form at the individual servers. Therefore, queue lengths could signal two factors—potentially higher arrivals to the server or potentially slower service at the server. In this paper, we focus on both factors when customers' waiting costs are negligible. This allows us to understand how information externalities due to congestion impact customers' service choice behavior. In our model, based on private information about both the service-quality and queue-length information, customers decide which queue to join. When the service rates are the same and known, we confirm that it may be rational to ignore private information and purchase from the service provider with the longer queue when only one additional customer is present in the longer queue. We find that, due to the information externalities contained in queue lengths, there exist cycles during which one service firm is thriving whereas the other is not. Which service provider is thriving depends on luck; i.e., it is determined by the private signal of the customer arriving when both service providers are idle. These phenomena continue to hold when each service facility has multiple servers, or when a facility may go out of business when it cannot attract customers for a certain amount of time. Finally, we find that when the service rates are unknown but are negatively correlated with service values, our results are strengthened; long queues are now doubly informative. The market share of the high-quality firm is higher when there is service rate uncertainty, and it increases as the service rate decreases. When the service rates are positively correlated with unknown service values, long queues become less informative and customers might even join shorter queues."
/doi/10.1287/orsc.12.2.99.10114," Does owner management necessarily eliminate the agency costs of ownership? Drawing on agency literature and on the economic theory of the household, we argue that private ownership and owner management expose privately held, owner-managed firms to agency threats ignored by Jensen’s and Meckling’s (1976) agency model. Private ownership and owner management not only reduce the effectiveness of external control mechanisms, they also expose firms to a “self-control” problem created by incentives that cause owners to take actions which “harm themselves as well as those around them” (Jensen 1994, p. 43). Thus, shareholders have incentive to invest resources in curbing both managerial and owner opportunism. We extend this thesis to the domain of the family firm. After developing hypotheses which describe how family dynamics and, specifically, altruism, exacerbate agency problems experienced by these privately held, owner-managed firms, we use data obtained from a large-scale survey of family businesses to field test our hypotheses and find evidence which suggests support for our proposed theory. Finally, we discuss the implications of our theory for research on family and other types of privately held, owner-managed firms."
/doi/10.1287/mksc.2013.0829," Disaggregate demand in the marketplace exists on a grid determined by the package sizes offered by manufacturers and retailers. Although consumers may want to purchase a continuous-valued amount of a product, realized purchases are constrained by available packages. This constraint might not be problematic for high-volume demand, but it is potentially troubling when demand is small. Despite the prevalence of packaging constraints on choice, economic models of choice have been slow to deal with their effects on parameter estimates and policy implications. In this paper we propose a general framework for dealing with indivisible demand in economic models of choice, and we show how to estimate model parameters using Bayesian methods. Analyses of simulated data and a scanner-panel data set of yogurt purchases indicate that ignoring packaging constraints can bias parameter estimates and measures of model fit, which results in the inaccurate measures of metrics such as price elasticity and compensating value. We also show that a portion of nonpurchase in the data (e.g., 2.27% for Yoplait Original) reflects the restriction of indivisibility, not the lack of preference. The importance of demand indivisibility is also highlighted by the counterfactual study where the removal of the smallest package size (i.e., 4 oz) mainly results in nonpurchase in the yogurt category instead of switching to larger package sizes."
/doi/10.1287/mnsc.2017.2888," This study provides novel evidence that expert economic agents’ work-related activities are systematically influenced by the time of day. We use archival data derived from time-stamped quarterly earnings conference calls together with linguistic algorithms to measure and track the moods of executives and analysts at different times of the day. The evidence indicates that the tone of conference call discussions deteriorates markedly over the course of the trading day, with both analysts’ and executives’ moods becoming more negative as the day wears on. Capital market pricing tests reveal that the time-of-day-induced negative tone leads to temporary stock mispricings. Our findings are relevant because the diurnal variations in behavior documented in the context of quarterly earnings calls are likely to extend across other important corporate communication, decision making, and performance situations, leading to potentially significant economic consequences. This paper was accepted by Shiva Rajgopal, accounting."
/doi/10.1287/isre.2021.1004," Pokémon Go, a mobile game that utilizes augmented reality (AR) technology, garnered tremendous public interest upon release with an average of 20 million active daily users. The game combines geospatial elements with gamification practices to incentivize user movement in the physical world. In this study, we examine the potential externalities that such incentives may have on associated businesses. Particularly, we study the impact of Pokémon Go on local restaurants by using online reviews as a proxy for consumer engagement and perception. We treat the release of Pokémon Go as a natural experiment and study the postrelease impact on associated restaurants. We find that restaurants associated with Pokémon Go do indeed enjoy a higher level of consumer engagement and more positive consumer perception. In addition, we find that the characteristics of a restaurant moderate these effects significantly. To the best of our knowledge, our study is the first to examine the economic implications of augmented-reality applications. Therefore, our research lays the foundation for future studies to examine how augmented-reality applications affect consumer economic behavior. Our study also provides insights for business owners and policymakers regarding the potential value of being associated with AR-based applications."
/doi/10.1287/mnsc.2015.2165," We show that the value of corporate diversification increased during the 2007–2009 financial crisis. Diversification gave firms both financing and investment advantages. First, conglomerates became significantly more leveraged relative to comparable focused firms. Second, conglomerates’ access to internal capital markets became more valuable, not just because external capital markets became more costly but also because the efficiency of internal capital allocation increased significantly during the crisis. Our analysis provides new evidence on how and why the value of diversification varies with financial constraints and economic conditions, and it suggests that corporate diversification can serve an important insurance function for investors. This paper was accepted by Amit Seru, finance."
/doi/10.1287/mnsc.2018.3036," We show that recent prominent equity factor models are to a large degree compatible with the Intertemporal CAPM (ICAPM) framework. Factors associated with alternative profitability measures forecast the equity premium in a way that is consistent with the ICAPM. Several factors based on firms’ asset growth predict a significant decline in stock market volatility, thus being consistent with their positive prices of risk. The investment-based factors are also strong predictors of an improvement in future economic activity. The time-series predictive ability of most equity state variables is not subsumed by traditional ICAPM state variables. Importantly, factors that earn larger risk prices tend to be associated with state variables that are more correlated with future investment opportunities or economic activity. Moreover, these risk price estimates can be reconciled with plausible risk-aversion parameter estimates. Overall, the ICAPM can be used as a common theoretical background for recent multifactor models. This paper was accepted by Karl Diether, finance."
/doi/10.1287/isre.2020.0986," Data security breaches (DSBs) are increasing investor and regulator pressure on firms to improve their IT governance (ITG) in an effort to mitigate the related risk. Drawing on upper echelon theory, we argue that DSB risk cannot be mitigated by one executive alone, but, rather, is a shared leadership responsibility of the top management team (TMT; i.e., Chief Executive Officer (CEO), Chief Financial Officer (CFO), and Chief Information Officer (CIO)). By examining a sample of DSBs from 2005 to 2017, our study finds that CEOs with IT expertise are associated with fewer DSBs, with some evidence of a focus on DSBs containing consumer information. Our evidence also suggests that CFOs with IT expertise are less likely to report a DSB in general, as well as DSBs involving employee information or instigated by a person outside of the firm and, to a weaker extent, DSBs containing consumer information. Further, the presence of a CIO as part of the TMT is significantly associated with reduced DSBs of all types examined. Our results are robust to endogeneity concerns and an alternative propensity score matched sample. This study contributes a granular investigation of DSB risk involving executives with IT expertise that extends the upper echelon and ITG literatures."
/doi/10.1287/mnsc.1080.0950," Although spillovers are a crucial factor in determining the optimal environment for innovation, there is no consensus regarding their impact on firm behavior. One reason for this may be that models differ in their assumptions for the functional form of the spillover pool. In industrial organization and economic geography, for example, the predominant convention is that all innovation within an industry/region contributes to a spillover pool that has a common value for all firms. An alternative convention prevalent in endogenous growth and evolutionary economics is that spillovers have directionality—the size of the relevant pool differs across firms. Knowing the correct functional form may facilitate theoretical consensus, either analytically (by modifying models' assumptions) or empirically (by supporting a critical test of competing theories). We characterize and test the functional form of spillover pools for efficiency-enhancing innovation across 50 markets in the banking industry. Our results in that setting are consistent with expectations for asymmetric spillovers but inconsistent with expectations for pooled spillovers."
/doi/10.1287/mnsc.2019.3520," Aggregate implied volatility spread (IVS), defined as the cross-sectional average difference in the implied volatilities of at-the-money call and put equity options, is significantly and positively related to future stock market returns at daily, weekly, and monthly to semiannual horizons. This return predictive power is incremental to existing return predictors, and it is significant both in sample and out of sample. Furthermore, IVS can forecast macroeconomic news up to one year ahead. The return predictability concentrates around macro news announcement. Common informed trading in equity options offers an integrated explanation for the ability of IVS to predict both future stock market returns and real economic activity. This paper was accepted by Tyler Shumway, finance."
/doi/10.1287/mnsc.2017.2840," We show that a model featuring an average commodity factor, a carry factor, and a momentum factor is capable of describing the cross-sectional variation of commodity returns. More parsimonious one- and two-factor models that feature only the average and/or carry factors are rejected. To provide an economic interpretation, we show that innovations in global equity volatility can price portfolios formed on carry, while innovations in a commodity-based measure of speculative activity can price portfolios formed on momentum. Finally, we characterize the relation between the factors and the investment opportunity set. Data and the Internet appendix are available at https://doi.org/10.1287/mnsc.2017.2840 This paper was accepted by Neng Wang, finance."
/doi/10.1287/isre.1090.0278," Enterprise systems software (ESS) is a multibillion dollar industry that produces systems components to support a variety of business functions for a widerange of vertical industry segments. Even if it forms the core of an organization's information systems (IS) infrastructure, there is little prior IS research on the competitive dynamics in this industry. Whereas economic modeling has generally provided the methodological framework for studying standards-driven industries, our research employs social network methods to empirically examine ESS firm competition. Although component compatibility is critical to organizational end users, there is an absence of industry-wide ESS standards and compatibility is ensured through interfirm alliances. First, our research observes that this alliance network does not conform to the equilibrium structures predicted by economics of network evolution supporting the view that it is difficult to identify dominant standards and leaders in this industry. This state of flux combined with the multifirm multicomponent nature of the industry limits the direct applicability of extant analytical models. Instead, we propose that the relative structural position acquired by a firm in its alliance network is a reasonable proxy for its standards dominance and is an indicator of its performance. In lieu of structural measures developed mainly for interpersonal networks, we develop a measure of relative firm prominence specifically for the business software network where benefits of alliances may accrue through indirect connections even if attenuated. Panel data analyses of ESS firms that account for over 95% of the industry revenues, show that our measure provides a superior model fit to extant social network measures. Two interesting counterintuitive findings emerge from our research. First, unlike other software industries compatibility considerations can trump rivalry concerns. We employ quadratic assignment procedure to show that firms freely form alliances even with their rivals. Second, we find that smaller firms enjoy a greater value from acquiring a higher structural position as compared to larger firms."
/doi/10.1287/mnsc.1060.0690," In this paper, a market-based decomposition method for decomposable linear systems is developed. The solution process iterates between a master problem that solves the market-matching problem , and subproblems that solve the agents’ bundle-determination problem s. Starting from any initial price and feasible allocation, system optimality can be achieved under a dynamic market-trading algorithm in a finite number of trades. The final market-clearing prices are discovered by this market trading and an efficient allocation is achieved by direct, wealth-improving resource exchanges among self-interested agents. Certain types of strategic behavior by the agents and a dealer in the marketplace are studied as well. Our proposed market mechanism addresses price dynamics, incentive issues, and economic transactions of real-world, distributed decision-making situations more realistically than traditional decomposition approaches. In addition, it can be operated in both synchronous and asynchronous environments. We provide a market-based paradigm for decentralized problem solving and information processing that can be easily implemented to support real-time optimization of distributed systems."
/doi/10.1287/mnsc.1050.0495," Coordination of interdependencies among firms’ productive activities has been advanced as a promising explanation for sustained heterogeneity in capabilities among firms. In this paper, we extend this line of research to determine the industry structures and patterns of expected firm profits for the case when difficulty optimizing interdependent activities does, in fact, generate and sustain capability heterogeneity among firms. We combine a widely used agent-based model where firms search to discover sets of activities that complement one another (reducing overall costs or raising product quality) with traditional economic models of competition among profit-maximizing firms. The agent-based model produces a distribution of performance (interpreted as variable cost or product quality) among firms and the competition models determine resulting industry outcomes including patterns of entry, exit, and profits. The integration of economic models of competition among firms with an agent-based model of search for improvement by firms reveals a rich relationship between interdependencies in production functions and industry structure, firm profits, and industry average profitability."
/doi/10.1287/msom.2.1.32.23268," Does cross-training workers allow a firm to achieve economies of scale when there is variability in the content of work, or does it create a workforce that performs many tasks with consistent mediocrity? To address this question we integrate a model of a stochastic service system with models for tenure- and experience-based service quality. When examined in isolation, the service system model confirms a well-known “rule of thumb” from the queueing literature: Flexible or cross-trained servers provide more throughput with fewer workers than specialized servers. However, in the integrated model these economies of scale are tempered by a loss in quality. Given multiple tasks, flexible workers may not gain sufficient experience to provide high-quality service to any one customer, and what is gained in efficiency is lost in quality. Through a series of numerical experiments we find that low utilization in an all-specialist system can also reduce quality, and therefore the optimal staff mix combines flexible and specialized workers. We also investigate when the performance of the system is sensitive to the staffing configuration choice. For small systems with high learning rates, the optimal staff mix provides significant benefits over either extreme case (a completely specialized or completely flexible workforce). If the system is small and the rate of learning is slow, flexible servers are preferred. For large systems with high learning rates, the model leans toward specialized servers. In a final set of experiments, the model analyzes the design options for an actual call center."
/doi/10.1287/mnsc.1050.0432," One commonly held belief in designing auctions is that increasing the number of bidders makes an auction more competitive. Therefore, a buyer who wishes to minimize her procurement costs is better off inviting more suppliers to participate. In this paper, we question the validity of this belief by shedding light on bidders’ behavior when bidders experience economies of scale in production and differ in their production capacity. We consider a setting with two different sized bidders, global and small. We assume that global bidders have a large production capacity (can win in more than one auction) and experience economies of scale in production, whereas small bidders can win in at most one auction. In this new setting, we focus on the impact of allowing both global and small suppliers to compete against each other on the performance of an auction."
/doi/10.1287/opre.2018.1767," We study how design decisions in project planning affect the cost of execution. In organizing a project’s tasks into work packages, trade-offs arise. Defining small work packages increases project complexity and workload, and reduces economies of scale, whereas defining large work packages reduces concurrent processing and adversely affects cash flow. Our work is apparently the first to study this trade-off. We consider the objective of minimizing total project cost, subject to a deadline on project makespan. For serial task networks, we describe an efficient algorithm that finds optimal work package sizes. For acyclic task networks, we develop a heuristic method and a lower bound for the unary NP -hard problem. A computational study shows that our heuristic routinely delivers near-optimal solutions that substantially improve on those found by benchmark procedures. Our results demonstrate the value of deliberately varying work package sizes within a project, in contrast to typical project management practice. Related issues including multiple serial paths in parallel, task incompatibility, and generalized precedence constrained work packages are also discussed. Our work enables more precise planning of work packages to improve performance, documents the value of integrating the planning of work packages and schedules, and provides insights that guide resource allocation decisions. The e-companion is available at https://doi.org/10.1287/opre.2018.1767 ."
/doi/10.1287/isre.2013.0492," Virtual communities continue to play a greater role in social, political, and economic interactions. However, how users value information from these communities and how that affects their behavior and future expectations is not fully understood. Stock message boards provide an excellent setting to analyze these issues given the large user base and market uncertainty. Using data from 502 investor responses from a field experiment on one of the largest message board operators in South Korea, our analyses revealed that investors exhibit confirmation bias, whereby they preferentially treat messages that support their prior beliefs. This behavior is more pronounced for investors with higher perceived knowledge about the market and higher strength of belief (i.e., sentiment) toward a particular stock. We also find a negative interaction effect between the perceived knowledge and the strength of prior belief on confirmation bias. Those exhibiting confirmation bias are also more overconfident; as a result, they trade more actively and expect higher market returns than is warranted. Collectively, these results suggest that participation in virtual communities may not necessarily lead to superior financial returns."
/doi/10.1287/inte.30.6.17.11631," We developed a decision support system (DSS) for Nortel Networks, an international digital and internet network equipment manufacturer, to improve its planning of its remanufacturing operations for circuit assemblies. This has resulted in economic benefits to the company during the past three years. The system embodies a reverse-logistics model that allows decision makers to better plan the outbound and inbound product flows involved in making design changes. Careful modeling of the decision-making process and its embodiment in appropriate information technology were keys to the successful implementation of the project in Nortel's operations."
/doi/10.1287/inte.2015.0791," A lean enterprise ties together and synchronizes all elements of its value-delivery system to provide for the needs of target customers. Organizations applying lean concepts in a systematic manner have been highly successful from operational, market, and financial perspectives. Once effectively in place, however, lean value-delivery systems can be more easily derailed than their leaders imagine, particularly in turbulent times. We identify three potential challenges for lean enterprises: (1) locked operating model: failure to change the value-delivery system in response to market and economic disruptions; (2) backward drift to pre-lean practices: regression toward wasteful inventory accumulations, backlogs, and complexity; (3) lean islands: isolation of lean practices within the walls of the company. Our observations at Quadrant Homes as it navigated the Great Recession, supplemented by our research at other companies, leads to recommendations for lean organizations to apply in countering these challenges."
/doi/10.1287/isre.2020.0931," When using digital goods that extensively collect user information, privacy uncertainty, which is consumers’ difficulty in assessing the privacy of the data they entrust to others, is a major concern. We extend the existing literature on uncertainty in online marketplaces by incorporating privacy uncertainty, and we distinguish among three subdimensions of privacy uncertainty—namely collection, use, and protection. We subsequently theorize and empirically test the antecedents and consequences of this new construct in the context of mobile apps. Consistent with economic theory, we argue that because consumers possess less information than the app seller as a result of the presence of hidden characteristics and hidden action, privacy uncertainty is evoked. Using the factorial survey method, we test our theoretical model in the context of buying a mobile app. The results show that privacy uncertainty significantly influences potential users’ intention to use an app above and beyond their uncertainty about the seller and the product. Privacy uncertainty also affects the perceived risk associated with using an app and the price consumers are willing to pay for it. In addition, we find that privacy uncertainty is driven by the uncertainty about privacy practices in regard to the collection, use, and protection of data collected at the time of downloading the app—and more important, the data collected while the app is being used. The results of another factorial survey study suggest that privacy uncertainty is distinct from seller and product uncertainties and has unique drivers. The results of a survey of current users of existing mobile apps indicate that the effects of privacy uncertainty extend to the postadoption stage, where it remains a strong influencer of continued use intentions and perceived risk."
/doi/10.1287/mnsc.1050.0371," Motivated by a $2.2 billion inventory write-off by Cisco Systems, we investigate how duplicate orders can lead a manufacturer to err in estimating the demand rate and customers’ sensitivity to delay, and to make faulty decisions about capacity investment. We consider a manufacturer that sells through two distributors. If a customer finds that his distributor is out of stock, then he will sometimes seek to make a purchase from the other distributor; if the latter is also out of stock, the customer will order from both distributors. When his order is filled by one of the distributors, the customer cancels any duplicate orders. Furthermore, the customer cancels all of his outstanding orders after a random period of time. Assuming that the manufacturer is unaware of duplicate orders, we prove that she will overestimate both the demand rate and the cancellation rate. Surprisingly, failure to account for duplicate orders can cause short-term underinvestment in capacity. However, in long-term equilibrium under stable demand conditions the manufacturer overinvests in capacity. Our results suggest that Cisco’s write-off was caused by estimation errors and cannot be blamed entirely on the economic downturn. Finally, we provide some guidance on estimation in the presence of double orders."
/doi/10.1287/isre.1060.0083," The theory of incomplete contracts has been used to study the relationship between buyers and suppliers following the deployment of modern information technology to facilitate coordination between them. Previous research has sought to explain anecdotal evidence from some industries on the recent reduction in the number of suppliers selected to do business with buyers by appealing to relationship-specific costs that suppliers may incur. In contrast, this paper emphasizes that information technology enables greater completeness of buyer-supplier contracts through more economical monitoring of additional dimensions of supplier performance. The number of terms included in the contract is an imperfect substitute for the number of suppliers. Based on this idea, alternative conditions are identified under which increased use of information technology leads to a reduction in the number of suppliers without invoking relationship-specific costs. We find that a substantial increase in contract completeness due to reduced cost of information technology could increase the cost per supplier even though the cost of coordination and the cost per term monitored decrease. Such an increase in the cost per supplier leads to a reduction in the number of suppliers with whom the buyer chooses to do business. Similarly, we find that if coordination cost is reduced when more information technology is deployed so that the number of suppliers in the buyer’s pool increases substantially, the buyer might choose to make the supplier contracts less complete, instead relying on a more market-oriented approach to finding a supplier with good fit."
/doi/10.1287/orsc.2017.1152," Using a unique longitudinal study of U.S. biotechnology ventures, we advance extant research by showing that a founding team’s educational heterogeneity and prior founding experience have a positive and significant effect on the likelihood of a firm’s creating breakthrough innovation. However, we demonstrate that these relationships depend on the firm’s stage of life and decision-making structure as reflected in its founder–CEO duality. Specifically, we show that the positive effect of a founding team’s human capital is stronger in the growth stage than the early stages of a startup. While founder–CEO duality increases the positive effect of the founding team’s human capital in the startup stage, during the growth stage, such a structure reduces the impact of the founding team’s human capital. Therefore, to fully appreciate the effect of human capital on a venture’s success in breakthrough innovation, we must consider both the firm’s stage of life and its decision-making structure. As such, our theory provides a meeting ground for economists and organizational theorists on issues associated with human capital, founder’s power, the life cycle of new ventures, and technological entrepreneurship. The online appendix is available at https://doi.org/10.1287/orsc.2017.1152 ."
/doi/10.1287/mnsc.1110.1402," This paper studies assortment planning and pricing for a product category with heterogeneous product types from two brands. We model consumer choice using the nested multinomial logit framework with two different hierarchical structures: a brand-primary model in which consumers choose a brand first, then a product type in the chosen brand, and a type-primary model in which consumers choose a product type first, then a brand within that product type. We consider a centralized regime that finds the optimal solution for the whole category and a decentralized regime that finds a competitive equilibrium between two brands. We find that optimal and competitive assortments and prices have quite distinctive properties across different models. Specifically, with the brand-primary model, both the optimal and the competitive assortments for each brand consist of the most popular product types from the brand. With the type-primary choice model, the optimal and the competitive assortments for each brand may not always consist of the most popular product types of the brand. Instead, the overall assortment in the category consists of a set of most popular product types. The price of a product under the centralized regime can be characterized by a sum of a markup that is constant across all products and brands, its procurement cost, and its marginal operational cost, implying a lower price for more popular products. The markup may be different for each brand and product type under the decentralized regime, implying a higher price for brands with a larger market share. These properties of the assortments and prices can be used as effective guidelines for managers to identify and price the best assortments and to rule out nonoptimal assortments. Our results suggest that to offer the right set of products and prices, category and/or brand managers should create an assortment planning process that is aligned with the hierarchical choice process consumers commonly follow to make purchasing decisions. This paper was accepted by Pradeep Chintagunta and Preyas Desai, special issue editors. This paper was accepted by Pradeep Chintagunta and Preyas Desai, special issue editors."
/doi/10.1287/mnsc.1120.1530," In this paper, we argue that managers confront a paradox in selecting strategy. On one hand, capital markets systematically discount uniqueness in the strategy choices of firms. Uniqueness in strategy heightens the cost of collecting and analyzing information to evaluate a firm's future value. These greater costs in strategy evaluation discourage the collection and analysis of information regarding the firm, and result in a valuation discount. On the other hand, uniqueness in strategy is a necessary condition for creating economic rents and should, except for this information cost, be positively associated with firm value. We find empirical support for both propositions using a novel measure of strategy uniqueness in a firm panel data set between 1985 and 2007. This paper was accepted by Bruno Cassiman, business strategy."
/doi/10.1287/mnsc.2015.2304," Consumer reviews are now part of everyday decision making. Yet the credibility of these reviews is fundamentally undermined when businesses commit review fraud, creating fake reviews for themselves or their competitors. We investigate the economic incentives to commit review fraud on the popular review platform Yelp, using two complementary approaches and data sets. We begin by analyzing restaurant reviews that are identified by Yelp’s filtering algorithm as suspicious, or fake—and treat these as a proxy for review fraud (an assumption we provide evidence for). We present four main findings. First, roughly 16% of restaurant reviews on Yelp are filtered. These reviews tend to be more extreme (favorable or unfavorable) than other reviews, and the prevalence of suspicious reviews has grown significantly over time. Second, a restaurant is more likely to commit review fraud when its reputation is weak, i.e., when it has few reviews or it has recently received bad reviews. Third, chain restaurants—which benefit less from Yelp—are also less likely to commit review fraud. Fourth, when restaurants face increased competition, they become more likely to receive unfavorable fake reviews. Using a separate data set, we analyze businesses that were caught soliciting fake reviews through a sting conducted by Yelp. These data support our main results and shed further light on the economic incentives behind a business’s decision to leave fake reviews. This paper was accepted by Lorin Hitt, information systems ."
/doi/10.1287/mnsc.2019.3503," A persistent question in industrial organization is whether regulations restricting price discrimination in input markets can promote efficiency. Despite the extensive study of the economic effects of input pricing regulations, the literature is bereft of an examination of the role of accounting information. In this paper, we seek to fill the gap by modeling the effects of uniform pricing restrictions in input markets on firms’ information generation and disclosure. In doing so, we find that information considerations present an impetus for uniform pricing requirements since they promote incentives for retail firms to both acquire and disclose relevant accounting information. In effect, by shielding retail firms from excessive supplier exploitation, uniform pricing regulations create a richer and more transparent information environment. This, then, leads to welfare gains and even benefits that can accrue naturally to all supply chain partners including the supplier, whose actions are constrained by the uniform pricing regulation. This paper was accepted by Brian Bushee, accounting."
/doi/10.1287/msom.2019.0794," Problem definition : Innovation contest platforms are often organized around specific fields and host contests that span a variety of interdependent problem domains. Whereas contestants may benefit from related experience in contests whose problem domains share an interdependency with the focal problem domain, it is unclear whether the benefits of related experience arise symmetrically from upstream experience (i.e., experience in problem domains that provide input information to the focal problem domain) and downstream experience (i.e., experience in problem domains that use output information from the focal problem domain) or differ among them. Academic/practical relevance : Given that innovation contest platforms serve to effectively match contest problem requirements with contestants’ skills, it is important to understand how a contestant’s prior experience on a platform contributes to her problem-solving performance. Our research provides a more granular examination of the benefits of related experience than what has been examined in prior studies on individual learning or innovation contests. Methodology : We collected detailed archival data from TopCoder, a leading innovation contest platform that hosts contests across multiple interdependent software development problem domains, from its launch in 2001 to September 2013. Our data set comprises detailed participation histories of 821 contestants in 3,274 contests across eight interdependent problem domains involving 8,985 observations. Results : Whereas a contestant’s related experience on the innovation contest platform is more positively associated with her focal contest performance compared with unrelated experience, the benefits of related experience arise only from downstream experience. That is, there are no significant performance benefits of upstream experience. Furthermore, the performance benefits of downstream experience are greater when the contest duration is shorter, highlighting its role in enabling more efficient search and problem solving in innovation contest platforms with interdependent problem domains. Managerial implications : Contrary to the notion of “hyperspecialization,” our findings suggest that contestants can reap benefits from diversifying their experience into downstream problem domains on innovation contest platforms. Furthermore, innovation contest platforms could facilitate such targeted diversification of contestant experience by developing more granular metrics of contestant experience across problem domains. Our findings also have implications for resource allocation and job rotation decisions in software development projects within firms."
/doi/10.1287/opre.2019.1907," Naor’s celebrated paper studies customer decisions in an observable M/M/1 queue in which joining-customers utility is linearly decreasing with the joining position. Naor derives the optimal threshold strategies for the individuals, social planner, and monopolist and proves that the monopoly optimal threshold is (weakly) smaller than the socially optimal threshold, which is (weakly) smaller than the individually optimal one. Studies show, based on numerical observations and/or ad hoc proof techniques, that this triangular relation holds within various specific setups, in which the queuing process is not M/M/1 and/or when the utility is not linear. We point out properties that imply the aforementioned result in Naor’s model and its extensions and suggest model applications for our findings. Our formulation gives strictly stronger results than those currently appearing in the literature. We further provide simple examples in which the inequality does not hold."
/doi/10.1287/mnsc.2020.3950," We study a buyer’s problem of auditing suppliers within an existing network to ensure social responsibility compliance. The buyer suffers economic damages if a violation at a supplier is exposed (whether by the media, regulator, or nongovernmental organization). To avoid damages, the buyer may audit the network to identify noncompliance. If a supplier fails an audit, the buyer must take one of two costly actions: either rectify the supplier or drop the supplier (along with any dependent suppliers). Dropping a supplier changes the network topology, reducing competition and thereby increasing the buyer’s input cost arising from an equilibrium. We show that the buyer’s optimal dynamic auditing policy has two subphases: the buyer will first audit and drop some suppliers before either auditing and rectifying all remaining suppliers or halting auditing altogether. By halting, the buyer tolerates some noncompliance in the network (“see no evil, hear no evil”). Within the audit-and-drop subphase, when auditing only in the upper tier, the buyer always audits a least valuable unaudited supplier , yielding greater balance in the network. When the buyer audits both tiers, it might choose a supplier other than the least valuable. The buyer may choose a supplier in a pivotal position to help ascertain the viability of a portion of the network (“litmus test”). In extensions, we find that when violations in tier 1 carry a higher penalty for the buyer, the buyer may audit and rectify only tier 1 suppliers; when audits may be inaccurate, the buyer more likely tolerates a greater level of noncompliance. This paper was accepted by Charles Corbett, operations management."
/doi/10.1287/mnsc.2018.3109," We use principal component analysis on 55 bilateral exchange rates of 11 developed currencies to identify two important global risk sources in foreign exchange (FX) markets. The risk sources are related to Carry and Dollar but are not spanned by these factors. We estimate the market prices associated with the two risk sources in the cross-section of FX market returns and construct FX market-implied country-specific stochastic discount factors (SDFs). The SDF volatilities are related to interest rates and expected carry trade returns in the cross-section. The SDFs price international stock returns and are related to important financial stress indicators and macroeconomic fundamentals. The first principal risk is associated with the Treasury-EuroDollar (TED) spread, quantities measuring volatility, tail and contagion risks, and future economic growth. It earns a relatively small implied Sharpe ratio. The second principal risk is associated with the default and term spreads and quantities capturing volatility and illiquidity risks. It further correlates with future changes in the long-term interest rate and earns a large implied Sharpe ratio. This paper was accepted by Lauren Cohen, finance."
/doi/10.1287/orsc.2019.1340," Over the past two decades, organizations have established sanctioning systems as an important component of their ethical infrastructures to detect and punish wrongdoing. However, empirical knowledge about the overall effectiveness of such systems remains limited. Existing studies have mostly adopted a single-party perspective even though many wrongdoing situations involve dynamic multiparty interactions between actors, recipients, and observers of wrongdoing. Moreover, most existing research has emphasized an economic perspective—that sanctioning systems only affect behavior because of economic considerations while crowding out ethical ones. In this research, we develop a moral and normative perspective of sanctioning systems. Using a novel experimental game design, our study focuses on the investigative dimension of sanctioning systems to examine their psychological and behavioral effects in actor–recipient–observer wrongdoing interactions . Findings reveal that investigative sanctioning systems influence wrongdoing, reporting, and helping behaviors as well as alter ethical and normative considerations, such that as systems become stronger, wrongdoing behaviors are judged as more unethical and perceived as less typical than when weaker systems are in place. These moral judgments and norm perceptions mediate the effect of investigative sanctioning system strength on wrongdoing behavior. Our research extends previous empirical and theoretical work on sanctioning systems by applying a more holistic perspective and by demonstrating that highly effective systems can serve as important behavioral guides because they activate and alter moral and normative considerations about wrongdoing."
/doi/10.1287/orsc.1070.0279," Cyert and March’s A Behavioral Theory of the Firm has been acknowledged as one of the most fundamental pillars on which evolutionary theorizing in economics is built. Nelson and Winter’s 1982 book is pervaded by the philosophy and concepts previously developed by Cyert, March, and Simon. Behavioral notions, such as bounded rationality are also at the heart of economic theories of institutions such as transaction costs economics. In this paper, after briefly reviewing the basic concepts of evolutionary economics, we discuss its implications for the theory of organizations (and business firms in particular), and we suggest that evolutionary theory should coherently embrace an “embeddedness” view of organizations, whereby the latter are not simply efficient solutions to informational problems arising from contract incompleteness and uncertainty, but also shape the “visions of the world,” interaction networks, behavioral patterns, and the identity of the agents. After outlining the basic features of this perspective, we analyze its consequences and empirical relevance."
/doi/10.1287/orsc.2013.0894," We report on a longitudinal study of the emergence of the ATLAS detector, a complex technological system developed at CERN, Geneva. Our data show that the coordination of initial architectural choices was driven by cycles of contestation and justification that resulted in the creation of what we term interlaced knowledge —pockets of shared knowledge interwoven within and across subsystem communities at ATLAS. We also found that these justifications were possible because of the presence of a boundary infrastructure that served as a common substrate of knowledge for all ATLAS participants. Together, the boundary infrastructure and interlaced knowledge enabled participants to make co-oriented technological choices, address latent interdependencies, and minimize the incidence and severity of glitches when integrating the various subsystems."
/doi/10.1287/mnsc.2020.3612," Entrepreneurial motivation is important to the process of economic growth. However, evidence on the motivations of innovative entrepreneurs, and how those motivations differ across fundamental characteristics, remains scant. We conduct three interrelated field experiments with the Massachusetts Institute of Technology Inclusive Innovation Challenge to study how innovative entrepreneurs respond to messages of money and social impact and how this varies across gender and culture. We find consistent evidence that women and individuals located in more altruistic cultures are more motivated by social-impact messages than money, whereas men and those in less altruistic cultures are more motivated by money than social impact. The estimates are not driven by differences in the type of company, its size, or other observable characteristics, but, instead, appear to come from differences in the underlying motivations of innovative entrepreneurs themselves. This paper was accepted by Joshua Gans, business strategy ."
/doi/10.1287/orsc.8.3.235," This paper underscores the importance of examining strategic response to institutional influences in light of hypercompetition. Focusing on the banking industry, which is hypercompetitive and highly institutionalized, affords a unique opportunity to understand how individual corporations in such an industry respond strategically to institutional pressures. We examine critical contingencies arising from hypercompetition that moderate institutional influences on information systems outsourcing in commercial banks. Using data from 226 banks and hierarchical moderated regression analyses, we show that the propensity of banks to conform to or resist institutional pressures depends on the nature of institutional pressures, perceived gain in production economies, financial capacity to resist institutional influences, and transaction cost considerations."
/doi/10.1287/orsc.1090.0509," We examine how long-term relationships affect brokers' returns, using project-level pricing data from an information technology staffing firm. We argue that long-term relationships between brokers and their counterparties affect both acquisition of private information and bargaining power, helping brokers to create and capture economic value. The results show that the staffing firm is able to charge a higher price and capture a higher proportion of that price when it has a long-term relationship with the worker. We also show that the staffing firm's ability to generate returns from its relationships is constrained when the brokered parties (worker and client firm) have a long-term relationship with each other. We discuss the implications of these findings for the study of market brokerage and long-term exchange relationships."
/doi/10.1287/mnsc.2016.2465," This study examines the relation between principles-based standards and earnings attributes. We create a firm-year-specific variable that measures the extent to which firms’ financial reporting is affected by principles-based standards. We find that firms’ earnings are more informative and persistent and have a larger positive association with future cash flows, on average, when firms’ standards are more principles based. We also find evidence that managers use the added discretion provided by principles-based standards to manage earnings when firms are near bankruptcy, issuing equity, or experiencing high growth, and if earnings are near prominent earnings benchmarks. Overall, our evidence is consistent with managers using the discretion provided by principles-based standards to communicate better the economic substance of transactions, on average, but also with some managers using the added discretion strategically when managerial incentives exist to increase reported earnings. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2016.2465 . This paper was accepted by Mary Barth, accounting ."
/doi/10.1287/inte.1110.0611," In this paper, we present a system that Compañía Sud Americana de Vapores (CSAV), one of the world's largest shipping companies, developed to support its decisions for repositioning and stocking empty containers. CSAV's main business is shipping cargo in containers to clients worldwide. It uses a fleet of about 700,000 TEU containers of different types, which are carried by both CSAV-owned and third-party ships. Managing the container fleet is complex; CSAV must make thousands of decisions each day. In particular, imbalances exist among the regions. For example, China often has a deficit of empty containers and is a net importer; Saudi Arabia often has a surplus and is a net exporter. CSAV and researchers from the University of Chile developed the Empty Container Logistics Optimization System (ECO) to manage this imbalance. ECO's multicommodity, multiperiod model manages the repositioning problem, whereas an inventory model determines the safety stock required at each location. CSAV uses safety stock to ensure high service levels despite uncertainties, particularly in the demand for containers. A hybrid forecasting system supports both the inventory and the multicommodity network flow model. Major improvements in data gathering, real-time communications, and automation of data handling were needed as input to the models. A collaborative Web-based optimization framework allows agents from different zones to interact in decision making. The use of ECO led to direct savings of $81 million for CSAV, a reduction in inventory stock of 50 percent, and an increase in container turnover of 60 percent. Moreover, the system helped CSAV to become more efficient and to overcome the 2008 economic crisis."
/doi/10.1287/msom.2018.0759," We empirically study the spatiotemporal location problem motivated by an online retailer that uses the Buy-Online-Pick-Up-In-Store fulfillment method. Customers pick up their orders from trucks parked at specific locations on specific days, and the retailer’s problem is to determine where and when these pickups occur. Customer demand is influenced by the convenience of pickup locations and days. We combine demographic and economic data, business location data, and the retailer’s historical sales and operations data to predict demand at potential locations. We introduce a novel procedure that combines machine learning and econometric techniques. First, we use a fixed effects regression to estimate spatial and temporal cannibalization effects. Then, we use a random forests algorithm to predict demand when a particular location operates in isolation. Based on the predicted demand and cannibalization effects, we solve the spatiotemporal integer program using a quadratic program relaxation to find the optimal pickup location configuration and schedule. We estimate a revenue increase of at least 51% from the improved location configuration and schedule. The online appendices are available at https://doi.org/10.1287/msom.2018.0759 ."
/doi/10.1287/mnsc.2019.3394," Interfirm contracts are plagued by opportunism arising from exchange hazards that increase the seller’s gains from holdup in fixed price contracts. These exchange hazards are higher when the seller can engage in unverifiable deliberate obfuscation. Although cost-plus contracts reduce holdup losses, they suffer from cost inefficiency. Past research has underscored the importance of trust as a control instrument to mitigate losses from exchange hazards, especially social relational trust that develops from past experiences. However, trust can also be calculative when it develops from the expectation of future economic gains to the buyer-seller dyad. We identify two dyadic mechanisms that generate calculative trust and curtail the likelihood of cost-inefficient behavior in cost-plus contracts. These mechanisms include future potential and bilateral reputation capital for cost containment. Analysis using probit estimations on 149 information technology outsourcing contracts for the period 1998 to 2005 suggests that calculative trust increases the likelihood of cost-plus contracts. Thus, calculative trust can mitigate inefficiencies in interfirm contracts. This paper was accepted by Shiva Rajgopal, accounting."
/doi/10.1287/mnsc.1050.0416," The “resource-based view of the firm” has become an important conceptual framework in strategic management but has been widely criticized for lack of an empirical base. To address this deficit, we utilize a new method for identifying interfirm differences in efficiency within the context of stochastic frontier production functions. Using data on Japanese and U.S. automobile manufacturers, we develop measures of resources and capabilities and test for linkages with firm performance. The results show the influence of manufacturing proficiency and scale economies at the firm and plant level. We apply the parameter estimates to account for Toyota’s superior efficiency relative to other producers."
/doi/10.1287/msom.2013.0461," In this paper, we explore how firms can better manage their sourcing by developing relationships not only with their suppliers but also with their suppliers' suppliers. We detail an empirical case study explaining how the firm developed relationships with its suppliers and raw material suppliers via a collaborative center, the sourcing hub. We then analytically model the scenarios encountered in our empirical work and examine two facets of upstream sourcing under uncertain demand scenarios: (a) firms can supply raw material directly to their suppliers, and this may be beneficial for the firm and its suppliers; and (b) firms can bring their suppliers together at the sourcing hub, and the resulting cooperation between suppliers is beneficial for the suppliers and the raw material suppliers. Overall, our work explores the market and economic conditions under which active management of upstream sourcing can add value to supply chains."
/doi/10.1287/mnsc.2014.1972," Advances in digital technologies have led to an increasing concern about piracy for providers of digital content (e.g., e-books, games, music, software, videos). Yet controversies exist over the influence of copyright protection on firm profitability. The objective of this paper is to provide an alternative rationale for the growing anti-protection trend and to investigate optimal copyright enforcement and quality provision in a monopoly setting. The proposed economic mechanism centers on the influence of copyright protection on consumer search when consumers can get to know the firm’s actions (e.g., price, quality) only after costly search. We show that more stringent copyright protection can induce the consumers to rationally expect lower ex post surplus, thus exerting a negative strategic effect on the consumers' willingness to search. This strategic effect may outweigh the positive main effect on the relative attractiveness of the authorized versus the pirated product, which can hence explain the optimality of incomplete and even zero copyright protection policies in markets where consumer prepurchase search is important (e.g., information goods, digital products). It is also because of this strategic effect that the firm may provide lower quality as copyright enforcement increases. Interestingly, quality unobservability can moderate this strategic effect and thus lead to an increasing incentive for copyright protection. This paper was accepted by Pradeep Chintagunta, marketing."
/doi/10.1287/mnsc.2020.3935," Cheap-talk communication between parties with conflicting interests is common in many business and economic settings. Two distinct behavioral economics theories, the trust-embedded model and the level-k model , have emerged to explain how cheap talk works between human decision makers. The trust-embedded model considers that decision makers are motivated by nonpecuniary motives to be trusting and trustworthy. In contrast, the level-k model considers that decision makers are purely self-interested but limited in their ability to think strategically. Although both theories have been successful in explaining cheap-talk behaviors in separate contexts, they point to contrasting drivers for human behaviors. In this paper, we provide the first direct comparison of both theories within the same context. We show that, in a cheap-talk setting that well represents many practical situations, the two models make characteristically distinct and empirically distinguishable predictions. We leverage past experiment data from this setting to determine what aspects of cheap-talk behavior each model captures well and which model (or combination of models) has better explanatory power and predictive performance. We find that the trust-embedded model emerges as the dominant explanation. Our results, thus, highlight the importance of investing in systems and processes to foster trusting and trustworthy relationships in order to facilitate more effective cheap-talk interactions. This paper was accepted by Charles Corbett, operations management."
/doi/10.1287/mnsc.2019.3542," Technical debt refers to the design, development, and implementation shortcuts taken by firms when deploying accounting information systems. Prior system-level studies have shown that such shortcuts decrease the reliability of systems and increase the long-term system maintenance obligations. On the one hand, technical debt may cause system disruptions that impair firm-level performance. On the other hand, incurring technical debt may aid firms to expedite their systems deployment and to implement idiosyncratic functionalities that may enhance performance. In this firm-level study, we examine the economic implications of technical debt accumulated by 26 firms in their customer relationship management (CRM) systems over an 11-year period. We find that firms operating in industries with higher “clockspeed” and higher competitive threats tend to accumulate more technical debt. After controlling for industry- and firm-level factors, our analysis reveals that technical debt embedded in the CRM systems negatively impacts firms’ performances, measured as gross profit scaled by beginning-of-year total assets ( GROA ). We estimate that a 10% increase in technical debt reduces GROA by 16% on average. The negative impact of technical debt on GROA increases over the lifecycle of the systems, which significantly reduces the long-term business value of those systems. Highly experienced information technology teams and the presence of a chief information officer in a firm’s top management team, however, serve to mitigate, at least partially, the negative impact of technical debt. We discuss the implications of these findings for research on the business value and governance of accounting information systems and performance evaluation. This paper was accepted by Shiva Rajgopal, accounting."
/doi/10.1287/mnsc.1030.0161," Long-distance telephone companies in the United States pay access fees to local telephone companies to transport calls that originate and terminate on their networks. These charges form the largest portion of the cost of providing long-distance service. Recent changes in the structure of access rates, which were mandated by the Federal Communications Commission (FCC), have created opportunities for long-distance companies to better manage access costs. In this paper, we develop an optimization-based approach to the economic design of access networks. Our novel solution approach combines stochastic aspects of the problem with a challenging discrete facility location problem in a three-phase algorithm. Computational results indicate a potential cost savings of hundreds of millions of dollars annually for long-distance companies."
/doi/10.1287/isre.2014.0534," We use the under-recognized income accounting identity to provide an important theoretical basis for using the Cobb-Douglas production function in IT productivity analyses. Within the income accounting identity we partition capital into non-IT and IT capital and analytically derive an accounting identity (AI)-based Cobb-Douglas form that both nests the three-input Cobb-Douglas and provides additional terms based on wage rates and rates of return to non-IT and IT capital. To empirically confirm the theoretical derivation, we use a specially constructed data set from a subset of the U.S. manufacturing industry that involve elaborate calculations of rates of return—a data set that is infeasible to obtain for most productivity studies—to estimate the standard Cobb-Douglas and our AI-based form. We find that estimates from our AI-based form correspond with those of the Cobb-Douglas, and our AI-based form has significantly greater explanatory power. In addition, empirical estimation of both forms is relatively robust to the assumption of intertemporally stable input shares required to derive the AI-based form, although there may be limits. Thus, in the context of future research the Cobb-Douglas form and its application in IT productivity work have a theoretically and empirically supported basis in the accounting identity. A poor fit to data or unexpected coefficient estimates suggests problems with data quality or intertemporally unstable input shares. Our work also shows how some returns to IT that do not show up in output elasticities can be found in total factor productivity (TFP)—the novel ways inputs are combined to produce output. The critical insight for future research is that many unobservables that have been considered part of TFP can be manifested in rates of return to IT capital, non-IT capital, and labor—rates of return that are separated from TFP in our AI-based form. Finally, finding that the additional rates of return terms partially explain TFP confirms the need for future IT productivity researchers to incorporate time-varying TFP in their models."
/doi/10.1287/msom.2019.0845," Problem definition : In this paper, we study a reliable hub location model, the objective of which is to minimize the associated costs plus the penalty for unserved demands. The model assigns to each origin–destination pair a primary path and a backup path to hedge against the risk of random disruptions. Aside from the fixed costs of locating hubs, the fixed costs incurred by hub-connecting arcs that exhibit economies of scale in transportation are considered. Academic/practical relevance : The widely adopted hub-and-spoke architecture in the network designs can trigger cascading effects during and in the aftermath of disruption events and lead to further losses. By incorporating a general specification of disruptions, our model helps firms improve network reliability while inheriting the benefits of routing via hubs. Therefore, this work extends the literature on hub location problems by considering reliability under disruptions. Methodology : We present a path-based model and formulate a combinatorial optimization problem. By exploiting the structural properties of the problem, we introduce a tractable mixed-integer linear program reformulation and develop a constraint generation method to accelerate the solution procedure. We further construct two effective heuristics. Results : We demonstrate the necessity of considering disruptions and the profound benefits of employing backup paths. Moreover, we show via numerical studies that the path-based model delivers efficient and effective reliable hub-and-spoke designs. Managerial implications : We find that establishing backup paths according to the proposed model barely escalates network costs yet significantly enhances service levels, and that exploiting disruption correlation helps reduce the expected total cost especially when the underlying disruption correlation is strong. We also discover that the decisions of our model on the locations of hubs and hub arcs are relatively robust compared with those on path assignments. We summarize key insights as observations that can be used directly as rules of thumb to guide designs in practice."
/doi/10.1287/isre.1100.0340," The highly competitive and rapidly changing market for online services is becoming increasingly effective at locking users in through the coercive effects of switching costs. Although the information systems field increasingly recognizes that switching costs plays a big part in enforcing loyalty, little is known about what factors users regard as switching costs or why they perceive these costs. Consequently, it is hard for online services to know what lock-in strategies to use and when to apply them. We address this problem by first developing a theory-driven structure of online users' perceived switching costs that distinguishes between vendor-related and user-related factors. We then propose that important antecedent influences on switching costs from economic value, technical self-efficacy, and past investments are more complex and intertwined than previously thought. We empirically validated the proposed model using data collected from home users of Internet service providers. Our findings demonstrate that an online service's economic value more heavily influences users' perceptions of vendor-related switching costs than does technical self-efficacy. However, users' technical abilities outweigh economic value in influencing user-related switching costs. Furthermore, although we confirmed the commonly held notion that deeply invested users are generally more vulnerable to lock-in, we also found that this relationship is contingent on users' technical abilities. Finally, we found that our multidimensional measure of switching costs is a valid predictor of user loyalty and is more powerful than previous global measures. Overall, this study uncovered a finer network of switching-cost production than had been previously established and suggests a new approach to modeling and exploiting online users' perceived switching costs."
/doi/10.1287/orsc.11.1.35.12567," The 1990s have seen a resurgence of interest in information privacy. Public opinion surveys show that many citizens are becoming greatly concerned about threats to their information privacy, with levels of such concern reaching all-time highs. Perhaps as a response to the growing concerns of citizens, the media are devoting more attention to privacy issues, and governmental regulation of the corporate privacy environment is increasing in many countries. Almost all developed countries have grappled with the trade-offs between open access to information—which enables economic efficiency—and an individual's right to privacy. Consistent with these trade-offs, many recent incidents suggest that regulatory approaches to information privacy, corporate management of personal data, and consumer reactions are becoming tightly interwoven around the world. To provide some insights into these relationships, we develop a conceptual model and test it with a cross-cultural sample from 19 different countries. In general, we find that a country's regulatory approach to the corporate management of information privacy is affected by its cultural values and by individuals' information privacy concerns. In addition, as governments become more involved in the corporate management of information privacy, internal management of such issues seems to tighten. This result supports previous observations that most firms take a primarily reactive approach to managing privacy by waiting for an external threat before crafting cohesive policies that confront their information practices. Moreover, when corporations are not perceived to adequately manage information privacy issues, and/or when privacy concerns rise, individuals are more inclined to prefer government intervention and be distrustful of firm self-regulation. As such, citizens may look to lawmakers to enact stricter regulation to reduce their privacy concerns. These findings and several international trends suggest that the self-regulatory model of privacy governance may not be sustainable over the long term. Findings from this research constitute an important contribution to the emerging theoretical base of information privacy research and should be particularly enlightening to those managing information privacy issues. Several directions for future research are also discussed."
/doi/10.1287/orsc.1070.0314," In this paper we propose that norms-based intellectual property (IP) systems exist today and are an important complement to or substitute for law-based IP systems. Norms-based IP systems, as we define them, operate entirely on the basis of implicit social norms that are held in common by members of a given community. Within that community, they offer functionality similar to contemporary law-based IP systems with respect to both the nature of rights protected and the effectiveness of the protection provided. We document the existence of a norms-based IP system among a sample of accomplished French chefs. These chefs consider recipes they develop to be a very valuable form of IP. At the same time, recipes are not a form of innovation that is effectively covered by law-based IP systems. Via grounded research, we identify three strong implicit social norms related to the protection of recipe IP. Via quantitative research, we find that accomplished chefs enforce these norms and apply them in ways that enhance their private economic returns from their recipe-related IP. In our discussion, we compare the attributes of norms-based and law-based IP systems, arguing that each has different advantages and drawbacks. We also point out that the existence of norms-based IP systems means that many information commons may prove to be criss-crossed by norms-based fences, with community access controlled by community IP owners."
/doi/10.1287/mnsc.45.12.1613," We study the strategy of bundling a large number of information goods, such as those increasingly available on the Internet, and selling them for a fixed price. We analyze the optimal bundling strategies for a multiproduct monopolist, and we find that bundling very large numbers of unrelated information goods can be surprisingly profitable. The reason is that the law of large numbers makes it much easier to predict consumers' valuations for a bundle of goods than their valuations for the individual goods when sold separately. As a result, this “predictive value of bundling” makes it possible to achieve greater sales, greater economic efficiency, and greater profits per good from a bundle of information goods than can be attained when the same goods are sold separately. Our main results do not extend to most physical goods, as the marginal costs of production for goods not used by the buyer typically negate any benefits from the predictive value of large-scale bundling. While determining optimal bundling strategies for more than two goods is a notoriously difficult problem, we use statistical techniques to provide strong asymptotic results and bounds on profits for bundles of any arbitrary size. We show how our model can be used to analyze the bundling of complements and substitutes, bundling in the presence of budget constraints, and bundling of goods with various types of correlations and how each of these conditions can lead to limits on optimal bundle size. In particular we find that when different market segments of consumers differ systematically in their valuations for goods, simple bundling will no longer be optimal. However, by offering a menu of different bundles aimed at each market segment, bundling makes traditional price discrimination strategies more powerful by reducing the role of unpredictable idiosyncratic components of valuations. The predictions of our analysis appear to be consistent with empirical observations of the markets for Internet and online content, cable television programming, and copyrighted music."
/doi/10.1287/ijoc.2020.0963," Emerald ash borer (EAB), a wood-boring insect native to Asia and invading North America, has killed untold millions of high-value ash trees that shade streets, homes, and parks and caused significant economic damage in cities of the United States. Local actions to reduce damage include surveillance to find EAB and control to slow its spread. We present a multistage stochastic mixed-integer programming (M-SMIP) model for the optimization of surveillance, treatment, and removal of ash trees in cities. Decision-dependent uncertainty is modeled by representing surveillance decisions and the realizations of the uncertain infestation parameter contingent on surveillance as branches in the M-SMIP scenario tree. The objective is to allocate resources to surveillance and control over space and time to maximize public benefits. We develop a new cutting-plane algorithm to strengthen the M-SMIP formulation and facilitate an optimal solution. We calibrate and validate our model of ash dynamics using seven years of observational data and apply the optimization model to a possible infestation in Burnsville, Minnesota. Proposed cutting planes improve the solution time by an average of seven times over solving the original M-SMIP model without cutting planes. Our comparative analysis shows that the M-SMIP model outperforms six different heuristic approaches proposed for the management of EAB. Results from optimally solving our M-SMIP model imply that under a belief of infestation, it is critical to apply surveillance immediately to locate EAB and then prioritize treatment of minimally infested trees followed by removal of highly infested trees. Summary of Contributions: Emerald ash borer (EAB) is one of the most damaging invasive species ever to reach the United States, damaging millions of ash trees. Much of the economic impact of EAB occurs in cities, where high-value ash trees grow in abundance along streets and in yards and parks. This paper addresses the joint optimization of surveillance and control of the emerald ash borer invasion, which is a novel application for the INFORMS society because, to our knowledge, this specific problem of EAB management has not been published before in any OR/MS journals. We develop a new multi-stage stochastic mixed-integer programming (MSS-MIP) formulation, and we apply our model to surveillance and control of EAB in cities. Our MSS-MIP model aims to help city managers maximize the net benefits of their healthy ash trees by determining the optimal timing and target population for surveying, treating, and removing infested ash trees while taking into account the spatio-temporal stochastic growth of the EAB infestation. We develop a new cutting plane methodology motivated by our problem, which could also be applied to other stochastic MIPs. Our cutting plane approach provides significant computational benefit in solving the problem. Specifically, proposed cutting planes improve the solution time by an average of seven times over solving the original M-SMIP model without cutting planes. We calibrate and validate our model using seven years of ash infestation observations in forests near Toledo, Ohio. We then apply our model to an urban forest in Burnsville, Minnesota, that is threatened by EAB. Our results provide insights into the optimal timing and location of EAB surveillance and control strategies."
/doi/10.1287/mnsc.2017.2863," Small suppliers often face challenges to obtain financing for their operations. Especially in developing economies, traditional financing methods can be very costly or unavailable to such suppliers. To reduce channel costs, large buyers have recently begun implementing their own financing methods that intermediate between suppliers and financing institutions. In this paper, we analyze the role and efficiency of buyer intermediation in supplier financing. Building a game-theoretical model, we show that buyer intermediated financing can significantly improve channel performance, and can simultaneously benefit both supply chain participants. Using data from a large Chinese online retailer and through structural regression estimation, we demonstrate that buyer intermediation lowers interest rates and wholesale prices, increases order fill rates, and boosts supplier borrowing. Based on counterfactual analysis on the data, we predict that the implementation of buyer intermediated financing will improve channel profits by 13.05%, increasing supplier and retailer profits by more than 10% each, and yielding approximately $44 million projected savings for the retailer. The online supplement is available at https://doi.org/10.1287/mnsc.2017.2863 . This paper was accepted by Vishal Gaur, operations management."
/doi/10.1287/mksc.2019.1220," In a randomized field experiment with the education charitable giving platform DonorsChoose.org ( N = 30,297), we examined email personalization using a potential donor’s name. We measured the effectiveness of matching potential donors to specific teachers in need based on surname, surname initial letters, gender, ethnicity, and surname country of origin. Full surname matching was most effective, with potential donors being more likely to open an email, click on a link in the email, and donate to teachers who shared their own surname. They also donated more money overall. Our results suggest that uniting people with shared names is an effective individual-level approach to email personalization. Potential donors who shared a surname first letter but not an entire name with teachers also behaved more generously. We discuss how using a person’s name in marketing communications may capture attention and bridge social distance."
/doi/10.1287/msom.2021.1009," Problem definition: The eco-toxicity arising from unused pharmaceuticals has regulators advocating the benign design concept of “green pharmacy,” but high research and development expenses can be prohibitive. We therefore examine the impacts of two regulatory mechanisms, patent extension and take-back regulation, on inducing drug manufacturers to go green. Academic/practical relevance: One incentive suggested by the European Environmental Agency is a patent extension for a company that redesigns its already patented pharmaceutical to be more environmentally friendly. This incentive can encourage both the development of degradable drugs and the disclosure of technical information. Yet, it is unclear how effective the extension would be in inducing green pharmacy and in maximizing social welfare. Methodology: We develop a game-theoretic model in which an innovative company collects monopoly profits for a patented pharmaceutical but faces competition from a generic rival after the patent expires. A social-welfare-maximizing regulator is the Stackelberg leader. The regulator leads by offering a patent extension to the innovative company while also imposing take-back regulation on the pharmaceutical industry. Then the two-profit maximizing companies respond by setting drug prices and choosing whether to invest in green pharmacy. Results: The regulator’s optimal patent extension offer can induce green pharmacy but only if the offer exceeds a threshold length that depends on the degree of product differentiation present in the pharmaceutical industry. The regulator’s correspondingly optimal take-back regulation generally prescribes a required collection rate that decreases as its optimal patent extension offer increases, and vice versa. Managerial implications: By isolating green pharmacy as a potential target to address pharmaceutical eco-toxicity at its source, the regulatory policy that we consider, which combines the incentive inherent in earning a patent extension on the one hand with the penalty inherent in complying with take-back regulation on the other hand, serves as a useful starting point for policymakers to optimally balance economic welfare considerations with environmental stewardship considerations."
/doi/10.1287/mksc.1110.0700," User-generated content on social media platforms and product search engines is changing the way consumers shop for goods online. However, current product search engines fail to effectively leverage information created across diverse social media platforms. Moreover, current ranking algorithms in these product search engines tend to induce consumers to focus on one single product characteristic dimension (e.g., price, star rating). This approach largely ignores consumers' multidimensional preferences for products. In this paper, we propose to generate a ranking system that recommends products that provide, on average, the best value for the consumer's money. The key idea is that products that provide a higher surplus should be ranked higher on the screen in response to consumer queries. We use a unique data set of U.S. hotel reservations made over a three-month period through Travelocity, which we supplement with data from various social media sources using techniques from text mining, image classification, social geotagging, human annotations, and geomapping. We propose a random coefficient hybrid structural model, taking into consideration the two sources of consumer heterogeneity the different travel occasions and different hotel characteristics introduce. Based on the estimates from the model, we infer the economic impact of various location and service characteristics of hotels. We then propose a new hotel ranking system based on the average utility gain a consumer receives from staying in a particular hotel. By doing so, we can provide customers with the “best-value” hotels early on. Our user studies, using ranking comparisons from several thousand users, validate the superiority of our ranking system relative to existing systems on several travel search engines. On a broader note, this paper illustrates how social media can be mined and incorporated into a demand estimation model in order to generate a new ranking system in product search engines. We thus highlight the tight linkages between user behavior on social media and search engines. Our interdisciplinary approach provides several insights for using machine learning techniques in economics and marketing research."
/doi/10.1287/mnsc.2019.3429," We draw on the skewness literature to propose regression-based performance evaluation tests designed for investments with option-like returns. These tests deliver conclusions valid for all risk-averse mean-variance-skewness investors and can better account for nonlinearities in returns than option-based factor models. Applied to mutual and hedge funds, our tests usually suggest selecting different funds than standard tests and find that a significant fraction (11%) of hedge funds adds value to investors, whereas this is an insignificant 4% for mutual funds. We also analyze the economic significance of these option-like returns and their out-of-sample persistence. This paper was accepted by Tyler Shumway, finance ."
/doi/10.1287/mnsc.49.1.71.12748," A widespread practice, particularly in public-sector procurement and dispersal, is to subsidize a class of competitors believed to be at an economic disadvantage. Arguments for such policies vary, but they typically assume that benefits of subsidization must be large enough to outweigh a presumed economic cost of the subsidy. When disadvantaged competitors compete in auctions, the subsidy serves to make them more competitive rivals. Other bidders rationally respond by bidding more aggressively. We consider a model of procurement auctions and show that a policy of subsidizing inefficient competitors can lower expected project cost and also enhance economic efficiency. Some subsidy is generally better than no subsidy for a wide range of parameters."
/doi/10.1287/mksc.21.1.14.159," People consume products in a variety of environments. They drink beer, for example, by themselves, with close friends, on the beach, when playing cards, at tailgate parties, and while having dinner with their boss. Within these environments, an individual may prefer Schaefer beer when drinking alone, Budweiser when having a party, Corona when lying on the beach, and Heineken when dining out. Preferences change across environments because the benefits sought by the consumer change. Consumers may feel thirsty while lying on the beach, and they may want to display refined tastes while dining out. Moreover, the effect of environment may not be homogeneous, as some people enjoy meeting new people in social gatherings while others may prefer to visit with those who are more familiar. Even though consumers face the same objective environment, different motivating conditions and brand preferences may arise. It is important for marketing managers to understand how brand preferences change across people, environments, and motivating conditions and, more importantly, which product attributes are associated with these changes. Communication and positioning decisions are more likely to be effective if the relationships among objective environment, motivating conditions, and preferences for brand attributes are known. If motivating conditions are uniquely associated with individuals across environments, or with environments across individuals, then the basis of marketing analysis is at the individual or environmental level. If, however, motivating conditions arise from the intersection of individuals and their environments, then analysis conducted at the individual or environmental level will be insufficient to understand human behavior. In such a case, firms may want to view different environments as distinct markets, each with its own pattern of heterogeneous wants and competitive environment. In this paper, the influence of objective environments and motivating conditions on brand preference is investigated. The mathematical model is based on the economic framework of utility maximization and discrete choice, and it accommodates three challenges that arise in modeling variation in brand preference. First, consumer consideration sets and purchase histories can vary widely across individuals in a relevant universe. Because brand preferences are the dependent variables in our analysis, our method must be able to accommodate a large number of brands to avoid restricting its measured variation as the objective environment and motivating conditions change. We propose a method using partial ranking data, combined with pairwise trade-off data, to obtain estimates of brand preference for all brands in our study. Second, the model must allow for multiple effects, leading to both within-person and across-person heterogeneity in preferences. Variation in brand preference is investigated within a hierarchical Bayes model in which motivating conditions are related to brand preference through a regression model in the random effects specification. Third, it is often counterintuitive for respondents to express preferences for attribute combinations that do not actually exist. A statistical method model is proposed for decomposing aggregate brand preferences into preferences for core and extended product attributes. Data are collected from a national survey of consumer off-premises beer consumption. A total of 842 respondents from six different geographic markets participated. Data include preferred brand sets under different objective environments, brand choice rankings, product attributes, and motivating conditions. Effect sizes for respondent and objective environment are both large. We found that the level of explained variance in brand and attribute preference attributable to motivating conditions is greater than that accounted for by a simple interaction of respondent and environmental effects, suggesting that motivations provide a more sensitive description of variation in brand preference. Our findings indicate that 1) across individuals the objective environment is associated with heterogeneous, not homogeneous, motivating conditions; 2) within an individual, motivating conditions may change with variation in the objective environment; and 3) motivating conditions are related to preferences for specific attributes. Our results imply that the unit of analysis for marketing is properly a person-activity occasion. Brands, for example, are used in individual instances of behavior—a brand performs well or poorly on individual occasions of use. The relevant universe is enumerated in person-activity occasions rather than in respondents. For some activities, such as doing the laundry, the occasions may typically occur in relatively unchanging environments, and it may be appropriate to allow respondents to summarize over occasions of the activity. For other activities, such as snacking or drinking beer, the activity may occur in distinct kinds of environment. In the case of such activities, it is appropriate to allow for the effect of changing environments to manifest themselves, if present. Doing so may require sampling from the relevant universe of person-activity occasions over an appropriate time frame. The design must be such as to record intraindividual variability due to changes in the environment for action."
/doi/10.1287/opre.1100.0823," Capacity addition and withdrawal decisions are among the most important strategic decisions made by firms in oligopolistic industries. In this paper, we develop and analyze a fully dynamic model of an oligopolistic industry with lumpy capacity and lumpy investment/disinvestment. We use our model to suggest answers to two questions: First, what economic factors facilitate preemption races? Second, what economic factors facilitate capacity coordination? With a series of examples we show that low product differentiation, low investment sunkness, and high depreciation tend to promote preemption races. The same examples also show that low product differentiation and low investment sunkness tend to promote capacity coordination. Although depreciation removes capacity, it might impede capacity coordination. Finally, our examples show that multiple equilibria arise over at least some range of parameter values. The distinct structures of these equilibria suggest that firms' expectations play a key role in determining whether or not industry dynamics are characterized by preemption races and capacity coordination. Taken together, our results suggest that preemption races and excess capacity in the short run often go hand-in-hand with capacity coordination in the long run."
/doi/10.1287/opre.1080.0588," This paper analyzes the relationships between personal decisions and premature deaths in the United States. The analysis indicates that over one million of the 2.4 million deaths in 2000 can be attributed to personal decisions and could have been avoided if readily available alternative choices were made. Separate analyses indicate 46% of deaths due to heart disease and 66% of cancer deaths are attributable to personal decisions, about 55% of all deaths for ages 15–64 are attributable to personal decisions, and over 94% of the deaths attributable to personal decisions result in the death of the individual making the decisions. Relative to the current 45%, retrospective appraisal suggests that roughly 5% of deaths in 1900 and 20%–25% of deaths in 1950 could be attributed to personal decisions. These results suggest that more effort directed toward improving personal choices regarding life risks may be an effective and economical way to save lives."
/doi/10.1287/orsc.2016.1069," Analogies to financial markets have proven powerful in establishing novel or potentially controversial business concepts, even in contexts that deviate significantly from financial markets. This phenomenon challenges theory that suggests analogies work best when elements from a source and target domain map closely to each other. To develop a theory that explains how organizations make initially imperfect analogies “work,” we use a case study of online advertising exchanges, a market-inspired model for buying and selling online advertising space. We find that as organizations stretch an initially misfitting exchange analogy from financial markets to online advertising, they iteratively bend their activities in superficial, structural, and generative ways to match the analogy and position themselves for advantage in the new space being created. Whereas prior studies emphasize shared cognition about familiar domains as the reason why analogies work, our study offers a dynamic account in which stretching, bending, and positioning combine to not only establish the financial market analogy but also subtly change the understanding of markets."
/doi/10.1287/mnsc.2017.2733," We study portfolio choice with multiple stocks and capital gains taxation, assuming that capital losses can only offset current or future realized capital gains. We show, through backtesting using empirical distributions, that optimal equity holdings over an extended period are significantly lower on average than benchmark holdings suggested in the literature. Using value and growth or small and large portfolios, the backtests show that allocations remain persistently underdiversified. Carryover losses have large economic significance since they can dramatically shrink the no-trade region. Finally, the backtested economic cost of incorrectly modeling capital losses is at least 8% of lifetime wealth. The Internet appendix is available at https://doi.org/10.1287/mnsc.2017.2733 . This paper was accepted by Neng Wang, finance."
/doi/10.1287/isre.2021.1064," This paper examines complementarity between clinical health information technology (HIT) applications and their effects on three hospital-level performance measures: clinical quality, experiential quality, and healthcare cost. We emphasize two aspects of HIT use in hospitals. First, we focus on whether HIT applications are used to perform primary or support clinical functions. Contingent on whether the use of HIT applications is for performing only primary functions or both primary and support functions, we conceptualize symbiotic and pooled HIT complementarity, respectively. Second, we focus on whether HIT applications are implemented in the same time period or different time periods. Contingent on this temporal aspect, we conceptualize simultaneous and sequential HIT complementarity, respectively. We collected panel data on HIT implementation, clinical quality, experiential quality, and healthcare cost for 715 hospitals in the United States from four sources. Our results suggest that symbiotic, pooled, simultaneous, and sequential complementarity among HITs impact hospital quality and cost outcomes. Our results further indicate that these complementary effects differ across chronic and acute conditions. We also find that three-way complementarity has significant economic effects on quality and cost. In fact, post hoc analyses indicate that three-way sequential complementarity effects, which have not been previously examined, are particularly significant. This paper contributes to the literature by empirically examining different forms of HIT complementarity in hospitals. Our central message is that when assessing HIT value in hospitals, managers and researchers must pay attention to (1) the clinical functions to which these technologies are applied; (2) the sequence in which these HITs are implemented; and (3) the prevalence of chronic versus acute patients admitted in the hospital."
/doi/10.1287/isre.1120.0438," We analyze the impact of information technology (IT) on the technical efficiency of firms in the context of their observed competitive settings. Because competition can be a driver of efficiency and industries display varying degrees of competitiveness, firm-level efficiency is likely to display considerable heterogeneity. To shed light on these questions, we analyze the economic impact of IT on technical efficiency, a key component of efficiency, in heterogeneous competitive settings. Our study employs a number of econometric techniques, including a stochastic frontier and a generalized method of moments approach, on data from firms in a wide cross-section of industries. We find, after controlling for firm-level heterogeneity and potential endogeneity, that IT is positively associated with gains in technical efficiency but its impact is moderated by the degree of competition. Firms display large variation in their levels of technical efficiency partly because of the heterogeneous market competitiveness conditions they face. In more competitive industries, firms tend to deploy IT more intensively and use it more efficiently. Our study makes a distinct contribution relative to prior studies that have focused on the productivity impacts of IT while assuming perfect competition and not allowing for potential heterogeneity in firm-level efficiency. Overall, our results demonstrate that IT and competition are significant determinants of gains in technical efficiency and provide insight into how competition affects the returns to IT investment."
/doi/10.1287/mnsc.1070.0823," This paper considers a call center outsourcing contract analysis and choice problem faced by a contractor and a service provider. The service provider receives an uncertain call volume over multiple periods and is considering outsourcing all or part of these calls to a contractor. Each call brings in a fixed revenue to the service provider. Answering calls requires having service capacity; thus implicit in the outsourcing decision is a capacity decision. Insufficient capacity implies that calls cannot be answered, which in turn means there will be a revenue loss. Faced with a choice between a volume-based and a capacity-based contract offered by a contractor that has pricing power, the service provider determines optimal capacity levels. The optimal price and capacity of the contractor together with the optimal capacity of the service provider determine optimal profits of each party under the two contracts being considered. This paper characterizes optimal capacity levels and partially characterizes optimal pricing decisions under each contract. The impact of demand variability and the economic parameters on contract choice are explored through numerical examples. It is shown that no contract type is universally preferred and that operating environments as well as cost-revenue structures have an important effect."
/doi/10.1287/trsc.2017.0751," If shipments have to be transported between many sources and sinks, direct connections from each source to each sink are often too expensive. Instead, a small number of nodes are upgraded to hubs that serve as transshipment points. All sources and sinks are connected to these hubs, so that only a few, strongly consolidated transport relations exist. While hubs and detours lead to additional costs, the savings from bundling shipments—i.e., economies of scale—usually outweigh these costs. Typical applications for hub networks are in cargo, air freight, and postal and parcel transport services. In this paper, we consider three classical and two recent formulations of single allocation hub location problems—i.e., hub location problems in which every source and sink is connected to exactly one hub. Solving larger instances of these problems to optimality is difficult because the inherent quadratic structure of the problem has to be linearized: This leads to a sharp rise in the number of variables. Our new approach relies on the fact that many instances—including the Civil Aeronautics Board and Australian Post data sets—have a Euclidean structure: The distances between the possible hub locations are Euclidean. This enables us to construct a new linearization together with a row generation procedure that solves instances of up to 200 nodes to optimality. For problems like the uncapacitated single allocation p -hub median problem and the uncapacitated single allocation hub location problem, we present the first optimal results for instances of this size."
/doi/10.1287/isre.2017.0703," Recommender systems are an integral part of the online retail environment. Prior research has focused largely on computational approaches to improving recommendation accuracy, and only recently researchers have started to study their behavioral implications and potential side effects. We used three controlled experiments, in the context of purchasing digital songs, to explore the willingness-to-pay judgments of individual consumers after being shown personalized recommendations. In Study 1, we found strong evidence that randomly assigned song recommendations affected participants’ willingness to pay, even when controlling for participants’ preferences and demographics. In Study 2, participants viewed actual system-generated recommendations that were intentionally perturbed (introducing recommendation error), and we observed similar effects. In Study 3, we showed that the influence of personalized recommendations on willingness-to-pay judgments was obtained even when preference uncertainty was reduced through immediate and mandatory song sampling prior to pricing. The results demonstrate the existence of important economic side effects of personalized recommender systems and inform our understanding of how system recommendations can influence our everyday preference judgments. The findings have significant implications for the design and application of recommender systems as well as for online retail practices. The online appendix is available at https://doi.org/10.1287/isre.2017.0703 ."
/doi/10.1287/orsc.1120.0813," A significant gap exists in our understanding of what explains the varying responses of multinational corporations (MNCs) to social issues in emerging markets. Arguably, in a setting where both market institutions and regulations and norms of corporate social responsibility are underdeveloped, it is more difficult for corporations to take actions beyond those that serve their immediate economic interests. Proposing a social movement perspective on MNCs’ responsiveness to social issues in emerging markets, we identify the mechanisms by which online activists grab firms’ attention and force them to become more socially responsive. A perception of organizational vulnerability and a home-country institutional logic that is consistent with the demands of the online campaign provide political opportunity structures that hasten the corporate response but affect the magnitude of firm response differently. We test our framework in the empirical context of corporate philanthropic action following the 2008 earthquake in Sichuan province in China, which triggered an online campaign that questioned MNCs’ donations to the disaster relief effort. Our study contributes to the literature on heterogeneous organizational responses to social movements, a better understanding of the antecedents for MNCs’ social responsiveness in emerging markets, and research on MNCs."
/doi/10.1287/serv.2019.0247," The base-of-the-pyramid (BoP) concept plays a prominent role among the market-based perspectives for poverty alleviation. Previous literature reviews discuss the evolution of the BoP concept as a research domain; however, several major research streams that approach the phenomenon from distinct angles have been overlooked. To address these shortcomings, we continue these reviews and formulate recommendations for theoretical and empirical advancement by considering emerging research domains. We use the problematization method to identify in-house and field assumptions shared across research domains and test them through critical empirical and theoretical interrogation. Therefore, we extend the original BoP concept by considering emerging research domains that have been neglected in earlier studies, including corporate social responsibility, inclusive business, microfinance, nonprofit expansion, social entrepreneurship, and subsistence marketplaces. We also revise the BoP business model idea and develop a framework that highlights key dimensions for management research in BoP markets. These dimensions include business ecosystems, financial viability, innovativeness, resource scarcity, role of the poor, and scalability. For each dimension, we develop future research questions."
/doi/10.1287/mnsc.45.1.25," Drawing on the premise that the diversification decisions are driven by antecedent factors such as a firm’s existing resources (Teece 1982) and industry structural conditions, this paper develops formal hypotheses for reciprocity between the type of diversification and mode of expansion decisions. We consider the specificity of antecedent resources that affect these two decisions and conceptually demonstrate that there is a contradictory tension in trying to optimize the decisions jointly implying that one or both diversification decisions have to be sub-optimized (i.e., there has to be a trade-off). We make a conceptual argument that this sub-optimization is likely to be in the form of subordination of the mode decision subject to constraints imposed by resources that are highly specific to the mode decision. Following this, we empirically investigate this contradictory tension by using a simultaneous equation model (SEM) on a large sample of firms between 1981 and 1989. The results suggest that one antecedent factor—internal funds—act as the key mediating influence in the joint optimization and leads to a subordination of the mode decision in the joint optimization process. However, the existence of time compression economies and market power benefits are the exceptions to this subordination and trade off process."
/doi/10.1287/mksc.2015.0956," User profile is a summary of a consumer’s interests and preferences revealed through the consumer’s online activity. It is a fundamental component of numerous applications in digital marketing. McKinsey & Company view online user profiling as one of the promising opportunities companies should take advantage of to unlock “big data’s” potential. This paper proposes a modeling approach that uncovers individual user profiles from online surfing data and allows online businesses to make profile predictions when limited information is available. The approach is easily parallelized and scales well for processing massive records of user online activity. We demonstrate application of our approach to customer-base analysis and display advertising. Our empirical analysis uncovers easy-to-interpret behavior profiles and describes the distribution of such profiles. Furthermore, it reveals that even for information-rich online firms profile inference that is based solely on their internal data may produce biased results. We find that although search engines cover smaller portions of consumer Web visits than major advertising networks, their data is of higher quality. Thus, even with the smaller information set, search engines can effectively recover consumer behavioral profiles. We also show that temporal limitations imposed on individual-level tracking abilities are likely to have a differential impact across major online businesses, and that our approach is particularly effective for temporally limited data. Using economic simulation we demonstrate potential gains the proposed model may offer a firm if used in individual-level targeting of display ads. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0956 ."
/doi/10.1287/mnsc.1060.0683," The agglomeration of the automobile industry around Detroit, Michigan is explained using a theory in which disagreements lead employees of incumbent firms to found spinoffs in the same industry. Predictions of the theory concerning entry and firm survival are tested using data on the origin, location, and years of production of every entrant into the industry from 1895 to 1966. The geographic concentration of the industry is attributed to four early successful entrants and the many successful spinoffs they spawned in the Detroit area and not to conventional agglomeration economies benefiting co-located firms, as featured in modern theories of agglomeration. Implications of the findings regarding firm strategy are discussed."
/doi/10.1287/opre.1040.0170," Complex systems like semiconductor wafer fabrication facilities (fabs), networks of data switches, and large-scale call centers all demand efficient resource allocation. Deterministic models like linear programs (LP) have been used for capacity planning at both the design and expansion stages of such systems. LP-based planning is critical in setting a medium range or long-term goal for many systems, but it does not translate into a day-to-day operational policy that must deal with discreteness of jobs and the randomness of the processing environment. A stochastic processing network, advanced by J. Michael Harrison (2000, 2002, 2003), is a system that takes inputs of materials of various kinds and uses various processing resources to produce outputs of materials of various kinds. Such a network provides a powerful abstraction of a wide range of real-world systems. It provides high-fidelity stochastic models in diverse economic sectors including manufacturing, service, and information technology. We propose a family of maximum pressure service policies for dynamically allocating service capacities in a stochastic processing network. Under a mild assumption on network structure, we prove that a network operating under a maximum pressure policy achieves maximum throughput predicted by LPs. These policies are semilocal in the sense that each server makes its decision based on the buffer content in its serviceable buffers and their immediately downstream buffers. In particular, their implementation does not use arrival rate information, which is difficult to collect in many applications. We also identify a class of networks for which the nonpreemptive, non-processor-splitting version of a maximum pressure policy is still throughput optimal. Applications to queueing networks with alternate routes and networks of data switches are presented."
/doi/10.1287/orsc.10.2.162," This paper explores the economic processes through which information technology can facilitate coordination within and between firms. The paper presents and analyzes a case study of the B-2 “Stealth” bomber, an aircraft that was designed by four firms almost entirely by computer. The key information systems used in the project were (1) a common-access database to manage part designs and (2) an advanced system to perform structural analysis. These systems played a crucial role in enabling the four firms to coordinate their design and development activities precisely enough to meet the demanding engineering requirements imposed by the aircraft's unique mission. The paper analyses the case study using transaction cost, agency, and information processing theories. The analysis leads to several conclusions about the mechanisms through which the variables emphasized in these theories operated to improve coordination. First, the information systems aided coordination directly by making information processing less costly. Second, this enhanced information processing made the governance of the project more efficient. In particular, by establishing a “technical grammar” for communication, the systems helped to create social conventions around which firms could coordinate their activities, thus limiting the need for a hierarchical authority to promote coordination. This technical grammar also reduced governance costs by reducing asset-specificity, thereby reducing risks associated with contractual holdup. These interactions between communication and governance effects have not been elucidated in the IT/coordination literature. They are important in part because they help explain why the vertically disintegrated organization of the project proved viable. Finally, the systems facilitated decentralized decision-making by reducing agency (measurement) costs. This combination of effects may generalize to other settings in which information technology is used to promote coordination, especially in “virtual” or “disaggregated” corporations."
/doi/10.1287/mnsc.2017.2903," Many jobs are connected to a prosocial mission—namely, to a social purpose beyond profit maximization. I use three laboratory experiments to investigate if employers can use the mission to economize on monetary incentives. In my first experiment, I exogenously vary whether the agents’ effort generates a donation to a charity of their choice (matched mission), generates a donation to a charity chosen by another subject (random mission), or generates no donation (no mission). I find that the mission, whether matched or random, increases effort compared to the no-mission condition. Consistent with the theory, nonmotivated principals exploit the agents’ motivation by offering lower piece rates, whereas very motivated principals pay higher piece rates to boost the donation. I find no difference in the effort and piece rate between the matched- and random-mission conditions. In my second experiment, I use a selected pool of motivated subjects but still observe no difference between these two treatments. In my third experiment, I explore whether the effect of mission-matching may arise through gift exchange: principals decide between choosing the mission or delegating the choice to the agent. I do not find evidence that agents who are delegated the mission choice reciprocate with higher effort. These findings suggest that while a prosocial mission allows for economizing on monetary incentives, there are no further gains from increasing the quality of the mission-matching. Data and the supplemental material are available at https://doi.org/10.1287/mnsc.2017.2903 . This paper was accepted by Uri Gneezy, behavioral economics."
/doi/10.1287/inte.1040.0103," In the current economic climate, many companies have lowered prices in response to their customers' demands and competition while incurring higher costs for labor, benefits, and raw materials. The Lord Corporation relies on productivity improvement from its lean-manufacturing program to mitigate the resulting squeeze on profit. Lean-manufacturing programs have, in fact, become a pervasive tactic to improve productivity for companies in all industries. Kaizen projects are a common component of lean-manufacturing programs whereby process performance is substantially improved by the concentrated effort of a work team over several days. The students of the semester-in-manufacturing (SiM) course at Cornell University's Johnson School were able to learn about kaizen projects by participating hands-on with the Lord Corporation in 2002 and 2003. They learned about continuous-improvement processes in a real-world manufacturing plant and gleaned general insights into organizational issues. The Lord Corporation benefited from the students' fresh perspective and the opportunity to highlight the kaizen projects and its lean-manufacturing program with its employees. The Lord-SiM alliance provides a model for an educational program that bridges the classroom world of education and theory and the real world where that theory must be applied. The alliance also revealed tactics for successfully managing kaizen projects."
/doi/10.1287/mnsc.46.9.1249.12238," This article studies the optimal prices and service quality grades that a queuing system—the “firm”—provides  to heterogeneous, utility-maximizing customers who measure quality by their experienced delay distributions. Results are  threefold: First, delay cost curves are introduced that allow for a flexible description of a customer's  quality sensitivity. Second, a comprehensive executable approach is proposed that analytically specifies scheduling,  delay distributions and prices for arbitrary delay sensitivity curves. The tractability of this approach derives from porting  heavy-traffic Brownian results into the economic analysis. The generalized c μ ( Gc μ) scheduling  rule that emerges is dynamic so that, in general, service grades need not correspond to a static priority ranking. A benchmarking example  investigates the value of differentiated service. Third, the notions of grade and rate incentive  compatibility (IC) are introduced to study this system under asymmetric information and are established for Gc μ  scheduling when service times are homogeneous and customers atomistic. Grade IC induces correct grade choice resulting  in perfect service discrimination; rate IC additionally induces centralized-optimal rates. Dynamic Gc μ  scheduling exhibits negative feedback that, together with time-dependent pricing, can also yield rate incentive compatibility  with heterogeneous service times. Finally, multiplan pricing , which offers all customers a menu with a choice of multiple rate plans, is analyzed."
/doi/10.1287/stsc.2018.0059," Uber provides a two-sided platform that matches travelers with drivers who choose when, where, and how long to drive using their own vehicles. Uber disrupts local markets with an incumbent taxi industry, and the industry fights back in both its market and institutional environments. This paper focuses on Uber’s strategy for addressing the challenges from the taxi industry with an emphasis on dual-purpose strategies that provide in a complementary manner benefits both in the marketplace and in the institutional environment. The focus is on three issues important to Uber’s success. The first is whether Uber is a platform company or a transportation service that falls under the jurisdiction of local regulators. The second is whether Uber drivers are classified under the law as independent contractors or as employees. The third is passenger safety and the qualifications required for ride-hailing drivers. The corresponding dual-purpose strategies considered are market engagement, work enhancement, and accommodation. Implementing these dual-purpose strategies involves lobbying, stakeholder mobilization, and institutional venue shifting. A model of a local transportation market is presented as a framework for analyzing market competition and for identifying the incentives of market and institutional participants, including Uber, its drivers and passengers, taxi companies and their drivers, and safety advocates, to challenge or support Uber and ride-hailing. The model distinguishes between full-time and part-time drivers, frequent and infrequent travelers, and whether taxi drivers or taxi companies own the taxi medallions."
/doi/10.1287/trsc.2014.0522," There has been extensive research on the optimal design of road congestion pricing schemes based on traffic network equilibrium models. Since travel demands are closely related to the locations of workplace and residence, it is of both theoretical and practical importance to extend the optimization approach to treat location choices as well as travel route choices. This paper addresses the problem of optimal location of housing supply and transportation network pricing in an integrated location and transportation model with heterogeneous households that have different values of travel time. In the model, housing exhibits external economies and diseconomies of scale, depending on the population density in residential zones. It is shown that in such a model the marginal cost pricing principle applied to internalize these externalities in general results in local optima of social surplus. Optimization algorithms based on sensitivity analysis are proposed to find combinations of housing supply patterns and road tolls that optimize social surplus under various conditions. A numerical example is given to illustrate the proposed method. The potential policy implications of the optimization approach are examined and compared with that of the marginal cost pricing principle."
/doi/10.1287/mnsc.17.8.B505," A decomposition model is developed which can be interpreted as a representation of decision-making in a three-level hierarchical organization. The central unit coordinates decision activity by generating goals for a subordinate level of management units. These latter are responsible for generating the economic indicators that (1) allow the central unit to evaluate its goal generating policies, and (2) guide the alternative generation activities of third level operating units. Because of the absence of a global objective function for the organization the decision activity is shown to be dependent on the structure of the organization (i.e., the solutions of the model are affected by the structure and degree of decomposition). Thus, unlike existing decomposition models, the model treats organization structure as a variable in the decision-making process. This feature of the model is shown to have implications for organizational design."
