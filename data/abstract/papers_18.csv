link,abstract
/doi/10.1287/inte.28.3.34," To examine the costs and consequences of prophylaxis against cytomegalovirus (CMV) infection, we developed a compartmental model of the natural history of late-stage HIV disease. We used data on the progression of illness, economic costs, the incidence of infections, the efficacy and toxicity of therapy, and patient quality of life from national cohort studies, randomized clinical trials, and resource-utilization surveys. We found that CMV prophylaxis confers additional quality-adjusted life-years at a lower-bound, marginal cost of $160,000. While this cost-effectiveness result compares unfavorably with alternative uses of scarce resources, it is sensitive to assumptions regarding the price of therapy and the incidence of infection."
/doi/10.1287/msom.2017.0644," Carsharing has been considered as an effective means to increase mobility and reduce personal vehicle usage and related carbon emissions. In this paper, we consider problems of allocating a carshare fleet to service zones under uncertain one-way and round-trip rental demand. We employ a two-stage stochastic integer programming model, in the first stage of which we allocate shared vehicle fleet and purchase parking lots or permits in reservation-based or free-floating systems. In the second stage, we generate a finite set of samples to represent demand uncertainty and construct a spatial–temporal network for each sample to model vehicle movement and the corresponding rental revenue, operating cost, and penalties from unserved demand. We minimize the expected total costs minus profit and develop branch-and-cut algorithms with mixed-integer, rounding-enhanced Benders cuts, which can significantly improve computation efficiency when implemented in parallel computing. We apply our model to a data set of Zipcar in the Boston–Cambridge, Massachusetts, area to demonstrate the efficacy of our approaches and draw insights on carshare management. Our results show that exogenously given one-way demand can increase carshare profitability under given one-way and round-trip price differences and vehicle relocation cost whereas endogenously generated one-way demand as a result of pricing and strategic customer behavior may decrease carshare profitability. Our model can also be applied in a rolling-horizon framework to deliver optimized vehicle relocation decisions and achieve significant improvement over an intuitive fleet-rebalancing policy. The online appendix is available at https://doi.org/10.1287/msom.2017.0644 ."
/doi/10.1287/mksc.2019.1159," This paper presents a methodology for identifying groups of products that exhibit similar patterns in demand and responsiveness to changes in price using store-level sales data. We use the concept of economic separability as the basis for establishing similarity between products and build a weakly separable model of aggregate demand. A common issue with separable demand models is that the partition of products into separable groups must be known a priori, which severely shrinks the set of admissible substitution patterns. We develop a methodology that allows the partition to be an estimated model parameter. In particular, we specify a log-linear demand system in which weak separability induces equality restrictions on a subset of cross-price elasticity parameters. An advantage of our approach is that we are able to find groups of separable products rather than just test whether a given set of groups is separable. Our method is applied to two aggregate, store-level data sets. We find evidence that the separable structure of demand can be inconsistent with category labels, which has implications for optimal category marketing strategies."
/doi/10.1287/ijoo.2019.0048," We first propose a semi-proximal augmented Lagrangian-based decomposition method to directly solve the primal form of a convex composite quadratic conic-programming problem with a primal block-angular structure. Using our algorithmic framework, we are able to naturally derive several well-known augmented Lagrangian-based decomposition methods for stochastic programming, such as the diagonal quadratic approximation method of Mulvey and Ruszczyński. Although it is natural to develop an augmented Lagrangian decomposition algorithm based on the primal problem, here, we demonstrate that it is, in fact, numerically more economical to solve the dual problem by an appropriately designed decomposition algorithm. In particular, we propose a semi-proximal symmetric Gauss–Seidel-based alternating direction method of multipliers (sGS-ADMM) for solving the corresponding dual problem. Numerical results show that our dual-based sGS-ADMM algorithm can very efficiently solve some very large instances of primal block-angular convex quadratic-programming problems. For example, one instance with more than 300,000 linear constraints and 12.5 million nonnegative variables is solved to the accuracy of 10 -5 in the relative KKT residual in less than a minute on a modest desktop computer."
/doi/10.1287/opre.2019.1954," A matching in a two-sided market often incurs an externality: a matched resource may become unavailable to the other side of the market, at least for a while. This is especially an issue in online platforms involving human experts, as the expert resources are often scarce. The efficient utilization of experts in these platforms is made challenging by the fact that the information available about the parties involved is usually limited. To address this challenge, we develop a model of a task-expert matching system where a task is matched to an expert using not only the prior information about the task but also the feedback obtained from the past matches. In our model, the tasks arrive online while the experts are fixed and constrained by a finite service capacity. For this model, we characterize the maximum task resolution throughput a platform can achieve. We show that the natural greedy approach where each expert is assigned a task most suitable to his or her skill is suboptimal, as it does not internalize the aforementioned externality. We develop a throughput-optimal backpressure algorithm which does so by accounting for the “congestion” among different task types. Finally, we validate our model and confirm our theoretical findings with data-driven simulations via logs of Math.StackExchange.com, a Stack Overflow forum dedicated to mathematics."
/doi/10.1287/isre.2017.0768," Word of mouth (WOM) plays an increasingly important role in shaping consumers’ behavior and preferences. In this paper, we examine whether latent personality traits of online users accentuate or attenuate the effectiveness of WOM in social media platforms. To answer this question, we leverage machine-learning methods in combination with econometric techniques utilizing a novel quasi-experiment. Our analysis yields two main results. First, there is a positive and statistically significant effect of the level of personality similarity between two social media users on the likelihood of a subsequent purchase from a recipient of a WOM message after exposure to the WOM message of the sender. In particular, exposure to WOM messages from similar users in terms of personality, rather than dissimilar users, increases the likelihood of a postpurchase by 47.58%. Second, there are statistically significant effects of specific pairwise combinations of personality characteristics of senders and recipients of WOM messages on the effectiveness of WOM. For instance, introverted users are responsive to WOM, in contrast to extroverted users. Besides this, agreeable, conscientious, and open social media users are more effective disseminators of WOM. In addition, WOM originating from users with low levels of emotional range affects similar users, whereas for high levels of emotional range, increased similarity usually has the opposite effect. The examined effects are also of significant economic importance, as, for instance, a WOM message from an extrovert user to an introvert peer increases the likelihood of a subsequent purchase by 71.28%. Our findings are robust to several alternative methods and specifications, such as controlling for latent user homophily and network structure roles based on deep-learning models. By extending the characteristics that have been theorized to affect the effectiveness of WOM from the observable to the latent space, tapping into users’ latent personality characteristics, and illustrating how companies can leverage the abundance of unstructured data in social media, our paper provides actionable insights regarding the future potential of social media advertising and advanced microtargeting based on big data and deep learning. The online appendix is available at https://doi.org/10.1287/isre.2017.0768 ."
/doi/10.1287/msom.2017.0616," Phase III clinical trials are expensive and require enrolling and treating hundreds or thousands of patients at many sites. The time and cost required to do so are uncertain, as is the economic value of the drug upon completion. We consider the problem of determining when and how many test sites should be opened and the rate at which patients should be recruited. We model the problem as a discrete time, discounted dynamic program with the objective of maximizing the expected net present value of a drug based on the costs of conducting the trial and on the drug’s quality-moderated likelihood of approval and its subsequent expected revenue stream if approved. We show the optimal policy is characterized by a series of thresholds on the number of patients enrolled over time that indicate when additional test centers should be opened and how many patients should be targeted. We demonstrate using data from completed clinical trials that for low- to moderate-valued drugs, these thresholds are relevant to the firm’s decisions. We extend the problem to the case with multiple interim analyses and demonstrate that optimizing the clinical trial capacity and its utilization provides significant value in addition to the option value of stopping the trial early. The e-companion is available at https://doi.org/10.1287/msom.2017.0616 ."
/doi/10.1287/mksc.21.3.318.139," The literature on cross-national diffusion models is gaining increased importance today due to the needs of present day managers. New product sales growth in a given nation or society is affected by many factors (Rogers 1995), and of these, sociocontagion (or word of mouth) has been found to be the most important factor that characterizes the diffusion process (Bass 1969, Moore 1995). Hence, it is interesting and perhaps challenging to analyze what would happen if a new product diffuses in parallel in two neighboring but culturally different countries. Not only will we expect the diffusion process in the two countries to be different, but we will also expect some interaction among them, especially if the two societies mingle with each other. There are two streams of research in cross-national diffusion. The first type focuses on exploring the differences between diffusion processes in two countries and finding out whether those differences can be attributed to social and cultural differences between the countries involved. Examples of this type of research are found in Takada and Jain (1991), Gatignon et al. (1989), Helsen et al. (1993), and Kumar et al. (1998). These studies did find some relationship between the cultural differences of the countries studied and the differences in the diffusion process. The second stream of research focuses on modeling explicitly the interaction between the diffusion processes in two countries. The interaction is typically captured through lead-lag effect (Eliashberg and Helsen 1996, Kalish et al. 1995), where the sales process in the lead country (i.e., the country where the product was first introduced) is modeled to affect the sales process in the lag country (i.e., the country where the product was introduced a few years later). Another method to study the interaction among the diffusion processes in two countries was suggested by Putsis et al. (1997), who used a “mixing model” to empirically explore the existence of such interactions. These studies basically observed that, when a new product is introduced early in one country and with a time lag in subsequent countries, the consumers in the lag countries learn about the product from the lead country adopters, resulting in a faster diffusion rate in the lag countries. Ganesh and Kumar (1996) formulized this effect as the learning effect and, subsequently, Ganesh et al. (1997) found this learning effect to be influenced by country-specific factors (cultural similarity, economic similarity, and time lag elapsed between the lead and the lag countries) and product-specific factors (continuous vs. discontinuous innovation and the presence or absence of a standardized technology). A careful analysis of the extant literature on the second stream of research would reveal that neither the learning effect model nor the mixing model can be modified to accommodate the other model. Our contribution to the literature exactly addresses this point. In this paper, an alternative framework is proposed that has two unique features. First, the framework is flexible enough to not only account for the lead country affecting the lag countries and vice versa, but also to accommodate the simultaneous interaction among countries in explaining the diffusion processes in the countries concerned. Using multiple product categories and a variety of new product introduction situations, we empirically demonstrate the flexibility and efficiency of our proposed framework. We found strong evidence of all types of interactions, namely, lead lag, lag lead, and simultaneous, which evidence suggests that one cannot afford to omit any of the interactions. The second unique feature of our paper is the estimation procedure that we used. Because statistical estimation of a dynamic process that includes lead-lag, lag-lead, and simultaneous types of causality within a single framework is not straightforward, we suggest an iterative estimation procedure for the estimation. This new procedure not only proved to be flexible in accommodating different types of interaction, but also converged rather quickly in all of the cases that we empirically tested. Noting that the statistical properties of these estimators are not generally available, we carried out a simulation exercise that clearly revealed the efficiency of the proposed estimation procedure. After analyzing the interaction, we went further and showed that the magnitude of the cross-national influences is affected by certain country-specific and product-specific factors. The flexibility of the proposed method over the existing methods is demonstrated through obtaining superior forecasts with the proposed method. Several interesting insights for managers concerned with formulating international marketing strategies are offered."
/doi/10.1287/mksc.2015.0948," Promotions are used in marketing to increase sales and drive profits by temporarily decreasing the price per unit of a good. Some price promotions apply to all quantities (20% off), some have limits on the number of units that can be purchased at a reduced price, and others only offer the discount if the volume purchased is sufficiently high. We develop a model of price promotions in the context of a direct utility model where its effects are incorporated through the budget constraint. Price promotions complicate the estimation and analysis of direct utility models because they induce kinks and points of discontinuity in the budget set. We propose a Bayesian approach to addressing these irregularities and demonstrate the ability of the direct utility model to be used in counterfactual analyses of price promotions. We investigate the stability of utility function estimates for consumers under alternative price promotions, and find that the majority of the effect of a price promotion is through the budget set, not through changes in the utility function. We also investigate the economic value of customized price promotions where the customization includes the value and format of the offer. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0948 ."
/doi/10.1287/mnsc.2015.2351," Working capital is an important indicator of firm operational efficiency. All else being equal, lower levels signal greater efficiency. Managers are thus likely to be motivated to report lower levels of working capital at times of greater external attention. We find that working capital levels decrease in the fourth fiscal quarter significantly more than expected, conditional on seasonal changes in economic activity. The decrease subsequently reverses in the following first fiscal quarter. Evidence indicates that firms manage down year-end working capital through transactions that increase year-end operating cash flow and that firms spread this activity over all working capital accounts. Finally, the temporary decrease in year-end working capital is correlated with compensation benchmarks and analysts’ annual cash flow forecasts. The temporary drop is also more pronounced for firms with industry dominance. This paper was accepted by Mary Barth, accounting ."
/doi/10.1287/mnsc.2016.2442," We develop a model in which the parties to a joint production project have a choice of specifying contractual performance in terms of actions or deliverables. Penalties for noncompliance are not specified; rather, they are left to the courts under the legal doctrine of compensatory damages. We analyze three scenarios of increasing uncertainty: full information , where implications of partner actions are known; risk , where implications can be probabilistically quantified; and ambiguity , where implications cannot be so quantified. Under full information, action requirements dominate: they always induce the maximum economic value. This dominance vanishes in the risk scenario. Under ambiguity, deliverables specifications can interact with compensatory damages to create a form of “ambiguity insurance,” where ambiguity aversion is assuaged in a way that increases the aggregate, perceived value of the project. This effect does not arise under contracts specifying action requirements. Thus, deliverables contracts may facilitate highly novel joint projects that would otherwise be foregone as a result of excessive uncertainty. Suggested empirical implications include the choice of contract clause type depending on the level of uncertainty in a joint development project, one application being the level of partner experience with interfirm collaborations. This paper was accepted by Bruno Cassiman, business strategy ."
/doi/10.1287/orsc.14.2.209.14992," Currently, two models of innovation are prevalent in organization science. The “private investment” model assumes returns to the innovator result from private goods and efficient regimes of intellectual property protection. The “collective action” model assumes that under conditions of market failure, innovators collaborate in order to produce a public good. The phenomenon of open source software development shows that users program to solve their own as well as shared technical problems, and freely reveal their innovations without appropriating private returns from selling the software. In this paper, we propose that open source software development is an exemplar of a compound “private-collective” model of innovation that contains elements of both the private investment and the collective action models and can offer society the “best of both worlds” under many conditions. We describe a new set of research questions this model raises for scholars in organization science. We offer some details regarding the types of data available for open source projects in order to ease access for researchers who are unfamiliar with these, and also offer some advice on conducting empirical studies on open source software development processes."
/doi/10.1287/inte.1070.0318," Since 2005, Chile's professional soccer league has used a game-scheduling system that is based on an integer linear programming model. The Chilean league managers considered several operational, economic, and sporting criteria for the final tournaments' scheduling. Thus, they created a highly constrained problem that had been, in practice, unsolvable using their previous methodology. This led to the adoption of a model that used some techniques that were new in soccer-league sports scheduling. The schedules they generated provided the teams with benefits such as lower costs, higher incomes, and fairer seasons. In addition, the tournaments were more attractive to sports fans. The success of the new scheduling system has completely fulfilled the expectations of the Asociación Nacional de Fútbol Profesional (ANFP), the organization for Chilean professional soccer."
/doi/10.1287/mnsc.2015.2199," Whereas early location work established the role of economic factors in entrepreneurs’ location choices, recent studies suggest personal reasons as another factor. However, because the places where entrepreneurs want to live are often the places where they will likely do well, it is difficult to empirically separate the two mechanisms. We focus on entrepreneurs founding firms abroad, allowing us to more effectively isolate the effect of personal location attractiveness. We leverage entrepreneurs’ decisions to relocate and manage their firms personally or to remain in a home country and hire a manager. We find that entrepreneurs who view a host country as an attractive location are more likely to relocate and manage their firms personally. However, such entrepreneur-managers have lower firm performance. These results are consistent with the idea that entrepreneurs are more likely to reside in personally attractive places and are willing to substitute benefits of living there for some firm profit. This paper was accepted by Jesper Sørensen, organizations ."
/doi/10.1287/isre.12.1.34.9716," In this paper the author describes a formal language for communication based on linguistics—more specifically, a theory of natural language communication and models of natural language conversations. The language has a small number of general message types that are formally defined by their intended effects on the recipient. For each message type he defines a standard automated method of responding that depends only on the message type and is independent of the message's content. For more complex conversations he provides methods for responding that do depend on the content. In this system, a message's sender—automated or human—constructs and sends a message knowing that he cannot know, but can only predict, how it will be interpreted. The agent receiving the message interprets it and then uses it as a basis for inferring how he should respond. The message interpretation mechanism for this language is reusable, modular, and shared by all applications. The benefit of this communication system is that it makes the communication infrastructure more flexible, easier to modify, easier to expand, and more capable."
/doi/10.1287/serv.2020.0267," Amid population aging, the senior living industry has become an important sector of the service industry, especially in China. Population control throughout the past four decades has led to the quintessential “sandwich” family, wherein a young couple faces pressure to care for four elderly parents as well as their own child(ren). A lack of proper home care for older adults in China has thus accelerated social and economic demand for senior living institutions (SLIs). Although a growing number of SLIs are being built in the country, occupancy rates remain low. It is therefore crucial to explore why older adults are not accepting SLIs as a solution to their needs. This study broaches this topic by investigating the mechanism underlying the impact of family support for choosing an SLI on older adults’ attitudes toward these institutions. Specifically, a mediation model was proposed to investigate the roles of life satisfaction and older adults’ subjective knowledge of SLIs among 1,723 older Chinese adults. Multigroup path analysis was also carried out to test applicable sociodemographic differences. Findings revealed a significant suppressing effect of life satisfaction and a partial mediating effect of subjective knowledge along with group differences. Overall, this study provides meaningful implications for SLI marketing."
/doi/10.1287/mksc.17.1.29," Product design, pricing policies, and promotional activities influence the primary and secondary demand for goods and services. Brand managers need to develop an understanding of the relationships between marketing mix decisions and consumer decisions of whether to purchase in the product category, which brand to buy, and how much to consume. Knowledge about factors most effective in influencing primary and secondary demand of a product allows firms to grow by enhancing their market share as well their market size. The purpose of this paper is to develop an individual level model that allows an investigation of both the primary and secondary aspects of consumer demand. Unlike models of only primary demand or only secondary demand, this more comprehensive model offers the opportunity to identify changes in product features that will result in the greatest increase in demand. It also offers the opportunity to differentially target consumer segments depending upon whether consumers are most likely to enter the market, increase their consumption level, or switch brands. In the proposed hierarchical Bayes model, an integrative framework that jointly models the discrete choice and continuous quantity components of consumer decision is employed instead of treating the two as independent. The model includes parameters that capture individual specific reservation value, attribute preference, and expenditure sensitivity. The model development is based upon the microeconomic theory of utility maximization. Heterogeneity in model parameters across the sample is captured by using a random effects specification guided by the underlying microeconomic model. This requires that some of the effects are strictly positive. This is accommodated through the use of a gamma distribution of heterogeneity for some of the parameters. A normal distribution of heterogeneity is used for the remaining parameters. Gibbs sampling is used to estimate the model. The key methodological contribution of this paper is that we show how to specify a hierarchical Bayes continuous random effects model that integrates consumer choice and quantity decisions such that individual-level parameters can be estimated. Individual level estimates are desirable because insights into primary demand involve nonlinear functions of model parameters. For example, consumers not in the market are those whose utilities for the choice alternatives fall below some reservation value. The proposed methodology yields individual specific estimates of reservation values and expenditure sensitivity, which allow assessment of the origins of demand other than the switching behavior of consumers. The methodology can also be used to help identify changes in product features most likely to bring new customers into a market. Our work differs from previous research in this area as we lay the framework needed to obtain individual-level parameter estimates in a continuous random effects model that integrates choice and quantity. The methodology is demonstrated with survey data collected about consumer preferences and consumption for a food item. For the data available, a large response heterogeneity was observed across all model parameters. In spite of limited data available at the individual level, a majority of the individual level estimates were found to be significant. Predictive tests demonstrated the superiority of the proposed model over existing latent class and aggregate models. Particularly, significant gains in predictive accuracy were observed for the “no-buy” behavior of the respondents. These gains demonstrate that by structurally linking the choice and quantity models results in a more accurate characterization of the market than existing finite mixture approaches that model choice and quantity independently. We show that our joint model makes more efficient use of the available data and results in better parameter estimates than those that assume independence. Finally, the individual level demand analysis is illustrated through a simple example involving a $1.00 price cut. We demonstrate practical usefulness of the model for targeting by developing the demographic, attitudinal, and behavioral profiles of consumer groups most likely to increase consumption, enter the market, or switch brands because of a price cut decision."
/doi/10.1287/opre.1110.0971," We formulate and solve the problem of making advance energy commitments for wind farms in the presence of a storage device with conversion losses, mean-reverting price process, and an autoregressive energy generation process from wind. We derive an optimal commitment policy under the assumption that wind energy is uniformly distributed. Then, the stationary distribution of the storage level corresponding to the optimal policy is obtained, from which the economic value of the storage as the relative increase in the expected revenue due to the existence of storage is obtained."
/doi/10.1287/mnsc.2018.3243," Celebrity endorsers can cause negative publicity that can spill over to the endorsed brand. However, little is known about the economic effects of firm reactions to these events. This study fills this gap and estimates how announcements of firms’ reactions (yes versus no), timing (slow versus fast), and type (maintain/suspend versus no reaction) affect daily abnormal stock returns (ARs) following negative publicity. Using 128 events of negative endorser publicity between 1988 and 2016 affecting firms in 230 cases, this study offers new and economically relevant insights. The most surprising finding is that firms can gain value depending on their response. Announcements of firms’ reactions positively affect ARs, especially if they occur quickly after negative publicity surfaces. The analyses reveal that fast (slow) announcements of firms’ reactions increase (decrease) firm value by 2.10% (−1.88%) over the next four trading weeks. Results also show that issuing statements suspending or maintaining the endorser both yield more positive ARs than not reacting at all. Further analyses identify conditions under which the stock market rewards maintaining or suspending an endorser. Firms have more positive ARs when they (1) suspend higher-blame endorsers, (2) suspend endorsers whose negative publicity is related to their occupation, (3) maintain endorsers with a high product fit, and (4) do not suspend apologetic endorsers. This study discusses implications for theory and practice and provides a strong empirical foundation for understanding the consequences of firm reaction announcements to negative celebrity endorser publicity. This paper was accepted by Juanjuan Zhang, marketing."
/doi/10.1287/ijoc.2015.0652," We apply virtual machine abstractions to networked vehicles, enabling what we call cloud computing in space to create performance isolation between customers. In analogy to conventional system virtualization and cloud computing, there are customer-operated virtual vehicles that essentially perform like real vehicles, although they are in reality hosted by fewer, shared, provider-operated real vehicles. The motion of the virtual vehicles and real vehicles creates migration gain. As a result, cloud computing in space can do better than conventional cloud computing in the sense of realizing high performance isolation (e.g., 98%) while requiring significantly fewer real vehicles (e.g., approximately one for five). There is a video associated with this paper. Click here to view the Video Overview . To save the file, right click and choose “Save Link As” from the menu."
/doi/10.1287/mnsc.1040.0227," This paper develops a new likelihood-based method for the simultaneous estimation of structural demand-and-supply models for markets with differentiated products. We specify an individual-level discrete-choice model of demand and derive the supply side assuming manufacturers compete in prices. The proposed estimation method considers price endogeneity through simultaneous estimation of demand and supply, allows for consumer heterogeneity, and incorporates a pricing rule consistent with economic theory. The basic idea behind the proposed estimation procedure is to simulate prices and choice probabilities by solving for the market equilibrium. By repeating this many times, we obtain an empirical distribution of equilibrium prices and probabilities. The empirical distribution is then smoothed and used in a likelihood procedure to estimate the parameters of the model. The advantage of this method is that it avoids the need to perform a transformation of variables. If the tastes of consumers are independent across market periods, our approach yields maximum likelihood estimates; otherwise, it yields consistent but not fully efficient partial likelihood estimates."
/doi/10.1287/mnsc.1110.1410," Apologies are part of a social institution designed to restore frayed relationships not only in daily life but also in the domains of corporate governance, medical malpractice litigation, political reputation, organizational culture, etc. The theory shows that in a general class of moral hazard games with imperfect information about agents with two-dimensional type, apologies exhibit regular properties—e.g., apologies are more frequent in long relationships, early in relationships, and between better-matched partners. A variant of the trust game demonstrates that communication matters in a manner consistent with economic theory; specifically, the words “I am sorry” appear to select equilibrium behavior consistent with the theory's main predictions. This paper was accepted by Brad Barber, Teck Ho, and Terrance Odean, special issue editors."
/doi/10.1287/mnsc.2020.3906," This study examines the effects of jurisdictions’ corporate taxes and other policies on firms’ headquarters (HQ) location decisions. Using changes in state corporate income tax rates across time and states as the setting, we find that a one-percentage-point increase in the HQ state corporate income tax rate increases the likelihood of firms relocating their HQ out of the state by 16.8%, and an equivalent decrease in the HQ state rate decreases the likelihood of HQ relocations by 9.1%. Exploiting the unique tax policy features within the state apportionment system lends strong support to the interpretation that taxation drives this effect. Our analyses also demonstrate that state income tax features affect the destination of the HQ move. We contribute to the literature on corporate decision making by showing how state income taxation affects a real corporate decision that has significant economic consequences for the company and the state. This paper was accepted by Brian Bushee, accounting."
/doi/10.1287/mksc.2.1.19," The paper gives an overview and evaluates the theoretical traditions underlying choice models that are used in marketing. In particular, the emphasis of this essay is on the underlying assumptions, limitations and empirical demands of these choice models. Four fundamental choice models are used as a basis to analyze this complex field; the neoclassical economic theory as extended by Lancaster, the Risk-Preference Theory of Choices under uncertainty, the Strict Utility Theory and the Random Utility Theory. These four choice models and their extensions are compared and contrasted along the three critical steps in the model building process: theory generation, parameterization and estimation."
/doi/10.1287/opre.2018.1751," Food rescue—the collection of perishable products from food suppliers who are willing to make donations, and their distribution to welfare agencies that serve individuals in need—has become increasingly widespread in recent years. This phenomenon is a result of economic crises, but it is also encouraged by the tax and good image it provides to donor companies. The problem we study in this paper focuses on the logistic challenges of a food bank that on a daily basis uses vehicles of limited capacity to distribute food collected from suppliers in the food industry to welfare agencies, under an imposed maximal traveling time. We model this problem as a routing resource allocation problem, with the aim of maintaining equitable allocations to the different agencies while delivering overall as much food as possible. We introduce an innovative objective function that satisfies desired properties of the allocation, that is easy to compute and implement within a mathematical formulation, and that balances effectiveness and equity acceptably. We present both an exact solution method and a heuristic approach, based on the large neighborhood search framework, which relies on the fact that a certain subproblem is easy to solve. Numerical experiments on several real-life and randomly generated data sets confirm that high-quality solutions may be obtained. The online appendices are available at https://doi.org/10.1287/opre.2018.1751 ."
/doi/10.1287/trsc.1080.0250," Designing multimodal freight transport networks can facilitate the economic development of regions and countries as well as help to reduce negative environmental impacts. It is therefore crucial that such be undertaken in areas where more priority is given on road-based freight transport systems. This paper proposes a model for strategic transport planning, particularly in freight terminal development and interregional freight transport network design. The model determines a suitable set of actions from a number of possible actions, such as improving the existing infrastructure or establishing new roads, railways, sea links, and freight terminals. Modelling is undertaken within the framework of bilevel programming, where a multimodal multiclass user traffic assignment technique is incorporated within the lower-level problem, whilst the upper-level problem determines the best combination of actions such that the freight-related benefit-cost ratio is maximised. The upper-level problem involves combinatorial optimisation, and a heuristic approach based on genetic local search is applied as a solution technique. Empirical results of the model as applied to an actual large-sized interregional intermodal freight transport network show that genetic local search could provide better performance as compared to other genetic algorithm-based, as well as tabu search-based, heuristics. The model is successfully applied to transport network planning in the Philippines, where the development of a freight transport network is necessary to increase the utilisation of other modes rather than road-based vehicles."
/doi/10.1287/mksc.16.1.60," Every Day Low Pricing (EDLP) strategy has proved to be a successful innovation resulting in higher profits to supermarkets adopting it in competition with Promotional Pricing (PROMO). Conventional wisdom attributes this success either to lower costs or to EDLP better serving time constrained consumers, while discouraging cherry pickers who seek promotions. However, it is unclear that such cost savings are being fully realized since EDLP stores also engage in price promotions. Also, continued existence of PROMO stores means that costs are not the only factor, and they compete effectively without relying just on the cherry pickers. Furthermore, experimental evidence suggests that a supermarket cannot obtain higher profits by merely setting constant low prices, leading to the question: exactly what makes EDLP successful? This question is of particular relevance to both academics and practitioners who have been intrigued by the success of this retailing strategy. More generally, the retailing issues addressed in this paper, the economic analysis of competition, and the empirical findings should be of interest to the broader community of researchers and managers. We investigate the factors contributing to EDLP's success by analyzing the competition between supermarkets through a game theoretic analysis of a market consisting of both time constrained consumers and cherry pickers. Key features of our model are: consumers shop and purchase a basket of goods based on price announcements by stores and rational expectations of unannounced prices; stores carry more than one good and compete through prices, service, convenience, and appropriate communication strategies; and no exogenous cost asymmetries. We derive the conditions under which retailers choosing different strategies (EDLP and PROMO) is a perfect Nash equilibrium. Our analysis shows that the EDLP store's offering of constant every day low prices is an equilibrium outcome, endogenously determined. Successful implementation of the EDLP strategy involves communication of relative basket prices, implying that merely setting constant low prices is not viable. We further demonstrate that while time constrained consumers find every day low prices at EDLP attractive and cherry pickers the promotions at PROMO, clientele effects are in fact more complicated. Specifically, in equilibrium the PROMO store offers a higher service level as desired by time constrained consumers and the EDLP store a lower service level in keeping with the needs of cherry pickers. This choice of service by the two stores results in a cleaner segmentation of the market. The higher relative basket price and service at the PROMO store results in a larger base of time constrained consumers to shop at the PROMO store and a larger base of cherry pickers to shop at the EDLP store, even though some cherry pickers continue to visit the PROMO store to avail of the price specials. In this way, our results contradict the conventional wisdom on EDLP strategy as being mainly geared towards time constrained consumers. Finally, industry profits are higher in an EDLP-PROMO equilibrium than when stores adopt identical strategies. Our analysis and results also offer a more complete characterization of the EDLP and PROMO strategies. Indeed, we show that EDLP and PROMO strategies are positioning strategies, rather than merely pricing strategies, with different elements: price/promotions, service, and communications. While the EDLP store uses basket prices to attract both segments, the PROMO store uses service and price specials to compete in the time constrained and cherry picking segment, respectively. Given these different approaches of the two stores, the communication strategies of the EDLP and the PROMO stores emphasize these differences as well. In this way we show, as suggested by Corstjens and Corstjens (1994), that positioning in a retail context involves developing multidimensional strategies appealing to all segments, while each element of the strategy may focus on a different consumer segment. This is in contrast to the traditional view of segmentation in which different products in a product line, for example, are designed to appeal to different segments. We complete the analysis by examining the data from the trade press and a survey conducted in a major metropolitan area. These data, while limited in scope, support our theoretical results."
/doi/10.1287/mnsc.1060.0622," Empirical evidence on the distributional characteristics of common stock returns indicates: (1) A power-law tail index close to three describes the behavior of the positive tail of the survivor function of returns (pr( r > x ) ∼ x −α ), a reflection of fat tails; (2) general linear and nonlinear dependencies exist in the time series of returns; (3) the time-series return process is characterized by short-run dependence (short memory) in both returns as well as their volatility, the latter usually characterized in the form of autoregressive conditional heteroskedasticity; and (4) the time-series return process probably does not exhibit long memory, but the squared returns process does exhibit long memory. We propose a model of complex, self-referential learning and reasoning amongst economic agents that jointly produces security returns consistent with these general observed facts and which are supported here by empirical results presented for a benchmark sample of 50 stocks traded on the New York Stock Exchange. The market we postulate is populated by traders who reason inductively while compressing information into a few fuzzy notions that they can in turn process and analyze with fuzzy logic. We analyze the implications of such behavior for the returns on risky securities within the context of an artificial stock market model. Dynamic simulation experiments of the market are conducted, from which market-clearing prices emerge, allowing us to then compute realized returns. We test the effects of varying values of the parameters of the model on the character of the simulated returns. The results indicate that the model proposed in this paper can jointly account for the presence of a power-law characterization of the positive tail of the survivor function of returns with exponent on the order of three, for autoregressive conditional heteroskedasticity, for long memory in volatility, and for general nonlinear dependencies in returns."
/doi/10.1287/mnsc.1040.0229," This paper reviews the significant progress in “agency theory” (i.e., the economic theory of incentives) during the 1990s, with an eye toward applications to supply transactions. I emphasize six recent models, in three pairs: (1) new foundations for the theory of incentive contracts, (2) new directions in incentive theory, and (3) new applications to supply transactions. By reviewing these six models, I hope to establish three things. First, the theory of incentive contracts needed and received new foundations. Second, new directions in incentive theory teach us that incentive contracts are not the only source of incentives. Finally (and especially relevant to supply transactions), the integration decision is an instrument in the incentive problem."
/doi/10.1287/inte.30.3.215.11660," Walden Paddlers, a market leader in popularly priced recreational kayaks, used a simple, inexpensive guide to evaluate decisions against a parameter of environmental responsibility. This guide was applied across Walden's virtual corporation structure to yield quick innovations and economic, strategic, and environmental advantages. The guide enabled Walden and its network of partners to produce light, strong, inexpensive kayaks with superior performance characteristics made from 100-percent-recycled plastic, the only such kayaks on the market. Walden also employed the guide to create a nearly waste-free product-packaging-and-shipping system. Walden's decision guide succeeded within a network of collaborative alliances because it was applied comprehensively, it helped to clarify common goals, it created benefits for all participants, and it was implemented by a skilled entrepreneur-leader who coordinated decision making in the network."
/doi/10.1287/isre.2017.0726," In the software industry, commercial open-source software vendors have recognized that providing services to help businesses derive greater value in the implementation of open source–based systems can be a profitable business model. Moreover, society may greatly benefit when software originators choose an open-source development strategy as their products become widely available, readily customizable, and open to community contributions. In this study, we present an economic model to study how software licensing attributes affect a software originator’s decisions, aiming to provide policy makers with insights into how welfare-improving, open-source outcomes can be incentivized. We show that when a competing contributor is apt at reaping the benefits of software development investment, a less restrictive open source license (e.g., Berkeley Software Distribution, or BSD style) can improve welfare. On the other hand, when the originator is better at leveraging investment and service costs are high, a more restrictive license (e.g., General Public License, or GPL style) can be best for social welfare even when a contributor can cost-efficiently develop the software. The online appendix is available at https://doi.org/10.1287/isre.2017.0726 ."
/doi/10.1287/mnsc.1050.0428," Vagueness attitudes have been used to explain anomalies and irregularities in investment behavior. It is generally assumed (Ellsberg 1961) that decision makers (DMs) dislike vagueness, but this assumption has been challenged by empirical results documenting systematic alternative attitudes to vagueness as a function of its source, the domain of the decisions, and the response mode used. We investigate these three factors in a within-subjects design that was embedded in an investment context. DMs evaluated investment options that varied in terms of their sources of vagueness (probabilities and/or outcomes), in both domains (gains or losses), and employed two response modes (pricing or choice). We confirm that individuals’ vagueness attitudes are malleable, contingent on the dimension salience and the reference domain. In particular, we observed three distinct patterns of “reversals of attitudes” towards vagueness. Our results indicate that the ability of vagueness attitudes to predict investment behavior is limited, as decisions can be systematically influenced by task context and/or perceived gain or loss positions. Economic models may be improved by incorporating more flexible assumptions about individuals’ attitudes toward vagueness."
/doi/10.1287/isre.2017.0735," While the growth of the mobile apps market has created significant market opportunities and economic incentives for mobile app developers to innovate, it has also inevitably invited other developers to create rip-offs. Practitioners and developers of original apps claim that copycats steal the original app’s idea and potential demand, and have called for app platforms to take action against such copycats. Surprisingly, however, there has been little rigorous research analyzing whether and how copycats affect an original app’s demand. The primary deterrent to such research is the lack of an objective way to identify whether an app is a copycat or an original. Using a combination of machine learning techniques such as natural language processing, latent semantic analysis, network-based clustering, and image analysis, we propose a method to identify apps as original or copycat and detect two types of copycats: deceptive and nondeceptive. Based on the detection results, we conduct an econometric analysis to determine the impact of copycat apps on the demand for the original apps on a sample of 10,100 action game apps by 5,141 developers that were released in the iOS App Store over five years. Our results indicate that the effect of a specific copycat on an original app’s demand is determined by the quality and level of deceptiveness of the copycat. High-quality nondeceptive copycats negatively affect demand for the originals. By contrast, low-quality, deceptive copycats positively affect demand for the originals. Results indicate that in aggregate the impact of copycats on the demand of original mobile apps is statistically insignificant. Our study contributes to the growing literature on mobile app consumption by presenting a method to identify copycats and providing evidence of the impact of copycats on an original app’s demand. The online appendix is available at https://doi.org/10.1287/isre.2017.0735 ."
/doi/10.1287/opre.1070.0472," Forest ecosystem management often requires spatially explicit planning because the spatial arrangement of harvests has become a critical economic and environmental concern. Recent research on exact methods has addressed both the design and the solution of forest management problems with constraints on the clearcut size, but where simultaneously harvesting two adjacent stands in the same period does not necessarily exceed the maximum opening size. Two main integer programming approaches have been proposed for this area restriction model. However, both encompass an exponential number of variables or constraints. In this work, we present a new integer programming model with a polynomial number of variables and constraints. Branch and bound is used to solve it. The model was tested with both real and hypothetical forests ranging from 45 to 1,363 polygons. Results show that the proposed model's solutions were within or slightly above 1% of the optimal solution and were obtained in a short computation time."
/doi/10.1287/orsc.2017.1142," The link between organizational structure and innovation has been a longstanding interest of organizational scholars, yet the exact nature of the relationship has not been clearly established. Drawing on the behavioral theory of the firm, we take a process view and examine how hierarchy of authority—a fundamental element of organizational structure reflecting degree of managerial oversight—differentially influences behavior and performance in the idea generation versus idea selection phases of the innovation process. Using a multimethod approach that includes a field study and a lab experiment, we find that hierarchy of authority is detrimental to the idea generation phase of innovation, but that hierarchy can be beneficial during the screening or selection phase of innovation. We also identify a behavioral mechanism underlying the effect of hierarchy of authority on selection performance and propose that selection is a critical organizational capability that can be strategically developed and managed through organizational design. Our investigation helps clarify the theoretical relationship between structure and innovation performance and demonstrates the behavioral and economic consequences of organizational design choice. The online appendix is available at https://doi.org/10.1287/orsc.2017.1142 ."
/doi/10.1287/moor.1040.0143," We study the properties of ultramodular functions, a class of functions that generalizes scalar convexity and that naturally arises in some economic and statistical applications."
/doi/10.1287/inte.1100.0505," The increasing threat of terrorism makes security at major locations of economic or political importance a major concern. Limited security resources prevent complete security coverage, allowing adversaries to observe and exploit patterns in patrolling or monitoring, and enabling them to plan attacks that avoid existing patrols. The use of randomized security policies that are more difficult for adversaries to predict and exploit can counter their surveillance capabilities. We describe two applications, ARMOR and IRIS, that assist security forces in randomizing their operations. These applications are based on fast algorithms for solving large instances of Bayesian Stackelberg games. Police at the Los Angeles International Airport deploy ARMOR to randomize the placement of checkpoints on roads entering the airport and the routes of canine unit patrols within the airport terminals. The Federal Air Marshal Service has deployed IRIS in a pilot program to randomize the schedules of air marshals on international flights. This paper examines the design choices, information, and evaluation criteria that were critical to developing these applications."
/doi/10.1287/mksc.20.3.315.9763," Firms often search enthusiastically for distinguishing traits that they may use to price discriminate between segments. Yet there are occasions in which firms forgo the opportunity to price discriminate and instead charge a single price. Traditional explanations for why retailers forgo the opportunity to price discriminate focus on the cost of discriminating, including operational costs, explicit discrimination costs, and implicit discrimination costs. In this paper we identify an additional reason for why firms may forgo an opportunity to price discriminate. By revealing that a product is being sold to a broad range of segments, a retailer implicitly claims that the product is suitable for each segment. However, claiming that a premium-quality product is suitable for price-sensitive consumers undermines the credibility of a retailer's quality claim. The signaling explanation was motivated by extensive discussions over more than a year with a major catalog retailer that sells premium-quality jewelry and gifts. Discussions with managers revealed that they were reluctant to use any price-discrimination mechanism that signals their products are targeted at price-sensitive customers. For example, the catalog does not include sale or clearance sections and does not target more price-sensitive customers by using separate items. However, management was under some pressure to consider installment-billing offers, which allow customers to pay over a series of periods rather than in a lump sum. Management feared that offering installment billing may adversely affect customers' quality perceptions and demand. To investigate this issue, we develop a general game-the-oretic model, illustrate how the model extends to installment billing, and conduct a large-scale field test. The general model illustrates how selling to multiple segments may lead to an adverse quality signal. We illustrate how the model extends to installment-billing offers in a direct-mail catalog. Installment-billing offers allow customers to spread the total payment over a series of payments. All customers have the option of using installment billing, and customers who use the plan receive an economic benefit (an interest-free loan). We would normally expect this type of offer to increase demand or, at a minimum, leave demand unchanged. However, because installment-billing offers target credit-constrained customers, we predicted that the introduction of installment billing would prompt an unfavorable quality inference and reduce demand among quality-sensitive customers. We empirically investigated this prediction in a large-scale field test with a catalog that offers premium-quality jewelry and gifts. Two versions of the catalog were created: a test version that contained an installment-billing offer, and a control version in which installment billing was not offered. Importantly, the prices in both the test version and control version were identical. Approximately 240,000 catalogs were mailed, and customers were randomly assigned to either the test version or control version. Results show that the installment-billing offer (test version) was associated with both a reduction in the number of orders received and a reduction in aggregate revenue. Offering installment billing resulted in approximately $15,000 in lost revenue. The only plausible explanation for this counterintuitive finding appears to be the signaling theory. To investigate the long-term effects, the catalog agreed to survey their customers to measure how an offer of installment billing affects their customers' quality perceptions. Similar to the field test, two versions of a catalog were created, and customers were randomly mailed a catalog, along with a short survey. Respondents were asked to browse through the catalog and return their responses in a replypaid envelope. The findings are consistent with customer beliefs in the signaling model: Offering installment billing lowers the perceived quality of the items in the catalog. The field test and survey findings were both statistically significant and managerially relevant. Together, the results convinced the catalog not to include installment-billing offers in future catalogs."
/doi/10.1287/mnsc.1040.0327," We examine the impact of reduced search costs on prices of commodity products in electronic marketplaces. Conventionally, reduced consumer search costs may be expected to engender stronger price competition between firms, resulting in lower prices and improved consumer welfare. This notion was formalized in Stahl (1989, “Oligopolistic pricing with sequential consumer search,” American Economic Review , Vol. 79, No. 4, pp. 700–712) in a model of static firm competition. In this paper, we show that these standard welfare conclusions may be neutralized or reversed in a dynamic environment. We focus on self-enforcing collusion by firms and characterize the conditions under which collusive equilibria exist. We show that less costly consumer search can facilitate firms' abilities to collude, resulting in higher prices and reduced consumer welfare, even with imperfect or no monitoring by sellers of each other's prices. If the same technology that eases consumer search also allows firms to monitor each other's prices more easily, then firms can more easily detect cheating on a collusive price arrangement, allowing an even greater scope for collusion. This raises antitrust concerns with respect to the electronic marketplace and suggests that at least some of the anticipated competitive gains from electronic market systems may be difficult to realize."
/doi/10.1287/opre.35.2.185," This paper is concerned with the derivation of demand functions for primary resources in an economic environment with uncertainty. This issue is critical when resource markets must decide, in the face of uncertainty, whether to expand capacity and/or to introduce new technologies. The models proposed are primarily data oriented. They explore two alternative behavioral models of action under uncertainty: here-and-now and wait-and-see . We present an application to the U.S. energy sector's demand for coal and the related expansion of capacity of coal-fired electricity generation plants."
/doi/10.1287/mksc.3.3.247," New product planning models attempt to predict the market consequences of product line and product design decisions. One output of such models, especially those driven by subjective or market research data, is usually theoretical market shares based upon consumer preferences under idealized conditions. This paper describes a class of models that bridge the gap between such theoretical market shares and dynamic sales forecasts. This model accounts for differences in customer awareness of different products, for differences in product announcement dates, for differences in product availability, for differences in marketing efforts, for customer inertia and for customer purchasing delays. The paper describes specific details of such a model used in a system developed for market analysis of high speed nonimpact computer printers."
/doi/10.1287/inte.2014.0768," We describe a software application that enables owners of generation output from a virtualized Federal Columbia River Power System to safely operate the system, while also shaping the generation to meet their energy and economic needs. The application, the Optimizer, employs modern operations research techniques to convert a highly nonlinear problem into a linear one to create a robust solution for the six-dam system on an hourly basis, over a 10-day time horizon, and within a few minutes. The Optimizer helps to simultaneously manage and optimize the generation portfolios for 13 utilities in a stringent time frame around the clock, and enables hydroelectric (i.e., hydro) planners to ensure that the operation of the river meets all the requirements for flood control, fish management, electrical reliability, safe dam operations, and recreation under high degrees of uncertainty. The Optimizer allows utilities to integrate renewable, environmentally friendly wind and solar generation into their resource portfolio with hydro generation, and empowers these utilities to rapidly make decisions and adapt to changing conditions. We estimate that this project will reap benefits of $765–$952 million between 2011 and 2028."
/doi/10.1287/orsc.1120.0765," Organizations that enjoy some slack are believed to make good use of it in their strategic decisions. Using panel data on firms in the U.S. film distribution industry between 1985 and 2007, this article examines how financial slack affects the volume of new product introductions, the competitive strategies for those releases, and their economic performance. Unexpectedly successful “sleeper” films are exploited as a source of exogenous financial slack in the econometric analysis. The results suggest that unexpected financial slack leads to more product introductions, less marketing support for the new products, and no improvement in performance. These findings are consistent with an attribution process in which managers attempt to replicate extraordinary success even if it is largely random, providing real-world evidence of a mechanism recently developed in theory and laboratory research."
/doi/10.1287/mnsc.1070.0720," This paper explores the consumer value of publicly associating oneself with a brand image. The economic value of such association to the consumer of a brand is coming from its affect on the information exchange between consumers engaged in a search for partnerships with each other. It turns out that the brand use can be valuable to consumers for communication even when they do not have the proper incentives to make simple conversations valuable or informative. In particular, when the correlation of the interests of agents in a partnership is low, conversations are not very informative, while brand use remains informative and valuable. Furthermore, the more widespread the brand use is, the less truthful (and informative) one can expect conversations to be. In addition, the consumer value of a brand image is shown to have an inverse-U shape in the difficulty of searching, as consumers look for conformity when a search is difficult, and conversations become more and more truthful when a search becomes very easy."
/doi/10.1287/trsc.1090.0279," City logistics aims to reduce the nuisances associated to freight transportation in urban areas while supporting their economic and social development. The fundamental idea is to view individual stakeholders and decisions as components of an integrated logistics system. This implies the coordination of shippers, carriers, and movements as well as the consolidation of loads of several customers and carriers into the same environment-friendly vehicles. City logistics explicitly aims to optimize such advanced urban transportation systems. We focus on a challenging city logistics planning issue, the integrated short-term scheduling of operations and management of resources, for the general case involving a two-tiered distribution structure. We investigate the main issues related to the problem, introduce a new problem class, propose both a general model and formulations for the main system components, and identify promising solution avenues."
/doi/10.1287/orsc.2021.1428," This paper develops a new understanding about how “client managers”—those using platform labor markets to hire and manage workers—attempt to maintain control when managing skilled contractors. We conducted an inductive field study analyzing interactions between client managers and contractors in software development “gigs” mediated by a platform labor market. The platform provided multiple tools client managers could use for control, including in response to unexpected events. We found that, when managers used the tools to exert coercive control over contractors acting unexpectedly, it backfired and contributed to uncompleted project outcomes. In contrast, when they refrained from using the tools for coercive control in such circumstances and instead engaged in what we call collaborative repair , their actions contributed to completed project outcomes. Collaborative repair refers to interactions that surface misaligned interpretations of a situation and help parties negotiate new, reciprocal expectations that restore trust and willingness to continue an exchange. Client managers’ attempts at collaborative repair yielded fuller understanding of project-related breakdowns and shared investment in new expectations, facilitating effective control and completed projects. This study extends prior theories of control by characterizing the new client manager role created by platforms and demonstrating how initiating repair is integral for managers’ capacity to accomplish control in these comparatively brittle work relationships."
/doi/10.1287/ijoc.1030.0056," The pressure from today’s economic and energy markets demands further distribution of decision making in large, dynamic networks. To this end, distributed model predictive control (MPC) divides the task of operating a dynamic network into a set of small, localized subtasks, one for each distributed control agent. To guide the decomposition of the overall task, which plays a central role in the quality of the operation yielded by the distributed agents, this paper proposes a model for problem decomposition based on the design of agent communication networks. The model gives rise to the communication-network design problem: A bicriteria optimization problem whose objectives are the maximization of the influence perceived by the agents and the minimization of the communication cost. The benefit of the model is the potential to concentrate the effort of implementing distributed MPC, and measuring its actual performance, on a reduced number of decompositions, namely those that are Pareto efficient for the model. The paper gives an account of related work, including the computational complexity of finding Pareto efficient solutions, an integer programming formulation, and families of valid inequalities. Its main contribution is the demonstration that the model can be effective, meaning that Pareto efficient solutions to the model tend to induce efficient problem decompositions. The experimental evidence was gathered by applying the model to decompose control problems of two representative networks, namely arrays of pendulums and electric power networks, and measuring the quality of the operation delivered by the distributed control agents."
/doi/10.1287/msom.2019.0854," Problem definition : We consider a setting where online advertisers seek to acquire impressions from an advertising exchange through a multitier network of intermediaries, and study the mechanisms offered by the ad exchange and intermediaries when the advertisers’ values are private. Academic/practical relevance : As opposed to traditional manufacturer/retailer settings, intermediaries in display advertising auction off contingent goods that they purchase only if downstream buyers signal interest. This motivates our study of how intermediaries should bid on behalf of their customers in the mechanism of an upstream intermediary and how the structure of the intermediation network affects the profits of its participants. Methodology : We provide a game-theoretic model to study the mechanisms offered by the ad exchange and intermediaries within a practically relevant class of mechanisms. Results : We characterize a subgame perfect equilibrium of the game among the intermediaries and the seller, and show that the equilibrium mechanisms have a simple and appealing structure: intermediaries bid the virtual value associated with the maximum downstream report in the upstream intermediary’s mechanism, whenever this quantity is positive. Managerial implications : We show that economic incentives are not necessarily aligned along the network and that the position in the intermediation network has a significant impact on the profits of the intermediaries. That is, when advertisers’ value distribution has a “light tail,” upstream intermediaries profit more (relative to downstream ones). Moreover, as the tail of the advertisers’ value distribution gets heavier, downstream intermediaries profit more, and their profits eventually exceed those of upstream ones. In addition, we show that a horizontal merger may not be profitable for intermediaries, and we analyze the impact of market size on the profits of intermediaries."
/doi/10.1287/mnsc.46.12.1528.12079," This paper provides an economic rationale for modern manufacturing control practices such as the minimal inventories in Just in Time (JIT)systems, zero-defect policies, and continuous improvement. The popular and academic literature contains descriptive studies on the mechanics of these systems and their perceived benefits. We use a model of production to analyze both informational and incentive rationales for reduced inventories. A JIT-like environment of low inventory levels is optimal in our model because it helps workers to better observe and understand the production process and to think and act creatively to improve operational reliability and yields. Empirical evidence using data obtained from 116 plants worldwide supports our conclusions about the effect of reduced inventories on process reliability, product quality, and cost."
/doi/10.1287/mnsc.48.3.399.7726," This paper derives the optimal simultaneous capacity and production plan for a shortlife-cycle, produce-to-stock good under stochastic demand. Capacity can be reduced as well as added, at exogenously set unit prices. In both cases studied, with and without carryover of unsold units, a target interval policy is optimal: There is a (usually different) target interval for each period such that capacity should be changed as little as possible to bring the level available into that interval. Our contribution in the case of no carry-over, is a detailed characterization of the target intervals, assuming demands increase stochastically at the beginning of the life cycle and decrease thereafter. In the case of carry-over, we establish the general result and show that capacity and inventory are economic substitutes: The target intervals decrease in the initial stock level and the optimal unconstrained base stock level decreases in the capacity level. In both cases, optimal service rates are not necessarily constant over time. A numerical example illustrates the results."
/doi/10.1287/mnsc.2017.2902," We describe the effect of social media advertising content on customer engagement using data from Facebook. We content-code 106,316 Facebook messages across 782 companies, using a combination of Amazon Mechanical Turk and natural language processing algorithms. We use this data set to study the association of various kinds of social media marketing content with user engagement—defined as Likes , comments, shares, and click-throughs—with the messages. We find that inclusion of widely used content related to brand personality—like humor and emotion—is associated with higher levels of consumer engagement ( Likes , comments, shares) with a message. We find that directly informative content—like mentions of price and deals—is associated with lower levels of engagement when included in messages in isolation, but higher engagement levels when provided in combination with brand personality–related attributes. Also, certain directly informative content, such as deals and promotions, drive consumers’ path to conversion (click-throughs). These results persist after incorporating corrections for the nonrandom targeting of Facebook’s EdgeRank (News Feed) algorithm and so reflect more closely user reaction to content than Facebook’s behavioral targeting. Our results suggest that there are benefits to content engineering that combines informative characteristics that help in obtaining immediate leads (via improved click-throughs) with brand personality–related content that helps in maintaining future reach and branding on the social media site (via improved engagement). These results inform content design strategies. Separately, the methodology we apply to content-code text is useful for future studies utilizing unstructured data such as advertising content or product reviews. The online appendix is available at https://doi.org/10.1287/mnsc.2017.2902 . This paper was accepted by Chris Forman, information systems."
/doi/10.1287/trsc.2013.0477," Driver fatigue is internationally recognized as a significant factor in approximately 15%–20% of commercial road transport crashes. In their efforts to increase road safety and improve working conditions of truck drivers, governments worldwide are enforcing stricter limits on the amount of working and driving time without rest. This paper describes an effective optimization algorithm for minimizing transportation costs for a fleet of vehicles considering business hours of customers and hours of service regulations. The algorithm combines the exploration capacities of population-based metaheuristics, the quick improvement abilities of local search, with forward labeling procedures for checking compliance with complex hours of service regulations. Several speed-up techniques are proposed to achieve an overall efficient approach. The proposed approach is used to assess the impact of different hours of service regulations from a carrier-centric point of view. Extensive computational experiments for various sets of regulations in the United States, Canada, the European Union, and Australia are conducted to provide an international assessment of the impact of different rules on transportation costs and accident risks. Our experiments demonstrate that European Union rules lead to the highest safety, whereas Canadian regulations are the most competitive in terms of economic efficiency. Australian regulations appear to have unnecessarily high risk rates with respect to operating costs. The recent rule change in the United States reduces accident risk rates with a moderate increase in operating costs."
/doi/10.1287/trsc.1090.0313," Data about the mortality risk of scheduled passenger air travel over 2000–2007 around the world is examined in this paper. Worldwide, the average passenger death risk per scheduled flight over 2000–2007 was about one in 3.0 million. However, much as the center of mass of a doughnut is the center of the hole—where there is no mass—the worldwide average represents the actual risk level in few if any countries. The data support a three-population risk model across nations, in which the differences in death risk are not statistically significant within groups but are highly significant across groups. The safest nations are the traditional first-world countries (e.g., Canada, Japan), with a death risk per flight of about 1 in 14 million. Next safest are those developing-world nations that have either have recently attained first-world status (e.g., Singapore, South Korea) or are classified by experts as newly industrialized (e.g., Brazil, China) Their aggregrate death risk per flight was about 1 in 2 million. The least safe nations statistically are remaining developing-world countries, with a death risk per flight of about 1 in 800,000. In terms of relative risk, divergences within the developing world are modest compared to the overall difference between the first and developing worlds. The observed risk pattern might reflect a confluence of economic and cultural factors."
/doi/10.1287/trsc.1080.0242," This paper presents a new methodology for incorporating origin and destination (O&D) network effects into the fleet assignment process. The methodology uses a decomposition strategy to combine a modified version of a leg-based fleet assignment model (Leg-FAM) with the network flow aspects of probabilistic O&D yield management. By decomposing the problem, the nonlinear aspects of the O&D market effects and passenger flow are isolated in O&D yield management and incorporated in FAM using linear approximations to the total network revenue function. To illustrate this methodology and its economic benefits, an example consisting of 10 cities, 48 flight legs, and 534 O&D market classes is presented and compared to the results found using two commonly used Leg-FAM formulations. To benchmark the benefits of using an O&D approach to fleet assignment, we present two case studies comparing the Leg-FAM and O&D FAM approaches using actual airline schedules with more than 4,000 daily operations. For these benchmarks, we present the results of a general fleet assignment process in which all the scheduled flights must be assigned an aircraft type and a schedule reduction run in which nonprofitable flights can be cancelled. The methodology presented in this paper is ideally suited for demand-driven dispatch (D 3 ) scenarios, where near-term fleeting changes are made to better match O&D passenger demand to available aircraft capacity. To illustrate the use of the O&D FAM methodology to facilitate near-term refleeting process, we present results from implementing a D 3 process using O&D FAM at AMR's American Eagle Airlines."
/doi/10.1287/isre.1120.0465," By studying the change in employees' network positions before and after the introduction of a social networking tool, I find that information-rich networks (low in cohesion and rich in structural holes), enabled by social media, have a positive effect on various work outcomes. Contrary to the notion that network positions are difficult to alter, I show that social media can induce a change in network structure, one from which individuals can derive economic benefits. In addition, I consider two intermediate mechanisms by which an information-rich network is theorized to improve work performance—information diversity and social communication—and quantify their effects on productivity and job security. Analysis shows that productivity, as measured by billable revenue, is more associated with information diversity than with social communication. However, the opposite is true for job security. Social communication is more correlated with reduced layoff risks than with information diversity. This, in turn, suggests that information-rich networks enabled through the use of social media can drive both work performance and job security, but that there is a trade-off between engaging in social communication and gathering diverse information."
/doi/10.1287/trsc.1120.0438," At many congested airports, access rights are governed by a system of slot controls. A slot conveys to its owner the right to schedule an operation (flight arrival or departure). In this paper, stochastic optimization models are developed to determine the numbers of slots to make available over the course of a day, controlling for the long-term uncertainty induced in arrival or departure capacities because of weather conditions. Three related integer programming formulations for this problem are presented, which vary both in their computational properties and the economic trade-offs modeled. The models are compared both analytically and computationally. Experiments using data from New York's LaGaurdia Airport are reported to demonstrate the impact of these models on optimizing slot profiles while considering long-term capacity uncertainty and several policy objectives."
/doi/10.1287/mnsc.2017.2784," Prior research suggests that social connections, including acquaintances, friends, and family, are valuable in a job search process. In these studies, the size of an average job seeker’s network was much smaller and limited by the available modes of communication and the costs associated with maintaining social connections. However, the recent growth of online social networks has enabled job seekers to stay connected with many connections, weak or strong. Thus, the number of online connections—especially weak—has increased significantly. In this paper, we first examine whether an individual’s social network plays a role in driving job search behavior, taking into account online social networking sites (e.g., LinkedIn) and other job search modes. Second, we examine how ties in online social networks (both weak and strong) affect job search outcomes (modeled sequentially as job leads, interviews, and offers), and we compare the findings to job outcomes from traditional job search modes (e.g., career fairs, newspaper, Internet postings, and friends and family). To do so, we first construct an economic model of search behavior incorporating cost and benefit functions; we then estimate the model to recover structural parameters using the survey data of 424 users. Our findings show that users are spending more time searching for jobs on social networking sites. In addition, users’ strong ties play a significant role in job search and are especially helpful in generating job leads, interviews, and offers; the weak ties, on average, are ineffective in generating positive outcomes and marginally negative in some cases. This paper was accepted by Lorin Hitt, information systems."
/doi/10.1287/msom.2020.0914," The (relative) cost of the customer’s waiting time has long been used as a key parameter in queuing models, but it can be difficult to estimate. A recent study introduced a new queue characteristic, the value of the customer’s waiting time, which measures how an increase in the total customer waiting time reduces the servers’ idle time. This paper connects and contrasts these two fundamental concepts in the queuing literature. In particular, we show that the value can be equal to the cost of waiting when the queue is operated at optimal. In this case, we can use the observed queue length to compute the value of waiting, which helps infer the cost of waiting. Nevertheless, these two measures have very different economic interpretations, and in general, they are not equal. For nonoptimal queues, comparing the value with the cost helps shed light on the underlying causes of the customer’s waiting. Although it is tempting to conclude that customers in a queue with a lower value of waiting expect to wait longer, we find that the value of waiting in general does not have a monotonic relationship with the expected waiting time, nor with the expected queue length."
/doi/10.1287/orsc.2015.0990," This paper investigates whether and when affiliation to business groups enables or constrains firms’ international search behavior during institutional transitions. We theorize that given the unique structure and complex form of business group organizations, the search behavior of affiliated firms is influenced by the degree of (mis)alignment in outlook at the group and affiliate levels of management. We identify the scope of institutional changes, business group attributes, and affiliate characteristics as sources of such (mis)alignment. The results from panel data on 298 firms from the Indian pharmaceutical industry for the 1992–2007 period show that the constraining effects of business group affiliation are observed only when institutional changes are specific to the affiliates’ industry and not when broad institutional changes affect the business group as a whole. Moreover, we observe heterogeneity in the search behavior of group affiliated firms. First, the degree of misalignment is greater in the case of affiliates belonging to older business groups and those that are more distant in terms of age and industry since the group’s founding. Second, by contrast and suggesting an alignment in outlook, we find that affiliated firms that occupy a prominent position within a group or industry are able to bargain for and receive attention and support from the business group to undertake international search. Our findings have implications for research on the role of business groups in a changing institutional context and for the strategic adaptation of firms embedded in complex organizational and institutional settings."
/doi/10.1287/ited.2017.0183ca," Vastrapur Car Rental Services operating in Ahmedabad, Gujarat State, India offers five different options to its rental customers. Each option has a usage agreement (limits on mileage and duration) and rental rate and some customers exceed these limits. The central theme of the case is to compute the average profitability in the context of limited fleet size, uncertainty related to number and type of customer requests, and variability in the utilization of vehicles. In addition, the case provides an opportunity to discuss situations like negotiating wages, outsourcing vehicles on a need basis, and subscribing capacity to an integrator (an online platform linking the supply and the demand sides). The case was given to MBA students in a core probability course and a core decision sciences course. The discussion can span over one or two classes depending on the students' background and nature of the course. The case helps students apply different concepts in probability (including random variables and expected value) to analyze a real-life managerial situation. When resolved appropriately, it demonstrates the power of simple probabilistic tools in addressing managerial challenges. The case is amenable for a simulation based analysis to study the interplay between demand variability (quantity and mix) and economic consequences in multiple scenarios. Supplemental Material : The teaching note is available at https://www.informs.org/Publications/Subscribe/Access-Restricted-Materials ."
/doi/10.1287/mksc.1090.0531," We appreciate the opportunity to respond to the commentaries and additional analyses by Fornell et al. [Fornell, C., S. Mithas, F. V. Morgeson III. 2009a. The economic and statistical significance of stock returns on customer satisfaction. Marketing Sci . 28 (5) 820–825] and Ittner et al. [Ittner, C., D. Larcker, D. Taylor. 2009. The stock market's pricing of customer satisfaction. Marketing Sci. 25 (5) 826–835]. Both studies have multiple theoretical and econometric limitations that challenge the validity of their arguments and findings (e.g., neither study allows for time-varying risk factor loadings in their assessments of mispricing although the composition of firms in their analyzed portfolios changes over time, Fornell et al. mischaracterize the efficient markets hypothesis, and Ittner et al. do not use standard panel data econometric methods and models). Generalizations about customer satisfaction, like any other construct, should be assessed by appropriate econometric methods and should withstand rigorous scrutiny. We believe an open, frank dialogue can help clear up misconceptions, air central issues, and advance better understanding of methods and analyses for assessing the financial market implications of marketing metrics such as customer satisfaction."
/doi/10.1287/inte.29.2.89," We conducted a study for Efes Beverage Group to evaluate various sites as potential locations for new malt plants. We performed an economic analysis that showed the inferiority of some alternatives. To evaluate the remaining alternatives, we developed a mixed-integer-programming model that considers both the location of new malt plants and the distribution of barley and malt. It considers the long-run effects of the decisions and minimizes the present value of total costs. Sakarya, Izmir, and Ankara turned out to be the best locations for new malt plants. Efes is currently using the model for distribution decisions. Based on our results and new developments that occurred since then, top managers are currently debating where and when to locate the new malt plants."
/doi/10.1287/opre.2014.1325," This paper analyzes a periodic-review, joint inventory and pricing control problem for a firm that faces stochastic, price-sensitive demand under a nonstationary environment with fixed ordering costs. Any unsatisfied demand is backlogged. The objective is to maximize expected profit over a finite selling horizon by coordinating the inventory and pricing decisions in each period. We show that for an additive demand model, an ( s , S , p ) policy is optimal when the expected revenue is quasi-concave in price, the inventory cost (of holding and/or backlogging) is quasi-convex, and the nonnegative random demand has a Pólya or uniform density function. For the special case with no fixed ordering cost, the optimality of a base stock list price policy is demonstrated for more general demand distributions and convex inventory cost. These sets of sufficient conditions generalize the existing conditions in the literature that require, for example, the demand and/or revenue functions to be concave or the model parameters to be stationary in time. Our generalization makes the structural results applicable to models broadly supported by economic theory and empirical data. In addition, our proof uses a distinct sequential optimization technique for iteratively establishing the quasi- K -concavity of dynamic optimal value functions."
/doi/10.1287/opre.1110.0961," Urban growth compromises open space and ecosystem functions. To mitigate the negative effects, some agencies use reserve selection models to identify conservation sites for purchase or retention. Existing models assume that conservation has no impact on nearby land prices. We propose a new integer program that relaxes this assumption via adaptive cost coefficients. Our model accounts for the two key land price feedbacks that arise in markets where conservation competes with development: the amenity premium and price increases driven by shifts in market equilibriums. We illustrate the mechanics of the proposed model in a real land retention context. The results suggest that in competitive land markets, the optimal conservation strategy during the initial phase of the retention effort might be to use available dollars to buy fewer parcels with smaller total area that are under high risk of development. We show that failure to capture the land-price feedbacks can lead to significant losses in biological conservation. The present study is the first to create a reserve selection model that captures the economic theory of competitive land markets in a dynamic framework, produces tangible, parcel-level conservation recommendations, and works on problems with thousands of potential site selection decisions and several planning periods."
/doi/10.1287/mnsc.2017.2771," “Bespoke,” or mass customization strategy, combines demand learning and preference learning. We develop an analytical framework to study the economic value of bespoke systems and investigate the interaction between demand learning and preference learning. We find that it is possible for demand learning and preference learning to be either complements or substitutes, depending on the customization cost and the demand uncertainty profile. They are generally complements when the personalization cost is low and the probability of having high demand is large. Contrary to usual belief, we show that higher demand uncertainty does not necessarily yield more complementarity benefits. Our numerical study shows that the complementarity benefit becomes weaker when customers are more strategic. Interestingly, the substitute loss can occur when the personalization cost is small and the probability of having high demand is large, when customers are strategic. The online supplement is available at https://doi.org/10.1287/mnsc.2017.2771 . This paper was accepted by Serguei Netessine, operations management."
/doi/10.1287/mnsc.1050.0482," Recent theoretical and empirical research on cognitive bias in decision making suggests that overoptimism critically influences entrepreneurs’ decisions to establish and sustain new ventures. We investigate whether such cognitive bias influences entrepreneurial venture performance using data on commercialization efforts for university inventions. In contrast to prior studies, our results suggest that entrepreneurial overoptimism does not appear to be the determining factor in the decision to found a firm. We do find that entrepreneurs continue unsuccessful development efforts for longer periods of time than do established firms, which is consistent with entrepreneurial overoptimism in the development of technologies with uncertain market prospects. This latter finding is also consistent with rationality-based models of decision-making behavior, however. We find that the economic returns associated with many of the technologies in our sample are realized after the start-up has been acquired by an established firm, suggesting that start-ups may serve as a transitional organizational form in the market for technology commercialization."
/doi/10.1287/mnsc.2013.1722," Within an industry, stock returns of larger firms lead those of smaller firms, suggesting an intraindustry information diffusion process. Most industry leaders, however, have business segments in other industries (henceforth, minor-segment industries), whereas most small firms are pure players operating in one industry only. If investors cannot filter out the irrelevant information from the leaders' minor segments, the pure players will be mispriced due to spurious cross-industry information diffusion (SCIID). Consistent with the SCIID hypothesis, we document both a strong contemporaneous and a lead–lag relation in stock returns between firms from industry leaders' minor-segment industries and pure players in the industry leaders' major-segment industry. Our results are not due to potential missing common factors or economic relationships between pure players and firms in the minor-segment industries. This paper was accepted by Wei Xiong, finance."
/doi/10.1287/mnsc.1030.0178," A major challenge in the creation of custom-designed products lies in the elicitation of customer needs. As customers are frequently unable to accurately articulate their needs, designers typically create one or several prototypes, which they then present to the customer. This process, which we call collaborative prototyping , allows both parties to anticipate the outcome of the design process. Prototypes have two advantages: They help the customer to evaluate the unknown customized product, and they guide both parties in the search for the ideal product specification. Collaborative prototyping involves two economic agents, with different information structures and different—and potentially conflicting—objective functions. This raises several interesting questions: how many prototypes should be built, who should pay for them, and how should they and the customized product be priced. We show that, depending on the design problem and the market characteristics, the designer should offer prototypes at a profit, at cost, or even for free."
/doi/10.1287/mnsc.2019.3543," Advances in medical testing and widespread access to the internet have made it easier than ever to obtain information. Yet, when it comes to some of the most important decisions in life, people often choose to remain ignorant for a variety of psychological and economic reasons. We design and validate an information preferences scale to measure an individual’s desire to obtain or avoid information that may be unpleasant but could improve future decisions. The scale measures information preferences in three domains that are psychologically and materially consequential: consumer finance, personal characteristics, and health. In three studies incorporating responses from over 2,300 individuals, we present tests of the scale’s reliability and validity. We show that the scale predicts a real decision to obtain (or avoid) information in each of the domains as well as decisions from out-of-sample, unrelated domains. Across settings, many respondents prefer to remain in a state of active ignorance even when information is freely available. Moreover, we find that information preferences are a stable trait but that an individual’s preference for information can differ across domains. This paper was accepted by Yuval Rottenstreich, judgment and decision making ."
/doi/10.1287/mnsc.2014.1996," Refining is indispensable to almost every natural-resource-based commodity industry. It involves a series of complex processes that transform inputs with a wide range of quality characteristics into refined finished products sold to end markets. In this paper, we take the perspective of a profit-maximizing refiner that considers upgrading its existing simple refinery to include intermediate-conversion flexibility, i.e., the capability of converting heavy intermediate components to light ones. We present a stylized two-stage stochastic programming model of a petroleum refinery to investigate the value drivers of conversion flexibility and the impact of input and output market conditions on its economic potential. Conversion flexibility adds value to refineries by either transforming a nonprofitable situation into a profitable one (referred to as purchase benefit) or improving profitability of an already profitable situation (referred to as unit revenue benefit). In a real-data-calibrated numerical study, we find the value of conversion flexibility (VoC) to be significant, accounting for 40% of the expected profit with conversion, and the purchase benefit and unit revenue benefit are equally important. Contrary to the intuition that, as a recourse action, conversion offers higher value for greater input price volatility, we find that VoC may decrease in input price volatility as a result of the differential impacts of increasing price volatility on the purchase benefit and the unit revenue benefit. Refineries also vary in their range flexibility, i.e., the ability to accommodate a narrow or wide range of inputs of different quality levels. Whether the range flexibility increases or decreases the value of conversion flexibility is affected by the direction in which the refinery expands its processing range and the heaviness of crude oils. This paper was accepted by Martin Lariviere, operations management."
/doi/10.1287/moor.1030.0072," This paper concerns a stochastic search problem in a forest. As motivation, consider the issue of investing in a research-and-development project. Each activity that could be undertaken in the project is represented as an edge in a forest. Each edge has a cost of being attempted and a probability of success. An edge can be attempted if each of its predecessors has been attempted, and if each of those attempts has succeeded. The overall project succeeds if a path is found from a stem to a leaf of the forest all of whose edges are successful. Overall success yields an economic benefit. The problem is to find an investment strategy that maximizes expected utility, either with a linear or an exponential utility function. This problem will be shown to have a simple solution. Each edge will be assigned an index such that expected utility is maximized by attempting, at each opportunity, an edge whose index is most positive, terminating the search when no edges remain whose indices are positive. These indices are nested in a way that makes them quick to compute."
/doi/10.1287/orsc.1070.0287," Interorganizational projects can provide a vehicle for innovation, despite the professional and organizational barriers that confront this form of organizing. The case of fire engineering shows how such projects use simulation technology as a boundary object to foster innovation in a new organizational field. Engineers use simulation technology to produce radical changes in fire control and management, such as using elevators to evacuate buildings during emergencies. A framework is developed that explores how decisions can be reached and tensions resolved amongst multiple, diverse, and discordant actors striving for a shared appreciation of negotiated futures. This framework extends theories of engineering knowledge and boundary objects. It sheds new light on how to organize collective, knowledge-based work to produce reliable and innovative designs."
/doi/10.1287/mnsc.2016.2490," Although the adoption of new technology has received significant attention in management research, investigations of abandonment have lagged. In this study, we examine differences in the rates of abandonment of medical technologies based on whether abandonment occurs in response to the emergence of a superior technology or in light of new information questioning its efficacy. We link differences in responses to underlying differences in the missions and incentives of organizations. Examining coronary stents across three technological regime changes using a census of approximately two million patients admitted to Florida hospitals from 1995 to 2007, we show meaningful differences across three hospital types: for-profit, not-for-profit, and academic medical centers. Results show that for-profit hospitals abandon the earlier generation in favor of a superior technology faster than not-for-profit hospitals, but this is not the case if the efficacy of the technology is questioned. Academic medical centers, however, have the highest rates of abandonment under both triggers. Importantly, we find that organizational factors dominate physician differences as explanatory factors for abandonment. Implications of these findings are twofold. First, we identify the factors likely at play, i.e., the salience of norms of science and the corresponding trade-offs with economic benefits, when organizations make abandonment decisions. Second, our work underscores the importance of organizational mission, which dominates individual preferences in determining rates of abandonment. This paper was accepted by David Hsu, entrepreneurship and innovation ."
/doi/10.1287/msom.2021.1035," Problem definition : Process innovation is commonly claimed to be a major source of competitive advantage for firms. Despite this perceived influence, it has received substantially less attention than product innovation, and much uncertainty remains about its true association with firm performance. We investigate the relationship between a pharmaceutical firm’s portfolio of manufacturing process innovations and its economic performance. Academic/practical relevance : We uniquely conduct a multidimensional evaluation of a firm’s portfolio of manufacturing process innovations at the product level. This allows a quantitative evaluation of both the relative benefit of the different dimensions of a portfolio as well as the potential complementarities between these in different technological landscapes. Methodology : Through a collaboration with expert patent attorneys, we develop a unique longitudinal data set that combines secondary data and evaluations of a firm’s portfolio of process patents along two key dimensions: novelty and scope. We conduct econometric analyses for a large-scale sample of active pharmaceutical ingredients (APIs) whose product patents have expired and for which process innovation is thus the main source of competitive advantage. Results : We find a positive association between the presence of manufacturing process innovation and firm performance. However, although portfolio’s scope appears to always be beneficial to performance, the effect of novelty alone depends on the ruggedness of the technological landscape: negative in smoother landscapes and positive in more rugged landscapes. Results further suggest that novelty and scope of a portfolio of process innovations are complementary across technological landscapes. Managerial implications : Our results provide important practical insights that can inform the organization and execution of the research and development process across high-technology industries. In particular, although process innovations can be economically beneficial, investing in high-novelty process innovations without a corresponding high scope could jeopardize payoffs, especially in technological landscapes that are relatively smooth."
/doi/10.1287/mnsc.2018.3257," We study a rebate program in the credit card industry where the firm automatically processed the rebate earned by consumers with flexible and effortless redemption opportunities in future transactions. Contrary to what the normative economic theory would predict, we document that consumers frequently failed to redeem their rebate. Given this observation, we examine what may have been the primary driving force behind this pattern and the implications for the firm. We provide evidence that the phenomenon that we observe in our study is a result of cardholders being influenced by the way their outcomes were framed. We find that redemption incidences coincide with episodes where rebates are a larger portion of the amount spent. On the basis of the empirical regularities, we formulate a model of cardholder behavior where the cardholder experiences increasing psychological utility when rebate redemption yields a deeper discount. The model estimates indicate that, although delayed redemption left customers monetarily worse off in each transaction (cardholders passed up on average $0.76 of card-level benefits to experience greater psychological utility), once the psychological utility is taken into account, customers were shown to fare better overall. For the firm, in addition to the card benefits that were passed up, the psychological utility generated an additional average benefit of $1.21 per redemption. Finally, through counterfactuals we show that carefully designing a rebate program that takes into account consumers’ psychological redemption utility can further benefit both consumers and firms. This paper was accepted by Juanjuan Zhang, marketing."
/doi/10.1287/trsc.1070.0199," The combination of a strict labor legislation with daily and weekly time-dependent demand patterns has meant that transit companies tend to suffer significant inefficiencies; indeed, as more drivers than the minimum required are hired, providing services at off-peak periods, many tend to be inactive during these periods. To attack this problem, two strategies have been proposed. The first is using flexible shifts that may change daily according to a predefined contract. The other is using split shifts in which drivers' working hours are split in two to serve both the morning and afternoon peak periods. However, evidence of driver acceptance of these strategies is lacking. This paper presents a methodology to identify which characteristics of a shift are important for drivers (considering their socioeconomic characteristics) and to quantify their impact. Such information would allow transit operators to offer better working shifts to their drivers, with mutual benefits. The approach includes estimation of a mixed logit model based on stated preference data for choice of working shift; the context definition, shift attributes, and in general the experimental design were quite involved. A sample of 436 drivers working at two important operators in Santiago were interviewed. Our modelling results show considerable heterogeneity in drivers' preferences and also show that a significant number of them would be prepared to participate in flexible shifts. Our method allows operators to determine the economic incentives they should attach to such shifts for them to be accepted by the required fraction of their drivers."
/doi/10.1287/mnsc.2020.3804," Media dissemination plays an important role in facilitating price discovery. Political pressure that restricts media dissemination can hinder this function and affect investors’ perceptions. This paper studies the magnitude of newspaper censorship in China and its economic consequences using a setting of tunneling scandals. We find significant evidence of censorship of tunneling-related negative news at the national and local levels. We further show that news that survives censorship reduces information asymmetry and improves pricing efficiency. Censorship blocks informative tunneling news and delays incorporation of tunneling reporting into prices. This paper was accepted by Brian Bushee, accounting."
/doi/10.1287/deca.2020.0415," Risk aversion and elasticity of intertemporal substitution (EIS) are separated via the celebrated recursive utility building on certainty equivalents of indirect utility. Based on an alternative separation method, we formulate a questionnaire for simultaneous and consistent estimation of risk aversion, subjective discount rate, and EIS. From a representative group of 1,153 respondents, we estimate parameters for these preferences and their variability within the population. Risk aversion and the subjective discount rate are found to be in the orders of 2 and 0, respectively, not diverging far away from results from other studies. Our estimate of EIS in the order of 10 is larger than often reported. Background variables like age and income have little predictive power for the three estimates. Only gender has a significant influence on risk aversion in the usually perceived direction that females are more risk-averse than males. Using individual estimates of preference parameters, we find covariance between preferences toward risk and EIS. We present the background reasoning on objectives, the questionnaire, a statistical analysis of the results, and economic interpretations of these, including relations to the literature."
/doi/10.1287/moor.2021.1155," A single homogeneous resource needs to be fairly shared between users that dynamically arrive and depart over time. Although good allocations exist for any fixed number of users, implementing these allocations dynamically is impractical: it typically entails adjustments in the allocation of every user in the system whenever a new user arrives. We introduce a dynamic fair resource division problem in which there is a limit on the number of users that can be disrupted when a new user arrives and study the trade-off between fairness and the number of allowed disruptions, using a fairness metric: the fairness ratio . We almost completely characterize this trade-off and give an algorithm for obtaining the optimal fairness for any number of allowed disruptions."
/doi/10.1287/mnsc.1120.1657," A question of increasing interest to researchers in a variety of fields is whether the biases found in judgment and decision-making research remain present in contexts in which experienced participants face strong economic incentives. To investigate this question, we analyze the decision making of National Football League teams during their annual player draft. This is a domain in which monetary stakes are exceedingly high and the opportunities for learning are rich. It is also a domain in which multiple psychological factors suggest that teams may overvalue the chance to pick early in the draft. Using archival data on draft-day trades, player performance, and compensation, we compare the market value of draft picks with the surplus value to teams provided by the drafted players. We find that top draft picks are significantly overvalued in a manner that is inconsistent with rational expectations and efficient markets, and consistent with psychological research. This paper was accepted by Uri Gneezy, behavioral economics."
/doi/10.1287/mnsc.2019.3382," A buying firm might in the future incur costs associated with a supplier’s carbon dioxide emissions, safety violations, or other social or environmental impacts. Learning about a supplier’s impacts requires costly effort, but it is necessary (and sometimes sufficient) to reduce those impacts. The capital market valuation of a buying firm reflects investors’ estimate of future costs associated with a supplier’s impacts, as well as any costs that the buying firm incurs in order to learn about and reduce a supplier’s impacts. This paper analyzes a game theoretic model in which a manager—with the objective of maximizing the capital market valuation of the buying firm—decides whether to learn about a supplier’s impacts, how much cost to incur to reduce the supplier’s impacts, and whether to disclose the supplier’s impacts to investors. The investors have rational expectations (e.g., that a manager might withhold bad news about the supplier’s impacts) and value the buying firm accordingly. The paper considers a mandate to disclose information learned about a supplier’s impacts. The paper shows that the disclosure mandate deters learning and thus, under plausible conditions, results in higher expected impacts. The disclosure mandate can result in lower expected impacts only if buying firms face moderately high future costs associated with suppliers’ impacts. In contrast, a disclosure mandate always increases a buying firm’s expected discounted profit and capital market valuation. A disclosure mandate can induce cooperation among buying firms with a shared supplier, yet result in higher expected impacts by the supplier. When a buying firm has alternative suppliers, the disclosure mandate favors commitment to a supplier to facilitate learning about that supplier’s impacts (instead of searching for a lower-impact supplier). This paper was accepted by Serguei Netessine, operations management."
/doi/10.1287/opre.46.6.883," In this paper we address periodic base-stock policies for stochastic economic lot scheduling problems. These represent manufacturing settings in which multiple items compete for the availability of a common capacity source, in the presence of setup times and/or costs, incurred when switching between items, and in the presence of uncertainty regarding demand patterns, production, and setup times. Under periodic base-stock policies, items are produced according to a given periodic item-sequence. This paper derives effective heuristics for the design of a periodic item-sequence minimizing system-wide costs. This sequence is constructed based on desirable production frequencies for the items, obtained as the solution of lower bound mathematical programs. An extensive numerical study gauges the quality of the proposed heuristics."
/doi/10.1287/mnsc.1050.0419," We explore the problem of pricing and allocation of unique, one-time digital products in the form of data streams. We look at the short-term problem where the firm has a capacitated shared resource and multiple products or service levels. We formulate the allocatively efficient Generalized Vickrey Auction (GVA) for our setting and point out the computational challenges in determining the individual discriminatory transfer payments. We propose an alternative uniform-price, computationally efficient, revenue-maximizing knapsack formulation called the Multiple Vickrey Auction (MVA). While not incentive compatible, the MVA mechanism achieves bounded posterior regret and can be solved in real time. It has the added benefit of realizing imputed commodity prices for the various services, a feature lacking in the discriminatory GVA approach. For service providers that are concerned about the incentive compatibility but want imputed service prices, we suggest a maximal MVA (mMVA) uniform-pricing scheme that trades off revenue maximization for allocative efficiency. For sake of completeness we discuss the properties of a first-price pay-your-bid scheme. While NP-hard and not incentive compatible, this formulation has the perceived benefit of cognitive simplicity on the parts of sellers and bidders."
/doi/10.1287/mnsc.2016.2511," This paper studies the effects of unemployment benefit schemes on individual productivity. We created employment and unemployment in the field and compared workers’ productivity under no unemployment benefits to productivity under two different unemployment schemes. In one scheme, the unemployed received an unconditional monetary transfer. In the other, the monetary transfer was obtained conditional on the unemployed spending some time on an ancillary activity. Our results challenge the standard economic theory prediction that unemployment benefits, especially unconditional compensations, hinder workers’ effort. We find that workers employed under the unconditional scheme are more productive than workers under the conditional one, and both schemes make workers more productive than having no unemployment benefit. We discuss two possible explanations for our results based on reciprocity and differential psychological costs of unemployment across unemployment benefit schemes. Data, as supplemental material, are available at http://dx.doi.org/10.1287/mnsc.2016.2511 . This paper was accepted by John List, behavioral economics ."
/doi/10.1287/opre.1110.0935," We propose and analyze a general periodic-review model in which the firm has access to a set of potential suppliers, each with specific yield and price characteristics. Assuming that unsatisfied demand is backlogged, the firm incurs three types of costs: (i) procurement costs, (ii) inventory-carrying costs for units carried over from one period to the next, and (iii) backlogging costs. A procurement strategy requires the specification, in each period, of (i) the set of suppliers to be retained, (ii) their respective shares in this period's replenishments, as well as (iii) the traditional aggregate order placed (among the various suppliers). We show how the optimal procurement strategy can be obtained with an efficient algorithm. A base-stock policy is no longer optimal, but in each period there exists a maximum inventory level, such that orders are placed if and only if the starting inventory is below this threshold. In each period it is optimal to retain a given number of suppliers that are cheapest in terms of that period's effective cost rates, i.e., the expected cost per usable unit. The optimal number of suppliers to be retained in a given period depends on all current and future parameters and distributions, but this dependence can be aggregated into a single so-called benchmark cost measure. Under Normal yield and demand distributions, the suppliers' market shares are determined by a single aggregate score, itself the product of a simple reliability score and a cost score."
/doi/10.1287/msom.2021.1026," Problem definition : This paper examines whether and, if so, how much an online–off-line return partnership between online and third-party retailers with physical stores (or “location partners”) generates additional value to location partners. Academic/practical relevance : Online shoppers often prefer to return products to stores rather than mailing them back. Many online retailers have recently started to collaborate with location partners to offer the store return option to their customers, and we quantify its economic benefit to a location partner. Methodology : We analyze proprietary data sets from Happy Returns (which provides return services for more than 30 online retailers) and one of its location partners, using a panel difference-in-differences model. In our study, a treatment is the initiation of the return service at each of the location partner’s stores, and an outcome is the store and online channel performance of the location partner. We then explore the mechanisms of underlying customer behavior that drive these outcomes. Results : We find that the partnership increases the number of unique customers, items sold, and net revenue in both store and online channels. We identify two drivers for this improved performance: (1) the location partner acquires new customers in both store and online channels, and (2) existing customers change their shopping patterns only in the store channel after using the return service; in particular, they visit stores more often, purchase more items, and generate higher revenue after their first return service. Managerial implications : To our knowledge, we provide the first direct empirical evidence of value to location partners from a return partnership, and as these partnerships become more prevalent, our findings have important managerial implications for location partners and online retailers alike."
/doi/10.1287/inte.2020.1045," We describe the development of a decision-support tool to assist in the operations of a large concentrated apple and pear juice plant. The tool’s objective is to generate detailed schedules of clarified juice batches to be produced in the following weeks considering incoming fruit forecasts, commercial commitments, and infrastructural constraints. The tool is based on two interactive modules, PLANNER and SIMOPT, with different and complementary purposes. Each module is based on mixed-integer models with specific inputs, outputs, and user interfaces. PLANNER consists of three submodules: (i) planning assigns a batch of concentrated juice to be produced on a specific day, taking into account cleaning activities, rest days, raw material availability, and production and storage constraints; (ii) preprocessing organizes juice orders in batches; and (iii) pooling provides a detailed monitoring of semielaborated juice in storage pools in terms of inventories and sugar and acid content. Finally, SIMOPT provides a detailed optimal operative condition of the plant together with a thorough calculation of specific costs. This information is used by PLANNER to evaluate the corresponding economic objective functions. Besides providing optimal target conditions to the plant and feasible production schedules, the developed tools generate production guidelines in the long term and allow performing scenario studies."
/doi/10.1287/isre.2016.0652," Chronic excessive turnover among information technology (IT) professionals has been costly to firms for decades with annual turnover rates as high as 24% even among Computerworld’s “100 Best Places to Work in IT.” Prior information systems literature has identified two key factors affecting turnover: boundary-spanning roles and low promotability in one’s current firm. We draw on tournament theory, which is primarily concerned with inducing effort in employees, to decompose promotability into two distinct constructs: the likelihood of promotion and benefit from promotion , and demonstrate that each has a distinct role in affecting turnover rates. Our key result is that a job ladder motivating IT professionals with large, infrequent promotions will lead to higher turnover than a job ladder with smaller, more frequent promotions. We describe the conditions under which rearranging the job ladder creates economic value for the firm. We also offer an explanation for the observation that jobs characterized by boundary-spanning activities have higher turnover, and show that such jobs are more sensitive to the effect of likelihood of promotion on turnover. We test our hypotheses on a detailed data set covering 5,704 IT professionals over a five-year period. We confirm that likelihood of promotion has the predicted effects on turnover of IT professionals. A one standard deviation increase in likelihood of promotion decreases turnover by over 99%, consistent with our prediction. The empirical analysis also confirms the predicted effects of boundary spanning activities."
/doi/10.1287/isre.2018.0799," The internet facilitates information flow between sex workers and buyers, making it easier to set up paid sexual transactions online. Despite the illegality of selling sexual services online, Section 230 of Communications Decency Act shields websites from liability for unlawful postings by third parties. Consequently, websites such as Craigslist have become a haven for prostitution-related ads. With prostitution-related sites still in operation, it is imperative to understand the link between these sites and prostitution trends. Specifically, in this paper, we quantify the economic impact of Craigslist’s entry on prostitution incidence and identify potential pathways in which the website affects the sex industry. Using a national panel data set for 1,796 U.S. counties from 1999 to 2008, our analyses suggest that entry of Craigslist to a county leads to a 17.58% increase in prostitution cases. In addition, the analyses reveal that a majority of prostitution activity on Craigslist is induced by organized vice groups, in addition to voluntary participation by smaller set of independent providers. Further, we find site entry has a stronger impact in counties with a past history of prostitution and produces spillover effects in neighboring locations that are not directly served by Craigslist. Sex workers providing niche sexual services are found to increase with site entry. In addition, we learn that site entry leads to an increase in transactions of existing workers and also attracts new workers to the market. We find that the increase in prostitution arrests does not catch up with the growth in prostitution trends brought in by Craigslist. Finally, we find complementarity effects between erotic and casual sex ads in leading to the increase of prostitution. Our results contribute broadly to the emerging literature on the societal challenges associated with online intermediaries and internet penetration, and serve to provide guidelines for policy makers in regulating the sex industry in the internet era. The e-companion is available at https://doi.org/10.1287/isre.2018.0799 ."
/doi/10.1287/opre.48.6.894.12392," The open-pit mining problem is to determine the contours of a mine, based on economic data and engineering feasibility requirements, to yield maximum possible net income. This practical problem needs to be solved for very large data sets. In practice, moreover, it is necessary to test multiple scenarios, taking into account a variety of realizations of geological predictions and forecasts of ore value. The industry is experiencing computational difficulties in solving the problem. Yet, the problem is known to be equivalent to the minimum cut or maximum flow problem. For the maximum flow problem there are a number of very efficient algorithms that have been developed over the last decade. On the other hand, the algorithm that is most commonly used by the mining industry has been devised by Lerchs and Grossmann (LG). This algorithm is used in most commercial software packages for open-pit mining. This paper describes a detailed study of the LG algorithm as compared to the maximum flow “push-relabel” algorithm. We propose here an efficient implementation of the push-relabel algorithm adapted to the features of the open-pit mining problem. We study issues such as the properties of the mine and ore distribution and how they affect the running time of the algorithm. We also study some features that improve the performance of the LG algorithm. The proposed implementations offer significant improvements compared to existing algorithms and make the related sensitivity analysis problem practically solvable."
/doi/10.1287/trsc.2014.0565," This paper develops novel mathematical models to maximize the benefits derived from the distribution of critical supplies to populations in need after a disaster. The formulations are based on welfare economics and the use of social costs, which are incurred by the segments of society involved in, and impacted by, the relief distribution strategy. The costs to the relief group are assessed as logistical costs, whereas the impacts to the beneficiaries are measured as the effects that the relief distribution has on their deprivation costs, which are the economic value of human suffering resulting from the lack of access to a good or service. The impacts on beneficiaries take into account two components: the reduction in deprivation costs for the recipients of the aid and the increase in deprivation costs for those individuals who do not receive the aid at a delivery epoch (the opportunity costs of the delivery strategy). The paper develops an inventory-allocation-routing model for the optimal assignment of critical supplies that minimizes social costs, designs suitable heuristic solution approaches, and assesses the performance of the heuristics using numerical experiments. The results, coupled with the insight gained from the models developed, are used to establish conclusions and policy recommendations."
/doi/10.1287/mnsc.2015.2211," It is difficult to test the prediction that future career prospects create implicit effort incentives because researchers cannot randomly “assign” career prospects to economic agents. To overcome this challenge, we use data from professional soccer, where employees of the same club face different external career opportunities depending on their nationality. We test whether the career prospect of being selected to a Euro Cup national team affects players’ pre-cup performances, using nationals of countries that did not participate in the Euro Cup as a control group. We find that the Euro Cup career prospect has positive effects on the performances of players with intermediate chances of being selected to their national team, but negative effects on the performances of players whose selection is very probable. Our findings have implications for the incentive effects of within-firm promotions and of external career opportunities. This paper was accepted by John List, behavioral economics."
/doi/10.1287/orsc.2018.1221," We examine the impact of geographic location of alliance activities on the design of safeguards in contracts governing research and development (R&D) partnerships. Joining research on agglomeration and alliance governance, we argue that the Marshallian agglomerative forces at work in a given location produce governance-related externalities that extend beyond productivity-related externalities considered in previous research. We investigate how location characteristics linked to Marshallian forces, such as local knowledge spillovers, R&D rivalry, dense industry employment, and the strength of professional organizations, have an impact on the specification of formal governance mechanisms. In particular, these Marshallian forces have a bearing on formal governance mechanisms that safeguard the execution of the R&D partnership, such as joint administrative interfaces and termination provisions. We analyze R&D partnerships between biotechnology and pharmaceutical firms and find that misappropriation hazards arising from greater knowledge spillovers and R&D competition in the region where R&D activities are located promote the use of these formal governance mechanisms in R&D partnerships. We also find that factors supporting thick interpersonal networks, such as the intensity of sectoral employment and the strength of professional bodies, reduce the use of formal governance mechanisms in R&D partnerships."
/doi/10.1287/mnsc.2016.2524," The consequences of many economic decisions materialize only in the future. To make informed choices in such decision problems, consumers need to anticipate the likelihood of future states of the world, the state dependence of their preferences, and the choice alternatives that may become relevant. This complex task may expose consumers to psychological biases like extrapolative expectations, projection bias, or salience. We test whether customers are affected by such biases when they buy advance tickets for an outdoor movie theater, a real-world situation that, because of the availability of reliable weather forecasts, closely resembles a stylized decision problem under risk. We find that customers’ decisions are heavily influenced by the weather at the time of purchase, even though the latter is irrelevant for the experience of visiting the theater in the future. The empirical evidence cannot be fully explained by a range of candidate rational explanations, but is consistent with the presence of the aforementioned psychological mechanisms. This paper was accepted by John List, behavioral economics ."
/doi/10.1287/msom.2018.0745," Problem definition : This research focuses on elderly patients who have been hospitalized and are ready to be discharged, but they must remain in the hospital until a bed in a geriatric institution becomes available; these patients “block” a hospital bed. Bed blocking has become a challenge to healthcare operators because of its economic implications and the quality-of-life effect on patients. Indeed, hospital-delayed patients who do not have access to the most appropriate treatments (e.g., rehabilitation) prevent new admissions. Moreover, bed blocking is costly, because a hospital bed is more expensive to operate than a geriatric bed. We are thus motivated to model and analyze the flow of patients between hospitals and geriatric institutions to improve their joint operation. Academic/practical relevance : Practically, our joint modeling of hospital-institution is necessary to capture blocking effects. In contrast to previous research, we address an entire time-varying network, which enables an explicit consideration of blocking costs. Theoretically, our fluid model captures blocking without the need for reflection, which simplifies the analysis as well as the convergence proof of the corresponding stochastic model. Methodology : We develop a mathematical fluid model, which accounts for blocking, mortality, and readmission—all significant features of the discussed environment. Then, for bed allocation decisions, the fluid model and especially, its offered load counterpart turn out insightful and easy to implement. Results : The comparison between our fluid model, a two-year data set from a hospital chain, and simulation results shows that our model is accurate and useful. Moreover, our analysis yields a closed form expression for bed allocation decisions, which minimizes the sum of underage and overage costs. Solving for the optimal number of geriatric beds in our system shows that significant reductions in cost and waiting list length are achievable compared with current operations. Managerial implications : Our model can support healthcare managers in allocating geriatric beds to reduce operational costs. Moreover, our model facilitates three extensions: a periodic reallocation of beds, incorporation of setup costs into bed allocation decisions, and accommodating home care (or virtual hospitals) when feasible."
/doi/10.1287/orsc.2016.1102," Trust in interfirm exchange has traditionally been treated as mutually held and jointly determined by the two parties in a relationship. Yet, the expectations of exchange partners can, and routinely do, differ with respect to the goals, preferences, and vulnerabilities in their shared relationship. To account for such differences in expectations, we propose a broadened conceptualization of the sources of interorganizational trust as dyadic. Viewing the sources of trust as dyadic expands the conventional focus on mutual elements to further emphasize exclusive features of an exchange relationship. To substantiate our theory, we examine a key source of interorganizational trust, exchange hazards, and assess the extent to which its effects vary as a function of (1) the locus of exchange hazards (own versus other) in the dyad, (2) the degree of power imbalance in the dyad, and (3) each party’s power position in the dyad. To assess the validity of our claims, we devise a matched dyad research design and collect identical information from both buyers and suppliers in a given exchange relationship. Based on our results, we make three unique observations consistent with the notion of dyadic sources of trust. First, the same exchange hazards have contrasting effects on trust (enhancing versus diminishing) across the dyad. Second, the degree of power imbalance has opposing effects across the dyad. Third, the relative significance of partners’ exchange hazards varies based on their respective power positions. The online appendix is available at https://doi.org/10.1287/orsc.2016.1102"
/doi/10.1287/mnsc.2017.2883," This paper investigates the impact of a corporate wellness program on worker productivity using a panel of objective health and productivity data from 111 workers in five laundry plants. Although almost 90% of companies use wellness programs, existing research has focused on cost savings from insurance and absenteeism. We find productivity improvements based both on program participation and postprogram health changes. Sick and healthy individuals who improved their health increased productivity by about 10%, with surveys indicating sources in improved diet and exercise. Although the small worker sample limits both estimate precision and our ability to isolate mechanisms behind this increase, we argue that our results are consistent with improved worker motivation and capability. The study suggests that firms can increase operational productivity through socially responsible health policies that improve both workers’ wellness and economic value, and provides a template for future large-scale studies of health and productivity. The online appendix is available at https://doi.org/10.1287/mnsc.2017.2883 . This paper was accepted by Serguei Netessine, operations management."
/doi/10.1287/trsc.1120.0430," Serious environmental and economic problems of using fossil fuels in transportation sections force managers to think of alternative fuels such as hydrogen, ethanol, biodiesel, natural gas, or electricity. Meanwhile, lack of fuel network infrastructures is a major problem, which needs to be investigated considering the number and optimal location of alternative fuel stations. In the literature, two different flow-based demand modeling concepts (the maximum cover and set cover) have been proposed for solving this problem. Because of the huge number of combinations of fuel stations for covering the flow of each path, the models are impractical for the real size problems. In this paper, the flow refueling location model was reformulated and a flexible mixed-integer linear programming model was presented, which was able to obtain an optimal solution much faster than the previous set cover version. The model also could be solved in the maximum cover form in a reasonable time on the large-sized networks."
/doi/10.1287/isre.2013.0482," The development of information systems and software applications increasingly needs to deliver culturally rich and affective experiences for user groups. In this paper, we explore how the collaborative practices across different expert groups can enable this experiential dimension of use to be integrated into the development of a software product. In an empirical study of computer games development—an arena in which the novelty and richness of the user experience is central to competitive success—we identify the challenges of conceptualizing and realizing a desired user experience when it cannot be readily specified in an initial design template, nor represented within the expertise of existing groups. Our study develops a theoretical framework to address these challenges. Through this framework, we are able to show how achieving a desired user experience requires developer groups to not only work across the boundaries that arise from specialized expertise, but also across wider fields centred on cultural production and software development, respectively. We find that their ability to do this is supported by distinctive “envisioning practices” that sustain an emerging shared “vision” for each game. The key research contributions that we then make are (a) grounding envisioning practices as a means of theorizing the collaborative practices centred on conceptualizing the user experience; (b) identifying how these practices are interwoven with the “producing practices” of software development, thus enabling collaboration to span expert groups and disparate fields; and (c) theorizing the role of vision as an emerging conceptual boundary object in these practices."
/doi/10.1287/ijoc.15.2.123.14444," The World Wide Web (WWW) is the largest distributed information space and has grown to encompass diverse information resources. Although the web is growing exponentially, the individual's capacity to read and digest content is essentially fixed. The full economic potential of the web will not be realized unless enabling technologies are provided to facilitate access to web resources. Currently web personalization is the most promising approach to remedy this problem, and web mining, particularly web-usage mining, is considered a crucial component of any efficacious web-personalization system. In this paper, we describe a complete framework for web-usage mining to satisfy the challenging requirements of web-personalization applications. For online and anonymous web personalization to be effective, web usage mining must be accomplished in real time as accurately as possible. On the other hand, web-usage mining should allow a compromise between scalability and accuracy to be applicable to real-life websites with numerous visitors. Within our web-usage-mining framework, we introduce a distributed user-tracking approach for accurate, scalable, and implicit collection of the usage data. We also propose a new model, the feature-matrices (FM) model, to discover and interpret users' access patterns. With FM, various spatial and temporal features of usage data can be captured with flexible precision so that we can trade off accuracy for scalability based on the specific application requirements. Moreover, tunable complexity of the FM model allows real-time and adaptive access pattern discovery from usage data. We define a novel similarity measure based on FM that is specifically designed for accurate classification of partial navigation patterns in real time. Our extensive experiments with both synthetic and real data verify correctness and efficacy of our web-usage-mining framework for anonymous and efficient web personalization."
